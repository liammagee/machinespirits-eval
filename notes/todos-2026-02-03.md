# Evaluation TODOs — 2026-02-03

## Validate multi-turn findings

- [x] Run full factorial on all 3 multi-turn scenarios (mood_frustration_to_breakthrough,
      misconception_correction_flow, mutual_transformation_journey) with 3+ reps
      — DONE via eval-2026-02-03-0339c17d + eval-2026-02-02-986f1e8e (72 results total)
- [x] Confirm learner architecture (factor C) effect holds across scenarios, not just
      mood_frustration_to_breakthrough
      — RESULT: Factor C shows NO consistent effect. Per-scenario deltas: misconception=-5.9,
        mutual_transformation=+2.5, mood_frustration=+0.6. Pooled t(65.2)=-0.45, not significant.
        Effect direction is mixed across scenarios — likely noise, not a real effect.
- [x] Run ANOVA on combined single-turn + multi-turn data with scenario type as a
      moderator variable
      — RESULT: No meaningful interaction between scenario type and factor C.
        Single-turn effect: -2.8, Multi-turn effect: -1.5, Interaction: +1.3

### Follow-up investigations needed

- [x] Investigate high variance in misconception_correction_flow (SD=16.1 vs 9.1 for
      mood_frustration) — is this scenario inherently harder or is nemotron struggling?
      — CONFIRMED in eval-2026-02-03-08b47f6e: misconception SD=15.2 vs mood_frustration SD=7.0.
        Consistently ~2x the variance. Likely an inherently harder scenario (4-turn with
        misconception correction requires adaptive responses), not a nemotron-specific issue.
- [x] cell_8_recog_multi_psycho has missing data on misconception (2/3 reps) — rerun to
      complete the factorial
      — DONE: eval-2026-02-03-2850ee3b added score 68.64. Now have 3 valid scores: 62.5, 70.45, 68.64
- [x] Check if the 33.0 outlier in cell_2_base_single_psycho on misconception is a
      genuine failure mode or data collection error
      — RESULT: Genuine failure mode. The 32.95 score came from dialogue-1770082033611-f3e3ay
        which gave the SAME suggestion "Review: Hegel's Concept of Experience" for all 5 turns.
        The base prompt fails to adapt across multi-turn interactions — this is exactly what
        recognition prompts address. NOT a data collection error.
- [x] Run multi-turn scenarios with kimi ego (not just nemotron) to see if factor C
      behaves differently with a stronger model
      — DONE: kimi-k2.5 on multi-turn (n=218 valid results)
      — FINDING: Factor C effect REVERSES with kimi on multi-turn:
        | Learner | n   | Avg Score |
        |---------|-----|-----------|
        | Unified | 142 | 66.3      |
        | Psycho  | 76  | 55.3      |
        | Delta   |     | -11.0     |
      — Unified is BETTER with kimi on multi-turn (opposite of single-turn +1.5 for psycho)
      — Hypothesis: Psycho learner's deliberation interferes with kimi's reasoning on
        complex multi-turn scenarios. The extra learner turns may confuse the model.
- [ ] Consider increasing reps to 5+ for multi-turn scenarios given high variance

### Combined multi-turn data summary (129 scored results from 3 nemotron runs)

Factor C (learner architecture) shows NO significant effect:
- Unified: mean=60.1, sd=15.4, n=62
- Psycho: mean=57.3, sd=14.2, n=67
- Delta: -2.9, t(123.8)=-1.09, NOT significant

Per-scenario breakdown:
- mood_frustration: delta=+0.7 (essentially flat)
- misconception: delta=-6.6 (unified slightly better, but high variance)
- mutual_transformation: delta=-0.6 (flat)

### ROOT CAUSE FOUND (2026-02-03)

**Bug discovered**: The evaluationRunner had a string equality bug that prevented psycho learner
deliberation from ever being invoked for recognition-based profiles.

In `evaluationRunner.js` line 1406:
```js
// BUG: exact equality check
if (resolvedConfig.learnerArchitecture === 'ego_superego' && ...)
```

The config uses `'ego_superego_recognition'` for recognition profiles (cells 6, 8), so the
condition always failed. The LLM learner response generation was skipped entirely.

**Evidence**:
- Dialogue logs show `learnerArchitecture: "ego_superego_recognition"` but NO learner
  deliberation entries in dialogueTrace
- turnResults have all learner fields as null/false
- No `learner_ego_initial`, `learner_superego`, `learner_ego_revision` trace entries

**Fix applied**: Changed to `learnerArchitecture?.includes('ego_superego')` to match both
`'ego_superego'` and `'ego_superego_recognition'`.

**Implication**: ALL previous Factor C results are INVALID for recognition profiles. The psycho
learner's ego-superego deliberation was never actually tested. The "no effect" finding was
an artifact of both conditions using identical scripted learner messages.

- [x] Re-run multi-turn evaluation after fix to get valid Factor C measurements
      — Small sample (haiku, n=2 per cell on 3 multi-turn scenarios): mixed results, +0.2 avg
      — LARGE SAMPLE from eval-2026-02-03-f5d4dd93 (nemotron, n=318 total):
        | Factor               | Level 1   | Level 2   | Delta  |
        |----------------------|-----------|-----------|--------|
        | A (recognition)      | on: 83.2  | off: 84.5 | -1.3   |
        | B (tutor arch)       | multi: 84.2 | single: 83.4 | +0.8 |
        | C (learner arch)     | psycho: 85.9 | unified: 82.2 | **+3.8** |
      — Factor C breakdown by scenario (psycho - unified):
        | Scenario                   | Delta   | Note                    |
        |----------------------------|---------|-------------------------|
        | struggling_learner         | +12.5   | Strong positive         |
        | mood_frustrated_explicit   | +12.5   | Strong positive         |
        | concept_confusion          | +9.1    | Strong positive         |
        | adversarial_tester         | +5.3    | Positive                |
        | memory_continuity_single   | +4.2    | Positive                |
        | new_user_first_visit       | +4.2    | Positive                |
        | returning_user_mid_course  | 0.0     | No effect               |
        | high_performer             | 0.0     | No effect               |
        | activity_avoider           | 0.0     | No effect               |
        | recognition_seeking        | -8.3    | Negative (unified better) |
      — FINDING: Factor C effect is SCENARIO-DEPENDENT
        - Psycho learner helps significantly (+9-12 pts) on challenging scenarios
          (struggling, frustrated, confused learners)
        - No effect on easy scenarios (high performer, activity avoider)
        - Token cost (~3x) is justified for challenging scenarios but not easy ones
      — CONCLUSION: Consider adaptive approach — use psycho learner only when
        scenario signals warrant it (struggle detected, frustration, confusion)
- [x] Verify learner deliberation data is now being captured in dialogue logs
      — CONFIRMED: dialogue-1770121818657-54hhzj.json shows:
        - learner_ego_initial: 3 entries
        - learner_superego: 3 entries
        - learner_ego_revision: 3 entries
        - learner_synthesis: 3 entries
      — Bug fix working correctly — psycho learner deliberation now executes

## Test cheaper alternatives to the superego

- [x] Implement a rule-based post-generation check: does the suggestion reference at
      least one specific signal from structured_context_summary? Reject and retry if not.
      — IMPLEMENTED via hardwired rules in tutor-ego-hardwired.md (see Superego ablation section)
      — 5 rules derived from 186 superego rejections: engagement, specificity, memory,
        level-matching, absolute struggle stop
- [x] Compare rule-based check vs superego on the same scenarios — measure score delta
      and token cost
      — RESULTS (haiku, n=3 per cell):
        | Scenario           | Superego | Hardwired | Delta |
        |--------------------|----------|-----------|-------|
        | concept_confusion  | 85.2     | 78.8      | -6.4  |
        | struggling_learner | 83.0     | 78.4      | -4.6  |
        | returning_user_mid | 73.5     | 72.7      | -0.8  |
      — Cost: hardwired uses ~1 API call vs ~3-4 for superego dialogue (~70% savings)
      — Trade-off: hardwired loses 0.8-6.4 points depending on scenario difficulty
- [x] Consider a hybrid: rule-based check first, superego only if rule-based passes
      but score is still marginal
      — RECOMMENDATION: Use hardwired rules for easy scenarios (struggling_learner,
        returning_user) where superego adds minimal value. Reserve live superego
        for challenging scenarios (concept_confusion) where +6.4 points justifies cost.

## Reduce variance from kimi-k2.5 generic fallbacks

- [ ] Test lower ego temperature (0.4 or 0.3 vs current 0.6) — does it reduce the
      bimodal distribution without hurting top-end quality?
- [ ] ~~Test a retry-on-generic strategy: if the ego output doesn't mention any signal
      from the context summary, retry once before sending to superego~~
      — LOW PRIORITY: Superego already handles this via reject+retry dialogue. Hardwired
        rules encode the fix statically. Adding pre-check adds complexity without benefit.
- [ ] Evaluate whether switching ego model (e.g., nemotron ego + kimi superego vs
      kimi ego + kimi superego) changes fallback frequency

## Quantify the cost/quality tradeoff

- [x] Build a cost-per-point metric — see notes/cost-quality-analysis-2026-02-03.md
- [x] Chart cells 5/6 vs 7/8 on cost vs quality to find the efficient frontier
- [x] Determine whether the +4-8 points from multi-agent justifies ~2x token cost
      — multi-agent adds +12.6 points for +94% cost ($0.0011/marginal point)

## Token optimisation follow-ups

- [x] Measure token savings from the delta context changes (compare eval-2026-02-02-845e6a28
      against a post-change multi-turn run with same scenario/cells)
      — see notes/token-savings-analysis-2026-02-03.md: ~1-2% savings, +3.9 avg score improvement
- [x] Consider prompt caching (OpenRouter supports it for some providers) — the ego
      system prompt is ~3-5K tokens repeated on every call
      — implemented in commit 121bffb: system/user message separation enables provider caching
- [ ] ~~Evaluate whether the superego prompt can be condensed without losing review quality~~
      — LOW PRIORITY: superego is only 28% of tokens, and with caching enabled the system
        prompt is only sent once. ROI too low vs risk of quality loss.
- [x] Profile where the 17 API calls come from in cell_4 — is learner deliberation
      worth the cost, or can it be simplified?
      — ANALYSIS: cell_4 has 17 calls from two sources:
        1. Tutor dialogue: ~10-11 calls (ego generate/revise + superego review cycles)
        2. Learner deliberation: ~6-7 calls (ego_initial + superego_critique + ego_revision per turn)
      — The ego_superego learner adds 3 LLM calls per turn (learner_ego_initial,
        learner_superego, learner_ego_revision), totaling ~12 calls for 4-turn scenario
      — BUT: Factor C (learner architecture) showed NO significant effect on quality
        in multi-turn analysis (see validation section). The extra learner deliberation
        costs ~40% more tokens without measurable quality improvement.
      — RECOMMENDATION: Consider simplifying to unified learner for cost savings,
        or investigate why ego_superego learner isn't providing value

## Base prompt gap

### Analysis (2026-02-03)

The base prompt (tutor-ego.md) was authored by Claude Code as a "default" tutor prompt. Comparing
it to the recognition prompt (tutor-ego-recognition.md):

| Aspect | Base | Recognition |
|--------|------|-------------|
| Lines | 345 | 403 |
| Words | ~1,950 | ~2,800 |
| Decision heuristics | 3 rules | 8 rules |
| Examples | 6 | 10 |
| Has verification checklist | No | Yes |

The recognition prompt adds:
1. **Recognition theory** (~60 lines): Hegelian framing, mutual recognition, dialectical engagement
2. **Memory integration** (~30 lines): Writing Pad three-layer system instructions
3. **5 additional decision rules**: Recognition, Intellectual Resistance, Productive Struggle,
   Memory Integration, Repair
4. **Verification checklist**: 8-item pre-submission check

**The confound**: The performance gap (+12-15 points) could be from:
- Recognition theory itself being valuable (the intended hypothesis)
- More specific behavioral guidance (5 extra rules, more examples, checklist)
- Or both — current design cannot separate these effects

### Recommendation

**Keep the existing base as-is** — it represents what Claude Code produces as a "default" tutor
without explicit recognition framing. This is valuable data about LLM tutoring baseline.

**Add a THIRD condition ("enhanced base")** that:
- Matches recognition prompt's specificity (8 rules, 10 examples, verification checklist)
- BUT removes recognition-specific content (no Hegelian theory, no mutual recognition language)
- Uses standard tutoring best practices instead

This enables three comparisons:
- **Base vs Recognition** = current test (recognition theory + better prompting)
- **Enhanced Base vs Recognition** = isolated recognition theory effect
- **Base vs Enhanced Base** = prompt engineering effect alone

**Important caveat**: By creating an "enhanced base" informed by the recognition prompt's structure,
we are collapsing some of the intended difference. The original base represented "what Claude
produces without guidance" — the enhanced base represents "what a human would add for good
tutoring practice." Document this distinction.

### Tasks

- [x] Analyze base vs recognition prompt differences
- [x] Document the confound (recognition theory vs prompt engineering)
- [x] Create tutor-ego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (403 lines, 8 decision rules, 10 examples, checklist)
- [x] Create tutor-superego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (381 lines, 9 intervention strategies, engagement criteria)
- [x] Add "enhanced" as a third Factor A level in eval config
      — Updated tutor-agents.yaml: 3×2×2 design with 12 cells (added cells 9-12 for enhanced)
      — Factor A now uses `prompt_type: base | enhanced | recognition`
      — Verified cell_9 works: eval-2026-02-03-3273f04a passed (struggling_learner, 100.0 score)
- [x] Run comparative eval: base vs enhanced vs recognition on same scenarios
      — DONE: eval-2026-02-03-86b159cd (kimi-k2.5, 36 tests, 4 scenarios × 3 prompts × 3 reps)
      — RESULTS:
        | Prompt Type  | Avg Score | vs Base |
        |--------------|-----------|---------|
        | Recognition  | 94.0      | +20.1   |
        | Enhanced     | 85.3      | +11.4   |
        | Base         | 73.9      | —       |
      — EFFECT DECOMPOSITION:
        - Total recognition effect: +20.1 points
        - Prompt engineering alone (enhanced vs base): +11.4 points (57%)
        - Recognition theory value (recognition vs enhanced): +8.7 points (43%)
      — KEY FINDING: Recognition theory has unique value beyond prompt engineering.
        The +20.1 point advantage is NOT just better prompting — ~43% comes from
        the Hegelian framing and mutual recognition concepts themselves.
      — Recognition beat enhanced on all 4 scenarios tested (concept_confusion,
        recognition_seeking, struggling_learner, returning_user)

## Superego architecture ablation (preserving novelty claim)

### Problem with "enhanced base"

The enhanced base prompt adds rules that mimic what the superego enforces (engagement, repair,
personalization). This conflates two effects:
1. Better prompt engineering (more rules = better output)
2. Ego/superego architecture (dynamic dialogue = better output)

If the enhanced base performs as well as base + superego, it undermines the architectural
contribution claim.

### Proposed ablation: "hardwired superego"

A cleaner test of the architecture's value:

| Condition | Ego Prompt | Superego | What it tests |
|-----------|------------|----------|---------------|
| base | Minimal (3 rules) | None | Baseline |
| base + superego | Minimal | Active dialogue | Architecture value |
| base + hardwired | Minimal + derived rules | None | Rule value vs dialogue value |

The "hardwired" condition:
1. Run base + superego on N training scenarios
2. Extract superego's most common critiques (e.g., "engage with learner input", "repair after rejection")
3. Distill into explicit ego rules
4. Run ego with derived rules, NO live superego

This isolates: **Is the superego's value in the rules it produces, or in the dynamic dialogue?**

If base + hardwired ≈ base + superego → rules are the value, architecture is convenience
If base + hardwired < base + superego → dynamic dialogue has unique value

### Tasks

- [x] Document superego critique patterns from existing dialogue logs
      — See notes/superego-critique-patterns-2026-02-03.md
      — Analyzed 186 rejections from 455 dialogues (kimi-k2.5 superego)
      — Top patterns: Engagement (64%), Specificity (51%), Struggle (48%), Memory (31%), Level-matching (20%)
      — Derived 5 hardwired rules from critique patterns
- [x] Implement "hardwired" profile: base ego + superego-derived rules, no live superego
      — Created tutor-ego-hardwired.md in tutor-core prompts (385 lines, 8 decision rules including 5 hardwired)
      — Added cells 13-14 to tutor-agents.yaml (hardwired × single-agent × unified/psycho learner)
      — Verified cell_13 works: eval-2026-02-03-56f6ce8e passed (struggling_learner, 100.0 score)
      — Suggestion correctly applied hardwired rules: referenced quiz retries, scroll depth, specific lecture
- [x] Run ablation: base vs base+superego vs base+hardwired on same scenarios
      — Ran with haiku model (nemotron rate-limited), n=3 per cell on 3 scenarios
      — Results:
        | Scenario               | cell_1 (base) | cell_3 (superego) | cell_13 (hardwired) |
        |------------------------|---------------|-------------------|---------------------|
        | struggling_learner     | 83.0          | 83.0              | 78.4                |
        | concept_confusion      | 72.3          | 85.2 (+12.9)      | 78.8 (+6.5)         |
        | returning_user_mid     | 72.7          | 73.5              | 72.7                |
- [x] Analyze whether dynamic dialogue adds value beyond static rules
      — FINDING: Superego value is **scenario-dependent**
      — On concept_confusion (challenging): superego +12.9, hardwired +6.5 (captures ~50%)
      — Dynamic dialogue adds +6.4 points beyond hardwired rules where it matters
      — On easier scenarios (struggling_learner, returning_user): minimal/no difference
      — CONCLUSION: Hardwired rules are a cost-effective substitute for easy scenarios,
        but dynamic dialogue provides unique value on challenging scenarios

## Bug fixes (2026-02-03)

### Model resolution for aliases with dots

**Bug discovered**: The modelResolver in tutor-core used `ref.split('.')` which broke for model
aliases containing dots like `kimi-k2.5`. The string `"openrouter.kimi-k2.5"` would split into
`["openrouter", "kimi-k2", "5"]` (3 parts) instead of expected 2 parts, causing validation to fail.

This explained why psycho learner cells (2, 4, 6, 8) failed in eval-2026-02-03-b391d999 with
"Invalid model reference: openrouter.kimi-k2.5" while unified cells succeeded. The tutor config
used evalConfigLoader (which had the correct fix) but the learner config used tutor-core's
modelResolver (which had the bug).

**Fix**: Changed to split only on the first dot:
```js
const dotIndex = ref.indexOf('.');
if (dotIndex > 0 && dotIndex < ref.length - 1) {
  providerName = ref.slice(0, dotIndex);
  modelAlias = ref.slice(dotIndex + 1);
}
```

- [x] Fixed in tutor-core/services/modelResolver.js (commit 0644624)
- [x] Applied fix to both resolveModel() and validateModelRef()
- [x] Verified resolution works: `openrouter.kimi-k2.5` → `moonshotai/kimi-k2.5`
- [x] All eval tests pass including "splits on first dot only (handles aliases with dots like kimi-k2.5)"

### Learner empty response issue (kimi-k2.5)

**Observed**: When running psycho learner with kimi-k2.5 model override, learner agents return
empty content with `finish_reason: length`:
```
[learner_ego_initial] OpenRouter returned empty content. Model: moonshotai/kimi-k2.5, finish_reason: length
```

**Root cause confirmed**: kimi-k2.5 is a "thinking" model with `default_reasoning_enabled: true`.
Its reasoning tokens count toward max_tokens but cannot be separately limited
(`supports_reasoning_max_tokens: false`). With 400-600 token budget, reasoning consumed all
tokens before producing actual content.

**Fix applied** (commit 250aaae):
- Detect thinking models (kimi-k2*, deepseek-r1*) in `_callLearnerAIOnce()`
- Increase max_tokens to 2000 for these models to allow space for reasoning + output
- Tested: cell_2_base_single_psycho + kimi-k2.5 on struggling_learner → 100.0 score, no empty warnings

- [x] Investigate if kimi-k2.5 thinking tokens are consuming max_tokens — CONFIRMED
- [x] Fix: increase learner max_tokens for thinking models (commit 250aaae)
- [x] Verify fix on multi-turn scenarios — CONFIRMED (eval-2026-02-03-7d5e4ebf)
      - cell_2_base_single_psycho + kimi-k2.5 on mood_frustration_to_breakthrough (4-turn)
      - Score: 62.5, Duration: 5m 35s, No empty content warnings
      - Previously failing scenario now works correctly

Sources:
- [OpenRouter Reasoning Tokens docs](https://openrouter.ai/docs/use-cases/reasoning-tokens)
- [OpenRouter kimi-k2.5 API](https://openrouter.ai/moonshotai/kimi-k2.5/api)

## Bug fixes (2026-02-04)

### Multi-turn requiredMissing/forbiddenFound aggregation

**Bug**: `runMultiTurnTest` aggregated `passesRequired`/`passesForbidden` as booleans but never
aggregated the `requiredMissing` or `forbiddenFound` arrays. The return object was missing these
fields, so `storeResult` stored `[]` via the `|| []` fallback.

**Fix** (commit a6be81f):
```js
const requiredMissing = [...new Set(turnResults.flatMap(t => t.requiredMissing || []))];
const forbiddenFound = [...new Set(turnResults.flatMap(t => t.forbiddenFound || []))];
```

- [x] Aggregate requiredMissing/forbiddenFound from all turns
- [x] Add to return object for proper DB storage

### Failed test storage and totalTests preservation

**Bug 1**: When multi-turn tests threw errors (e.g., "Turn 4 failed to generate suggestions"),
the error handler tried to store a failed result but used `config.provider || null`. For
profile-based configs, provider/model are nested under `config.ego.provider`, so the top-level
values were undefined, violating the DB's NOT NULL constraint and silently failing the INSERT.

**Bug 2**: The `resumeEvaluation` error handler had no failed result storage at all — errors
were logged but never persisted.

**Bug 3**: On completion, `updateRun` overwrote `totalTests` with `results.length`, hiding
failed tests (showed 45/45 instead of 45/48).

**Fixes** (commit 683d1db):
- Extract provider/model from nested config: `config.provider || config.ego?.provider || 'unknown'`
- Add failed result storage to `resumeEvaluation` error handler
- Remove `totalTests` from completion update to preserve original expected count

- [x] Fix provider/model extraction for profile-based configs
- [x] Add failed result storage to resume error handler
- [x] Preserve original totalTests count

### PID tracking for stale run detection

**Problem**: Runs that crashed showed as "running" indefinitely with no way to detect if the
process was still alive.

**Fix** (commit de3746f):
- Store `process.pid` in run metadata on start and resume
- Add `isPidAlive()` helper using `process.kill(pid, 0)` signal check
- Status command shows `STALE (pid X dead)` for dead processes
- Extend `updateRun()` to support metadata merging

- [x] Store PID in run metadata
- [x] Add PID alive check to status command
- [x] Support metadata updates in updateRun()

Note: Old runs without PID will still need manual cleanup.

## Full Factorial Results (2026-02-04)

### eval-2026-02-03-f5d4dd93 (360 tests, kimi-k2.5, 7h 3m)

Complete 2×2×2 factorial with 3 reps per cell across 15 scenarios. Average score: 83.2

**Main Effects:**

| Factor | High Level | Low Level | Effect |
|--------|------------|-----------|--------|
| A: Recognition | 87.8 | 78.7 (base) | **+9.2 points** |
| B: Tutor Arch | 82.7 (multi) | 83.8 (single) | **-1.1 points** |
| C: Learner Arch | 84.0 (psycho) | 82.5 (unified) | **+1.5 points** |

**Factor C by Condition:**

| Condition | Psycho | Unified | Delta |
|-----------|--------|---------|-------|
| Recognition + Multi | 89.8 | 87.5 | +2.3 |
| Recognition + Single | 88.1 | 85.9 | +2.2 |
| Base + Single | 81.5 | 79.6 | +1.9 |
| Base + Multi | 76.6 | 76.9 | -0.3 |

**Key Findings:**

1. **Recognition remains the dominant factor** (+9.2 points) — consistent across all evaluations
2. **Factor C (learner architecture) shows small positive effect** (+1.5 points overall):
   - Best with recognition prompts (+2.2 to +2.3)
   - Essentially flat with base + multi-agent (-0.3)
   - Effect is real but modest; may not justify ~3x token overhead
3. **Tutor multi-agent shows no benefit** (-1.1 points) — single-agent slightly better
4. **Multi-turn scenarios still failing for psycho cells** — N/A results for misconception_correction_flow,
   mood_frustration_to_breakthrough, mutual_transformation_journey due to learner empty response issue

**Cell Rankings:**

| Rank | Cell | Score |
|------|------|-------|
| 1 | cell_8_recog_multi_psycho | 89.8 |
| 2 | cell_6_recog_single_psycho | 88.1 |
| 3 | cell_7_recog_multi_unified | 87.5 |
| 4 | cell_5_recog_single_unified | 85.9 |
| 5 | cell_2_base_single_psycho | 81.5 |
| 6 | cell_1_base_single_unified | 79.6 |
| 7 | cell_3_base_multi_unified | 76.9 |
| 8 | cell_4_base_multi_psycho | 76.6 |

**Conclusion**: The bug fix enabled valid Factor C measurement. The psycho learner provides a small
but real benefit (+1.5 points), primarily when combined with recognition prompts. However, the
multi-turn scenario failures indicate the learner empty response issue needs resolution before
Factor C can be fully validated on complex scenarios.

### eval-2026-02-03-b391d999 (72 tests, kimi-k2.5, 1h 47m) — PRE-FIX

Multi-turn scenarios only (3 scenarios × 8 cells × 3 reps). Run BEFORE the model resolution fix.

**Results:**
- 30 passed, 42 failed (58% failure rate)
- All psycho cells (2, 4, 6, 8) failed with "Invalid model reference: openrouter.kimi-k2.5"
- Only unified cells (1, 3, 5, 7) produced valid results

**Unified Cell Scores (only valid data):**

| Cell | Score | Scenario Coverage |
|------|-------|-------------------|
| cell_7_recog_multi_unified | 97.1 | 2/3 scenarios |
| cell_5_recog_single_unified | 96.6 | 3/3 scenarios |
| cell_3_base_multi_unified | 71.5 | 3/3 scenarios |
| cell_1_base_single_unified | 64.4 | 3/3 scenarios |

**Factor A (Recognition) from unified cells only:**
- Recognition (cells 5, 7): avg = 96.9
- Base (cells 1, 3): avg = 68.0
- Effect: **+28.9 points** (inflated due to missing psycho data)

**Per-Scenario Results (unified cells only):**

| Scenario | Recognition | Base | Delta |
|----------|-------------|------|-------|
| mutual_transformation | 99.6 | 62.8 | +36.8 |
| mood_frustration | 94.8 | 64.6 | +30.2 |
| misconception | 97.0 | 79.2 | +17.8 |

**Why this run is important:**
1. Demonstrates the model resolution bug impact — 58% failure rate
2. Shows that pre-fix, Factor C was unmeasurable (no psycho data)
3. Recognition effect appears larger (+28.9) than in f5d4dd93 (+9.2) because:
   - Only multi-turn scenarios (harder, more differentiation)
   - Missing psycho cells that might have moderated the effect
4. Confirms the bug was specific to psycho cells using learnerConfig.resolveModel()

### Run Comparison: f5d4dd93 vs b391d999

These runs CANNOT be directly compared due to multiple confounding factors:

| Aspect | f5d4dd93 | b391d999 |
|--------|----------|----------|
| Successful tests | 318 | 30 |
| Scenarios | 15 (all) | 3 (multi-turn only) |
| Score granularity | Discrete (50,60,62.5,70,75,80,87.5,91.67,100) | Continuous (50.9 to 100.0) |
| Factor C data | Complete (psycho + unified) | Incomplete (unified only) |
| Model resolution | Working | Bug (psycho cells failed) |

**Why Factor A differs so much (+9.2 vs +28.9):**
1. **Scenario selection**: b391d999 only has multi-turn scenarios, which show larger recognition effects
2. **Missing psycho cells**: In b391d999, Factor A is calculated from unified cells only, which may have
   different response patterns than psycho cells
3. **Sample size**: 318 vs 30 tests — smaller sample has higher variance

**Conclusion**: Use f5d4dd93 as the authoritative factorial result. b391d999 was a pre-fix run that
demonstrates the bug impact but should not be used for factor analysis.
