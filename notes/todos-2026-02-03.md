# Evaluation TODOs — 2026-02-03

## Validate multi-turn findings

- [x] Run full factorial on all 3 multi-turn scenarios (mood_frustration_to_breakthrough,
      misconception_correction_flow, mutual_transformation_journey) with 3+ reps
      — DONE via eval-2026-02-03-0339c17d + eval-2026-02-02-986f1e8e (72 results total)
- [x] Confirm learner architecture (factor C) effect holds across scenarios, not just
      mood_frustration_to_breakthrough
      — RESULT: Factor C shows NO consistent effect. Per-scenario deltas: misconception=-5.9,
        mutual_transformation=+2.5, mood_frustration=+0.6. Pooled t(65.2)=-0.45, not significant.
        Effect direction is mixed across scenarios — likely noise, not a real effect.
- [x] Run ANOVA on combined single-turn + multi-turn data with scenario type as a
      moderator variable
      — RESULT: No meaningful interaction between scenario type and factor C.
        Single-turn effect: -2.8, Multi-turn effect: -1.5, Interaction: +1.3

### Follow-up investigations needed

- [x] Investigate high variance in misconception_correction_flow (SD=16.1 vs 9.1 for
      mood_frustration) — is this scenario inherently harder or is nemotron struggling?
      — CONFIRMED in eval-2026-02-03-08b47f6e: misconception SD=15.2 vs mood_frustration SD=7.0.
        Consistently ~2x the variance. Likely an inherently harder scenario (4-turn with
        misconception correction requires adaptive responses), not a nemotron-specific issue.
- [x] cell_8_recog_multi_psycho has missing data on misconception (2/3 reps) — rerun to
      complete the factorial
      — DONE: eval-2026-02-03-2850ee3b added score 68.64. Now have 3 valid scores: 62.5, 70.45, 68.64
- [x] Check if the 33.0 outlier in cell_2_base_single_psycho on misconception is a
      genuine failure mode or data collection error
      — RESULT: Genuine failure mode. The 32.95 score came from dialogue-1770082033611-f3e3ay
        which gave the SAME suggestion "Review: Hegel's Concept of Experience" for all 5 turns.
        The base prompt fails to adapt across multi-turn interactions — this is exactly what
        recognition prompts address. NOT a data collection error.
- [ ] Run multi-turn scenarios with kimi ego (not just nemotron) to see if factor C
      behaves differently with a stronger model
- [ ] Consider increasing reps to 5+ for multi-turn scenarios given high variance

### Combined multi-turn data summary (129 scored results from 3 nemotron runs)

Factor C (learner architecture) shows NO significant effect:
- Unified: mean=60.1, sd=15.4, n=62
- Psycho: mean=57.3, sd=14.2, n=67
- Delta: -2.9, t(123.8)=-1.09, NOT significant

Per-scenario breakdown:
- mood_frustration: delta=+0.7 (essentially flat)
- misconception: delta=-6.6 (unified slightly better, but high variance)
- mutual_transformation: delta=-0.6 (flat)

### ROOT CAUSE FOUND (2026-02-03)

**Bug discovered**: The evaluationRunner had a string equality bug that prevented psycho learner
deliberation from ever being invoked for recognition-based profiles.

In `evaluationRunner.js` line 1406:
```js
// BUG: exact equality check
if (resolvedConfig.learnerArchitecture === 'ego_superego' && ...)
```

The config uses `'ego_superego_recognition'` for recognition profiles (cells 6, 8), so the
condition always failed. The LLM learner response generation was skipped entirely.

**Evidence**:
- Dialogue logs show `learnerArchitecture: "ego_superego_recognition"` but NO learner
  deliberation entries in dialogueTrace
- turnResults have all learner fields as null/false
- No `learner_ego_initial`, `learner_superego`, `learner_ego_revision` trace entries

**Fix applied**: Changed to `learnerArchitecture?.includes('ego_superego')` to match both
`'ego_superego'` and `'ego_superego_recognition'`.

**Implication**: ALL previous Factor C results are INVALID for recognition profiles. The psycho
learner's ego-superego deliberation was never actually tested. The "no effect" finding was
an artifact of both conditions using identical scripted learner messages.

- [x] Re-run multi-turn evaluation after fix to get valid Factor C measurements
      — Ran cell_5 vs cell_6 (recognition, single-agent) on all 3 multi-turn scenarios
      — Results (haiku, n=2 per cell):
        | Scenario               | cell_5 (unified) | cell_6 (psycho) | Delta |
        |------------------------|------------------|-----------------|-------|
        | mood_frustration       | 95.3             | 96.6            | +1.3  |
        | misconception          | 94.4             | 96.7            | +2.3  |
        | mutual_transformation  | 98.6             | 95.6            | -3.0  |
        | **Average**            | 96.1             | 96.3            | +0.2  |
      — FINDING: Mixed results — psycho learner helps on 2/3 scenarios but hurts on 1
      — Overall effect near zero (+0.2), but now reflects real variation (not bug artifact)
      — Token cost: psycho learner uses ~3x more tokens due to deliberation
      — CONCLUSION: Factor C effect remains small and inconsistent; cost may not be justified
- [x] Verify learner deliberation data is now being captured in dialogue logs
      — CONFIRMED: dialogue-1770121818657-54hhzj.json shows:
        - learner_ego_initial: 3 entries
        - learner_superego: 3 entries
        - learner_ego_revision: 3 entries
        - learner_synthesis: 3 entries
      — Bug fix working correctly — psycho learner deliberation now executes

## Test cheaper alternatives to the superego

- [x] Implement a rule-based post-generation check: does the suggestion reference at
      least one specific signal from structured_context_summary? Reject and retry if not.
      — IMPLEMENTED via hardwired rules in tutor-ego-hardwired.md (see Superego ablation section)
      — 5 rules derived from 186 superego rejections: engagement, specificity, memory,
        level-matching, absolute struggle stop
- [x] Compare rule-based check vs superego on the same scenarios — measure score delta
      and token cost
      — RESULTS (haiku, n=3 per cell):
        | Scenario           | Superego | Hardwired | Delta |
        |--------------------|----------|-----------|-------|
        | concept_confusion  | 85.2     | 78.8      | -6.4  |
        | struggling_learner | 83.0     | 78.4      | -4.6  |
        | returning_user_mid | 73.5     | 72.7      | -0.8  |
      — Cost: hardwired uses ~1 API call vs ~3-4 for superego dialogue (~70% savings)
      — Trade-off: hardwired loses 0.8-6.4 points depending on scenario difficulty
- [x] Consider a hybrid: rule-based check first, superego only if rule-based passes
      but score is still marginal
      — RECOMMENDATION: Use hardwired rules for easy scenarios (struggling_learner,
        returning_user) where superego adds minimal value. Reserve live superego
        for challenging scenarios (concept_confusion) where +6.4 points justifies cost.

## Reduce variance from kimi-k2.5 generic fallbacks

- [ ] Test lower ego temperature (0.4 or 0.3 vs current 0.6) — does it reduce the
      bimodal distribution without hurting top-end quality?
- [ ] Test a retry-on-generic strategy: if the ego output doesn't mention any signal
      from the context summary, retry once before sending to superego
- [ ] Evaluate whether switching ego model (e.g., nemotron ego + kimi superego vs
      kimi ego + kimi superego) changes fallback frequency

## Quantify the cost/quality tradeoff

- [x] Build a cost-per-point metric — see notes/cost-quality-analysis-2026-02-03.md
- [x] Chart cells 5/6 vs 7/8 on cost vs quality to find the efficient frontier
- [x] Determine whether the +4-8 points from multi-agent justifies ~2x token cost
      — multi-agent adds +12.6 points for +94% cost ($0.0011/marginal point)

## Token optimisation follow-ups

- [x] Measure token savings from the delta context changes (compare eval-2026-02-02-845e6a28
      against a post-change multi-turn run with same scenario/cells)
      — see notes/token-savings-analysis-2026-02-03.md: ~1-2% savings, +3.9 avg score improvement
- [x] Consider prompt caching (OpenRouter supports it for some providers) — the ego
      system prompt is ~3-5K tokens repeated on every call
      — implemented in commit 121bffb: system/user message separation enables provider caching
- [ ] ~~Evaluate whether the superego prompt can be condensed without losing review quality~~
      — LOW PRIORITY: superego is only 28% of tokens, and with caching enabled the system
        prompt is only sent once. ROI too low vs risk of quality loss.
- [x] Profile where the 17 API calls come from in cell_4 — is learner deliberation
      worth the cost, or can it be simplified?
      — ANALYSIS: cell_4 has 17 calls from two sources:
        1. Tutor dialogue: ~10-11 calls (ego generate/revise + superego review cycles)
        2. Learner deliberation: ~6-7 calls (ego_initial + superego_critique + ego_revision per turn)
      — The ego_superego learner adds 3 LLM calls per turn (learner_ego_initial,
        learner_superego, learner_ego_revision), totaling ~12 calls for 4-turn scenario
      — BUT: Factor C (learner architecture) showed NO significant effect on quality
        in multi-turn analysis (see validation section). The extra learner deliberation
        costs ~40% more tokens without measurable quality improvement.
      — RECOMMENDATION: Consider simplifying to unified learner for cost savings,
        or investigate why ego_superego learner isn't providing value

## Base prompt gap

### Analysis (2026-02-03)

The base prompt (tutor-ego.md) was authored by Claude Code as a "default" tutor prompt. Comparing
it to the recognition prompt (tutor-ego-recognition.md):

| Aspect | Base | Recognition |
|--------|------|-------------|
| Lines | 345 | 403 |
| Words | ~1,950 | ~2,800 |
| Decision heuristics | 3 rules | 8 rules |
| Examples | 6 | 10 |
| Has verification checklist | No | Yes |

The recognition prompt adds:
1. **Recognition theory** (~60 lines): Hegelian framing, mutual recognition, dialectical engagement
2. **Memory integration** (~30 lines): Writing Pad three-layer system instructions
3. **5 additional decision rules**: Recognition, Intellectual Resistance, Productive Struggle,
   Memory Integration, Repair
4. **Verification checklist**: 8-item pre-submission check

**The confound**: The performance gap (+12-15 points) could be from:
- Recognition theory itself being valuable (the intended hypothesis)
- More specific behavioral guidance (5 extra rules, more examples, checklist)
- Or both — current design cannot separate these effects

### Recommendation

**Keep the existing base as-is** — it represents what Claude Code produces as a "default" tutor
without explicit recognition framing. This is valuable data about LLM tutoring baseline.

**Add a THIRD condition ("enhanced base")** that:
- Matches recognition prompt's specificity (8 rules, 10 examples, verification checklist)
- BUT removes recognition-specific content (no Hegelian theory, no mutual recognition language)
- Uses standard tutoring best practices instead

This enables three comparisons:
- **Base vs Recognition** = current test (recognition theory + better prompting)
- **Enhanced Base vs Recognition** = isolated recognition theory effect
- **Base vs Enhanced Base** = prompt engineering effect alone

**Important caveat**: By creating an "enhanced base" informed by the recognition prompt's structure,
we are collapsing some of the intended difference. The original base represented "what Claude
produces without guidance" — the enhanced base represents "what a human would add for good
tutoring practice." Document this distinction.

### Tasks

- [x] Analyze base vs recognition prompt differences
- [x] Document the confound (recognition theory vs prompt engineering)
- [x] Create tutor-ego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (403 lines, 8 decision rules, 10 examples, checklist)
- [x] Create tutor-superego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (381 lines, 9 intervention strategies, engagement criteria)
- [x] Add "enhanced" as a third Factor A level in eval config
      — Updated tutor-agents.yaml: 3×2×2 design with 12 cells (added cells 9-12 for enhanced)
      — Factor A now uses `prompt_type: base | enhanced | recognition`
      — Verified cell_9 works: eval-2026-02-03-3273f04a passed (struggling_learner, 100.0 score)
- [ ] Run comparative eval: base vs enhanced vs recognition on same scenarios

## Superego architecture ablation (preserving novelty claim)

### Problem with "enhanced base"

The enhanced base prompt adds rules that mimic what the superego enforces (engagement, repair,
personalization). This conflates two effects:
1. Better prompt engineering (more rules = better output)
2. Ego/superego architecture (dynamic dialogue = better output)

If the enhanced base performs as well as base + superego, it undermines the architectural
contribution claim.

### Proposed ablation: "hardwired superego"

A cleaner test of the architecture's value:

| Condition | Ego Prompt | Superego | What it tests |
|-----------|------------|----------|---------------|
| base | Minimal (3 rules) | None | Baseline |
| base + superego | Minimal | Active dialogue | Architecture value |
| base + hardwired | Minimal + derived rules | None | Rule value vs dialogue value |

The "hardwired" condition:
1. Run base + superego on N training scenarios
2. Extract superego's most common critiques (e.g., "engage with learner input", "repair after rejection")
3. Distill into explicit ego rules
4. Run ego with derived rules, NO live superego

This isolates: **Is the superego's value in the rules it produces, or in the dynamic dialogue?**

If base + hardwired ≈ base + superego → rules are the value, architecture is convenience
If base + hardwired < base + superego → dynamic dialogue has unique value

### Tasks

- [x] Document superego critique patterns from existing dialogue logs
      — See notes/superego-critique-patterns-2026-02-03.md
      — Analyzed 186 rejections from 455 dialogues (kimi-k2.5 superego)
      — Top patterns: Engagement (64%), Specificity (51%), Struggle (48%), Memory (31%), Level-matching (20%)
      — Derived 5 hardwired rules from critique patterns
- [x] Implement "hardwired" profile: base ego + superego-derived rules, no live superego
      — Created tutor-ego-hardwired.md in tutor-core prompts (385 lines, 8 decision rules including 5 hardwired)
      — Added cells 13-14 to tutor-agents.yaml (hardwired × single-agent × unified/psycho learner)
      — Verified cell_13 works: eval-2026-02-03-56f6ce8e passed (struggling_learner, 100.0 score)
      — Suggestion correctly applied hardwired rules: referenced quiz retries, scroll depth, specific lecture
- [x] Run ablation: base vs base+superego vs base+hardwired on same scenarios
      — Ran with haiku model (nemotron rate-limited), n=3 per cell on 3 scenarios
      — Results:
        | Scenario               | cell_1 (base) | cell_3 (superego) | cell_13 (hardwired) |
        |------------------------|---------------|-------------------|---------------------|
        | struggling_learner     | 83.0          | 83.0              | 78.4                |
        | concept_confusion      | 72.3          | 85.2 (+12.9)      | 78.8 (+6.5)         |
        | returning_user_mid     | 72.7          | 73.5              | 72.7                |
- [x] Analyze whether dynamic dialogue adds value beyond static rules
      — FINDING: Superego value is **scenario-dependent**
      — On concept_confusion (challenging): superego +12.9, hardwired +6.5 (captures ~50%)
      — Dynamic dialogue adds +6.4 points beyond hardwired rules where it matters
      — On easier scenarios (struggling_learner, returning_user): minimal/no difference
      — CONCLUSION: Hardwired rules are a cost-effective substitute for easy scenarios,
        but dynamic dialogue provides unique value on challenging scenarios

## Bug fixes (2026-02-03)

### Model resolution for aliases with dots

**Bug discovered**: The modelResolver in tutor-core used `ref.split('.')` which broke for model
aliases containing dots like `kimi-k2.5`. The string `"openrouter.kimi-k2.5"` would split into
`["openrouter", "kimi-k2", "5"]` (3 parts) instead of expected 2 parts, causing validation to fail.

This explained why psycho learner cells (2, 4, 6, 8) failed in eval-2026-02-03-b391d999 with
"Invalid model reference: openrouter.kimi-k2.5" while unified cells succeeded. The tutor config
used evalConfigLoader (which had the correct fix) but the learner config used tutor-core's
modelResolver (which had the bug).

**Fix**: Changed to split only on the first dot:
```js
const dotIndex = ref.indexOf('.');
if (dotIndex > 0 && dotIndex < ref.length - 1) {
  providerName = ref.slice(0, dotIndex);
  modelAlias = ref.slice(dotIndex + 1);
}
```

- [x] Fixed in tutor-core/services/modelResolver.js (commit 0644624)
- [x] Applied fix to both resolveModel() and validateModelRef()
- [x] Verified resolution works: `openrouter.kimi-k2.5` → `moonshotai/kimi-k2.5`
- [x] All eval tests pass including "splits on first dot only (handles aliases with dots like kimi-k2.5)"

### Learner empty response issue (kimi-k2.5)

**Observed**: When running psycho learner with kimi-k2.5 model override, learner agents return
empty content with `finish_reason: length`:
```
[learner_ego_initial] OpenRouter returned empty content. Model: moonshotai/kimi-k2.5, finish_reason: length
```

**Hypothesis**: kimi-k2.5 uses reasoning/thinking tokens that consume the max_tokens budget
before generating actual content. The learner-agents.yaml settings (400-600 max_tokens) may
be insufficient for this model's reasoning behavior.

- [ ] Investigate if kimi-k2.5 thinking tokens are consuming max_tokens
- [ ] Consider increasing learner max_tokens for kimi-k2.5, or using a different model for learner
