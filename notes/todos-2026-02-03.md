# Evaluation TODOs — 2026-02-03

## Validate multi-turn findings

- [x] Run full factorial on all 3 multi-turn scenarios (mood_frustration_to_breakthrough,
      misconception_correction_flow, mutual_transformation_journey) with 3+ reps
      — DONE via eval-2026-02-03-0339c17d + eval-2026-02-02-986f1e8e (72 results total)
- [x] Confirm learner architecture (factor C) effect holds across scenarios, not just
      mood_frustration_to_breakthrough
      — RESULT: Factor C shows NO consistent effect. Per-scenario deltas: misconception=-5.9,
        mutual_transformation=+2.5, mood_frustration=+0.6. Pooled t(65.2)=-0.45, not significant.
        Effect direction is mixed across scenarios — likely noise, not a real effect.
- [x] Run ANOVA on combined single-turn + multi-turn data with scenario type as a
      moderator variable
      — RESULT: No meaningful interaction between scenario type and factor C.
        Single-turn effect: -2.8, Multi-turn effect: -1.5, Interaction: +1.3

### Follow-up investigations needed

- [ ] Investigate high variance in misconception_correction_flow (SD=16.1 vs 9.1 for
      mood_frustration) — is this scenario inherently harder or is nemotron struggling?
- [ ] cell_8_recog_multi_psycho has missing data on misconception (2/3 reps) — rerun to
      complete the factorial
- [ ] Check if the 33.0 outlier in cell_2_base_single_psycho on misconception is a
      genuine failure mode or data collection error
- [ ] Run multi-turn scenarios with kimi ego (not just nemotron) to see if factor C
      behaves differently with a stronger model
- [ ] Consider increasing reps to 5+ for multi-turn scenarios given high variance

## Test cheaper alternatives to the superego

- [ ] Implement a rule-based post-generation check: does the suggestion reference at
      least one specific signal from structured_context_summary? Reject and retry if not.
- [ ] Compare rule-based check vs superego on the same scenarios — measure score delta
      and token cost
- [ ] Consider a hybrid: rule-based check first, superego only if rule-based passes
      but score is still marginal

## Reduce variance from kimi-k2.5 generic fallbacks

- [ ] Test lower ego temperature (0.4 or 0.3 vs current 0.6) — does it reduce the
      bimodal distribution without hurting top-end quality?
- [ ] Test a retry-on-generic strategy: if the ego output doesn't mention any signal
      from the context summary, retry once before sending to superego
- [ ] Evaluate whether switching ego model (e.g., nemotron ego + kimi superego vs
      kimi ego + kimi superego) changes fallback frequency

## Quantify the cost/quality tradeoff

- [x] Build a cost-per-point metric — see notes/cost-quality-analysis-2026-02-03.md
- [x] Chart cells 5/6 vs 7/8 on cost vs quality to find the efficient frontier
- [x] Determine whether the +4-8 points from multi-agent justifies ~2x token cost
      — multi-agent adds +12.6 points for +94% cost ($0.0011/marginal point)

## Token optimisation follow-ups

- [x] Measure token savings from the delta context changes (compare eval-2026-02-02-845e6a28
      against a post-change multi-turn run with same scenario/cells)
      — see notes/token-savings-analysis-2026-02-03.md: ~1-2% savings, +3.9 avg score improvement
- [x] Consider prompt caching (OpenRouter supports it for some providers) — the ego
      system prompt is ~3-5K tokens repeated on every call
      — implemented in commit 121bffb: system/user message separation enables provider caching
- [ ] ~~Evaluate whether the superego prompt can be condensed without losing review quality~~
      — LOW PRIORITY: superego is only 28% of tokens, and with caching enabled the system
        prompt is only sent once. ROI too low vs risk of quality loss.
- [x] Profile where the 17 API calls come from in cell_4 — is learner deliberation
      worth the cost, or can it be simplified?
      — ANALYSIS: cell_4 has 17 calls from two sources:
        1. Tutor dialogue: ~10-11 calls (ego generate/revise + superego review cycles)
        2. Learner deliberation: ~6-7 calls (ego_initial + superego_critique + ego_revision per turn)
      — The ego_superego learner adds 3 LLM calls per turn (learner_ego_initial,
        learner_superego, learner_ego_revision), totaling ~12 calls for 4-turn scenario
      — BUT: Factor C (learner architecture) showed NO significant effect on quality
        in multi-turn analysis (see validation section). The extra learner deliberation
        costs ~40% more tokens without measurable quality improvement.
      — RECOMMENDATION: Consider simplifying to unified learner for cost savings,
        or investigate why ego_superego learner isn't providing value

## Base prompt gap

- [ ] Decide whether base prompt is a meaningful control or just under-specified
- [ ] If keeping it as a control: document what it's controlling for (recognition
      theory framing, not prompt quality)
- [ ] If upgrading it: add specificity instructions without recognition-specific
      language to isolate the recognition theory effect from the prompt engineering effect
