# Evaluation TODOs — 2026-02-03

## Validate multi-turn findings

- [ ] Run full factorial on all 3 multi-turn scenarios (mood_frustration_to_breakthrough,
      misconception_correction_flow, mutual_transformation_journey) with 3+ reps
- [ ] Confirm learner architecture (factor C) effect holds across scenarios, not just
      mood_frustration_to_breakthrough
- [ ] Run ANOVA on combined single-turn + multi-turn data with scenario type as a
      moderator variable

## Test cheaper alternatives to the superego

- [ ] Implement a rule-based post-generation check: does the suggestion reference at
      least one specific signal from structured_context_summary? Reject and retry if not.
- [ ] Compare rule-based check vs superego on the same scenarios — measure score delta
      and token cost
- [ ] Consider a hybrid: rule-based check first, superego only if rule-based passes
      but score is still marginal

## Reduce variance from kimi-k2.5 generic fallbacks

- [ ] Test lower ego temperature (0.4 or 0.3 vs current 0.6) — does it reduce the
      bimodal distribution without hurting top-end quality?
- [ ] Test a retry-on-generic strategy: if the ego output doesn't mention any signal
      from the context summary, retry once before sending to superego
- [ ] Evaluate whether switching ego model (e.g., nemotron ego + kimi superego vs
      kimi ego + kimi superego) changes fallback frequency

## Quantify the cost/quality tradeoff

- [x] Build a cost-per-point metric — see notes/cost-quality-analysis-2026-02-03.md
- [x] Chart cells 5/6 vs 7/8 on cost vs quality to find the efficient frontier
- [x] Determine whether the +4-8 points from multi-agent justifies ~2x token cost
      — multi-agent adds +12.6 points for +94% cost ($0.0011/marginal point)

## Token optimisation follow-ups

- [ ] Measure token savings from the delta context changes (compare eval-2026-02-02-845e6a28
      against a post-change multi-turn run with same scenario/cells)
- [ ] Consider prompt caching (OpenRouter supports it for some providers) — the ego
      system prompt is ~3-5K tokens repeated on every call
- [ ] Evaluate whether the superego prompt can be condensed without losing review quality
- [ ] Profile where the 17 API calls come from in cell_4 — is learner deliberation
      worth the cost, or can it be simplified?

## Base prompt gap

- [ ] Decide whether base prompt is a meaningful control or just under-specified
- [ ] If keeping it as a control: document what it's controlling for (recognition
      theory framing, not prompt quality)
- [ ] If upgrading it: add specificity instructions without recognition-specific
      language to isolate the recognition theory effect from the prompt engineering effect
