# Evaluation TODOs — 2026-02-03

## Validate multi-turn findings

- [x] Run full factorial on all 3 multi-turn scenarios (mood_frustration_to_breakthrough,
      misconception_correction_flow, mutual_transformation_journey) with 3+ reps
      — DONE via eval-2026-02-03-0339c17d + eval-2026-02-02-986f1e8e (72 results total)
- [x] Confirm learner architecture (factor C) effect holds across scenarios, not just
      mood_frustration_to_breakthrough
      — RESULT: Factor C shows NO consistent effect. Per-scenario deltas: misconception=-5.9,
        mutual_transformation=+2.5, mood_frustration=+0.6. Pooled t(65.2)=-0.45, not significant.
        Effect direction is mixed across scenarios — likely noise, not a real effect.
- [x] Run ANOVA on combined single-turn + multi-turn data with scenario type as a
      moderator variable
      — RESULT: No meaningful interaction between scenario type and factor C.
        Single-turn effect: -2.8, Multi-turn effect: -1.5, Interaction: +1.3

### Follow-up investigations needed

- [ ] Investigate high variance in misconception_correction_flow (SD=16.1 vs 9.1 for
      mood_frustration) — is this scenario inherently harder or is nemotron struggling?
- [ ] cell_8_recog_multi_psycho has missing data on misconception (2/3 reps) — rerun to
      complete the factorial
- [ ] Check if the 33.0 outlier in cell_2_base_single_psycho on misconception is a
      genuine failure mode or data collection error
- [ ] Run multi-turn scenarios with kimi ego (not just nemotron) to see if factor C
      behaves differently with a stronger model
- [ ] Consider increasing reps to 5+ for multi-turn scenarios given high variance

## Test cheaper alternatives to the superego

- [ ] Implement a rule-based post-generation check: does the suggestion reference at
      least one specific signal from structured_context_summary? Reject and retry if not.
- [ ] Compare rule-based check vs superego on the same scenarios — measure score delta
      and token cost
- [ ] Consider a hybrid: rule-based check first, superego only if rule-based passes
      but score is still marginal

## Reduce variance from kimi-k2.5 generic fallbacks

- [ ] Test lower ego temperature (0.4 or 0.3 vs current 0.6) — does it reduce the
      bimodal distribution without hurting top-end quality?
- [ ] Test a retry-on-generic strategy: if the ego output doesn't mention any signal
      from the context summary, retry once before sending to superego
- [ ] Evaluate whether switching ego model (e.g., nemotron ego + kimi superego vs
      kimi ego + kimi superego) changes fallback frequency

## Quantify the cost/quality tradeoff

- [x] Build a cost-per-point metric — see notes/cost-quality-analysis-2026-02-03.md
- [x] Chart cells 5/6 vs 7/8 on cost vs quality to find the efficient frontier
- [x] Determine whether the +4-8 points from multi-agent justifies ~2x token cost
      — multi-agent adds +12.6 points for +94% cost ($0.0011/marginal point)

## Token optimisation follow-ups

- [x] Measure token savings from the delta context changes (compare eval-2026-02-02-845e6a28
      against a post-change multi-turn run with same scenario/cells)
      — see notes/token-savings-analysis-2026-02-03.md: ~1-2% savings, +3.9 avg score improvement
- [x] Consider prompt caching (OpenRouter supports it for some providers) — the ego
      system prompt is ~3-5K tokens repeated on every call
      — implemented in commit 121bffb: system/user message separation enables provider caching
- [ ] ~~Evaluate whether the superego prompt can be condensed without losing review quality~~
      — LOW PRIORITY: superego is only 28% of tokens, and with caching enabled the system
        prompt is only sent once. ROI too low vs risk of quality loss.
- [x] Profile where the 17 API calls come from in cell_4 — is learner deliberation
      worth the cost, or can it be simplified?
      — ANALYSIS: cell_4 has 17 calls from two sources:
        1. Tutor dialogue: ~10-11 calls (ego generate/revise + superego review cycles)
        2. Learner deliberation: ~6-7 calls (ego_initial + superego_critique + ego_revision per turn)
      — The ego_superego learner adds 3 LLM calls per turn (learner_ego_initial,
        learner_superego, learner_ego_revision), totaling ~12 calls for 4-turn scenario
      — BUT: Factor C (learner architecture) showed NO significant effect on quality
        in multi-turn analysis (see validation section). The extra learner deliberation
        costs ~40% more tokens without measurable quality improvement.
      — RECOMMENDATION: Consider simplifying to unified learner for cost savings,
        or investigate why ego_superego learner isn't providing value

## Base prompt gap

### Analysis (2026-02-03)

The base prompt (tutor-ego.md) was authored by Claude Code as a "default" tutor prompt. Comparing
it to the recognition prompt (tutor-ego-recognition.md):

| Aspect | Base | Recognition |
|--------|------|-------------|
| Lines | 345 | 403 |
| Words | ~1,950 | ~2,800 |
| Decision heuristics | 3 rules | 8 rules |
| Examples | 6 | 10 |
| Has verification checklist | No | Yes |

The recognition prompt adds:
1. **Recognition theory** (~60 lines): Hegelian framing, mutual recognition, dialectical engagement
2. **Memory integration** (~30 lines): Writing Pad three-layer system instructions
3. **5 additional decision rules**: Recognition, Intellectual Resistance, Productive Struggle,
   Memory Integration, Repair
4. **Verification checklist**: 8-item pre-submission check

**The confound**: The performance gap (+12-15 points) could be from:
- Recognition theory itself being valuable (the intended hypothesis)
- More specific behavioral guidance (5 extra rules, more examples, checklist)
- Or both — current design cannot separate these effects

### Recommendation

**Keep the existing base as-is** — it represents what Claude Code produces as a "default" tutor
without explicit recognition framing. This is valuable data about LLM tutoring baseline.

**Add a THIRD condition ("enhanced base")** that:
- Matches recognition prompt's specificity (8 rules, 10 examples, verification checklist)
- BUT removes recognition-specific content (no Hegelian theory, no mutual recognition language)
- Uses standard tutoring best practices instead

This enables three comparisons:
- **Base vs Recognition** = current test (recognition theory + better prompting)
- **Enhanced Base vs Recognition** = isolated recognition theory effect
- **Base vs Enhanced Base** = prompt engineering effect alone

**Important caveat**: By creating an "enhanced base" informed by the recognition prompt's structure,
we are collapsing some of the intended difference. The original base represented "what Claude
produces without guidance" — the enhanced base represents "what a human would add for good
tutoring practice." Document this distinction.

### Tasks

- [x] Analyze base vs recognition prompt differences
- [x] Document the confound (recognition theory vs prompt engineering)
- [x] Create tutor-ego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (403 lines, 8 decision rules, 10 examples, checklist)
- [x] Create tutor-superego-enhanced.md — matches recognition specificity without recognition theory
      — created in tutor-core prompts directory (381 lines, 9 intervention strategies, engagement criteria)
- [ ] Add "enhanced" as a third Factor A level in eval config
- [ ] Run comparative eval: base vs enhanced vs recognition on same scenarios
