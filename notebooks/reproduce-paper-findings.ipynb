{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Notebook: Recognition Theory in AI Tutoring\n",
    "\n",
    "This notebook independently reproduces all 17 tables and key statistical findings from the paper,\n",
    "using only the raw SQLite database and dialogue log files.\n",
    "\n",
    "**Data sources:**\n",
    "- `../data/evaluations.db` — 3,458 scored evaluation rows\n",
    "- `../logs/tutor-dialogues/*.json` — 654 dialogue log files\n",
    "\n",
    "**Six key run IDs:**\n",
    "\n",
    "| Key | Run ID | Section | N |\n",
    "|-----|--------|---------|---|\n",
    "| recognition_validation | eval-2026-02-03-86b159cd | 6.1 | 36 |\n",
    "| full_factorial | eval-2026-02-03-f5d4dd93 | 6.2 | 342 |\n",
    "| ab_nemotron | eval-2026-02-04-948e04b3 | 6.3 | 17 |\n",
    "| ab_kimi | eval-2026-02-05-10b344fb | 6.3 | 60 |\n",
    "| domain_nemotron | eval-2026-02-04-79b633ca | 6.4 | 47 |\n",
    "| domain_kimi | eval-2026-02-05-e87f452d | 6.4 | 60 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ── Data availability check ──────────────────────────────────────────────\n# The evaluation database and dialogue logs are distributed separately\n# as a GitHub Release artifact (~19 MB compressed).\n\nimport os, sys\nfrom pathlib import Path\n\nDB_PATH = Path('../data/evaluations.db')\nLOGS_DIR = Path('../logs/tutor-dialogues')\n\nmissing = []\nif not DB_PATH.exists():\n    missing.append(f'  - Database: {DB_PATH}')\nif not LOGS_DIR.exists() or not any(LOGS_DIR.glob('*.json')):\n    missing.append(f'  - Dialogue logs: {LOGS_DIR}/')\n\nif missing:\n    print('DATA NOT FOUND — this notebook requires the evaluation dataset.\\n')\n    print('Missing:')\n    print('\\n'.join(missing))\n    print('\\nTo obtain the data:\\n')\n    print('  1. Download machinespirits-eval-data-v0.2.0.tar.gz from:')\n    print('     https://github.com/liammagee/machinespirits-eval/releases/tag/v0.2.0\\n')\n    print('  2. Extract from the repository root:')\n    print('     tar xzf machinespirits-eval-data-v0.2.0.tar.gz\\n')\n    print('This will populate data/evaluations.db and logs/tutor-dialogues/.')\n    print('Then re-run this cell and continue.')\n    # Uncomment the next line to halt execution if data is missing:\n    # sys.exit(1)\nelse:\n    print(f'Database found: {DB_PATH} ({DB_PATH.stat().st_size / 1e6:.1f} MB)')\n    n_logs = len(list(LOGS_DIR.glob('*.json')))\n    print(f'Dialogue logs found: {n_logs} files in {LOGS_DIR}/')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "%matplotlib inline\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Database connection ──────────────────────────────────────────────────\n",
    "DB_PATH = '../data/evaluations.db'\n",
    "LOGS_DIR = '../logs/tutor-dialogues'\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "df_all = pd.read_sql_query(\"\"\"\n",
    "    SELECT *\n",
    "    FROM evaluation_results\n",
    "    WHERE success = 1\n",
    "\"\"\", conn)\n",
    "\n",
    "print(f'Total rows loaded: {len(df_all)}')\n",
    "print(f'Rows with overall_score: {df_all[\"overall_score\"].notna().sum()}')\n",
    "print(f'Distinct run_ids: {df_all[\"run_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Run ID dictionary ────────────────────────────────────────────────────\nRUN_IDS = {\n    'recognition_validation': 'eval-2026-02-03-86b159cd',\n    'full_factorial':         'eval-2026-02-03-f5d4dd93',\n    'ab_nemotron':            'eval-2026-02-04-948e04b3',\n    'ab_kimi':                'eval-2026-02-05-10b344fb',\n    'domain_nemotron':        'eval-2026-02-04-79b633ca',\n    'domain_kimi':            'eval-2026-02-05-e87f452d',\n}\n\n# Expected N per run from the paper\nEXPECTED_N = {\n    'recognition_validation': 36,\n    'full_factorial':         342,\n    'ab_nemotron':            17,\n    'ab_kimi':                60,\n    'domain_nemotron':        47,\n    'domain_kimi':            60,\n}\n\ndef get_run(key):\n    \"\"\"Get scored rows for a named run, deduplicating if needed.\n    \n    Some runs have the same response judged by multiple models (rejudging).\n    When total rows > expected N but unique suggestions == expected N,\n    we keep only the first row per unique suggestion to match paper's analysis.\n    When total rows == expected N (e.g., domain_nemotron with 47 rows from\n    multiple judgments of different responses), we keep all rows.\n    \"\"\"\n    run_id = RUN_IDS[key]\n    mask = (df_all['run_id'] == run_id) & df_all['overall_score'].notna()\n    df_run = df_all[mask].copy()\n    \n    expected = EXPECTED_N.get(key)\n    if expected and len(df_run) > expected:\n        # Deduplicate: keep first row per unique suggestions content\n        df_run['_content_hash'] = df_run['suggestions'].apply(\n            lambda s: hashlib.md5(s.encode()).hexdigest() if isinstance(s, str) else str(id(s))\n        )\n        df_run = df_run.drop_duplicates(subset='_content_hash', keep='first')\n        df_run = df_run.drop(columns=['_content_hash'])\n    \n    return df_run\n\n# Verify run sizes\nfor key, run_id in RUN_IDS.items():\n    n = get_run(key).shape[0]\n    expected = EXPECTED_N[key]\n    status = 'OK' if n == expected else f'MISMATCH (expected {expected})'\n    print(f'  {key:30s}  N={n}  {status}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Helper functions ──────────────────────────────────────────────────────\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Compute Cohen's d (pooled SD).\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "def ci_95(data):\n",
    "    \"\"\"95% confidence interval for the mean.\"\"\"\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf(0.975, n - 1)\n",
    "    return m - h, m + h\n",
    "\n",
    "def parse_scores_json(row):\n",
    "    \"\"\"Parse scores_with_reasoning JSON into individual recognition dimension scores.\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(row)\n",
    "        result = {}\n",
    "        for dim in ['mutual_recognition', 'dialectical_responsiveness',\n",
    "                    'transformative_potential', 'memory_integration',\n",
    "                    'tutor_adaptation', 'learner_growth']:\n",
    "            if dim in parsed and 'score' in parsed[dim]:\n",
    "                result[dim] = parsed[dim]['score']\n",
    "            else:\n",
    "                result[dim] = np.nan\n",
    "        return result\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return {dim: np.nan for dim in ['mutual_recognition', 'dialectical_responsiveness',\n",
    "                                         'transformative_potential', 'memory_integration',\n",
    "                                         'tutor_adaptation', 'learner_growth']}\n",
    "\n",
    "# Parse recognition dimension scores from JSON\n",
    "recog_dims = df_all['scores_with_reasoning'].apply(parse_scores_json).apply(pd.Series)\n",
    "for col in recog_dims.columns:\n",
    "    df_all[f'score_{col}'] = recog_dims[col]\n",
    "\n",
    "print('Recognition dimension columns added:')\n",
    "print([c for c in df_all.columns if 'mutual' in c or 'dialectical' in c or 'transformative' in c or 'memory_int' in c or 'tutor_adapt' in c or 'learner_growth' in c])\n",
    "\n",
    "# ── Derive factors from profile_name ──────────────────────────────────────\n",
    "# CRITICAL: factor columns are only 48% populated; always derive from profile_name\n",
    "\n",
    "def derive_factors(profile):\n",
    "    \"\"\"Derive experimental factors from profile_name.\"\"\"\n",
    "    if pd.isna(profile):\n",
    "        return pd.Series({'factor_A_recognition': None, 'factor_B_multi': None,\n",
    "                          'factor_C_learner': None, 'prompt_type': None})\n",
    "    p = str(profile).lower()\n",
    "    # Factor A: Recognition\n",
    "    if 'recog' in p:\n",
    "        recognition = True\n",
    "        prompt_type = 'recognition'\n",
    "    elif 'enhanced' in p or 'cell_9' in p or 'cell_10' in p or 'cell_11' in p or 'cell_12' in p:\n",
    "        recognition = False\n",
    "        prompt_type = 'enhanced'\n",
    "    elif 'placebo' in p:\n",
    "        recognition = False\n",
    "        prompt_type = 'placebo'\n",
    "    else:\n",
    "        recognition = False\n",
    "        prompt_type = 'base'\n",
    "    # Factor B: Multi-agent tutor\n",
    "    multi = '_multi_' in p\n",
    "    # Factor C: Learner architecture\n",
    "    psycho = 'psycho' in p\n",
    "    return pd.Series({\n",
    "        'factor_A_recognition': recognition,\n",
    "        'factor_B_multi': multi,\n",
    "        'factor_C_learner': psycho,\n",
    "        'prompt_type': prompt_type,\n",
    "    })\n",
    "\n",
    "factors = df_all['profile_name'].apply(derive_factors)\n",
    "for col in factors.columns:\n",
    "    df_all[col] = factors[col]\n",
    "\n",
    "print(f'\\nFactor derivation complete. Sample:')\n",
    "print(df_all[['profile_name', 'factor_A_recognition', 'factor_B_multi', 'factor_C_learner', 'prompt_type']].drop_duplicates().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table 1 — Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Verify model configurations per run\n",
    "print('Table 1: Model Configuration by Run')\n",
    "print('=' * 70)\n",
    "\n",
    "for key, run_id in RUN_IDS.items():\n",
    "    df_run = get_run(key)\n",
    "    models = df_run[['model', 'judge_model']].drop_duplicates()\n",
    "    print(f'\\n{key} ({run_id}):')\n",
    "    print(f'  Tutor model(s):  {df_run[\"model\"].unique().tolist()}')\n",
    "    print(f'  Judge model(s):  {df_run[\"judge_model\"].unique().tolist()}')\n",
    "    if 'ego_model' in df_run.columns:\n",
    "        ego = df_run['ego_model'].dropna().unique()\n",
    "        sup = df_run['superego_model'].dropna().unique()\n",
    "        if len(ego) > 0:\n",
    "            print(f'  Ego model(s):    {ego.tolist()}')\n",
    "            print(f'  Superego model(s): {sup.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Table 2 — Sample Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Table 2: Sample summary\nprint('Table 2: Evaluation Sample Summary')\nprint('=' * 70)\n\nsummary_rows = []\nfor key, run_id in RUN_IDS.items():\n    total_raw = len(df_all[df_all['run_id'] == run_id])\n    scored_raw = len(df_all[(df_all['run_id'] == run_id) & df_all['overall_score'].notna()])\n    scored_dedup = len(get_run(key))\n    summary_rows.append({\n        'Evaluation': key,\n        'Run ID': run_id,\n        'Total (raw)': total_raw,\n        'Scored (raw)': scored_raw,\n        'Scored (dedup)': scored_dedup,\n        'Expected N': EXPECTED_N[key],\n    })\n\ndf_summary = pd.DataFrame(summary_rows)\nprint(df_summary.to_string(index=False))\n\nraw_total = df_summary['Total (raw)'].sum()\nraw_scored = df_summary['Scored (raw)'].sum()\ndedup_scored = df_summary['Scored (dedup)'].sum()\nprint(f'\\nRaw totals:   {raw_total} attempted, {raw_scored} scored')\nprint(f'Deduplicated: {dedup_scored} scored (paper primary N)')\nprint(f'Expected:     623 attempted, 562 scored')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table 3 — Inter-Judge Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Inter-judge reliability\n",
    "# Match responses by MD5 hash of suggestions content\n",
    "print('Table 3: Inter-Judge Reliability')\n",
    "print('=' * 70)\n",
    "\n",
    "df_judged = df_all[df_all['judge_model'].notna() & df_all['overall_score'].notna() & df_all['suggestions'].notna()].copy()\n",
    "df_judged['content_hash'] = df_judged['suggestions'].apply(\n",
    "    lambda s: hashlib.md5(s.encode()).hexdigest() if isinstance(s, str) else None\n",
    ")\n",
    "\n",
    "# Group by content hash to find same-response multi-judge pairs\n",
    "hash_groups = df_judged.groupby('content_hash')\n",
    "paired_data = []\n",
    "\n",
    "for content_hash, group in hash_groups:\n",
    "    judges = group['judge_model'].unique()\n",
    "    if len(judges) < 2:\n",
    "        continue\n",
    "    judge_list = sorted(judges)\n",
    "    for i in range(len(judge_list)):\n",
    "        for j in range(i + 1, len(judge_list)):\n",
    "            j1_rows = group[group['judge_model'] == judge_list[i]]\n",
    "            j2_rows = group[group['judge_model'] == judge_list[j]]\n",
    "            for _, r1 in j1_rows.iterrows():\n",
    "                for _, r2 in j2_rows.iterrows():\n",
    "                    paired_data.append({\n",
    "                        'judge1': judge_list[i],\n",
    "                        'judge2': judge_list[j],\n",
    "                        'score1': r1['overall_score'],\n",
    "                        'score2': r2['overall_score'],\n",
    "                        'content_hash': content_hash,\n",
    "                    })\n",
    "\n",
    "df_pairs = pd.DataFrame(paired_data)\n",
    "print(f'Total paired judgments: {len(df_pairs)}')\n",
    "\n",
    "if len(df_pairs) > 0:\n",
    "    # Per-pair correlation\n",
    "    for pair_key, pair_df in df_pairs.groupby(['judge1', 'judge2']):\n",
    "        j1, j2 = pair_key\n",
    "        n = len(pair_df)\n",
    "        r_pearson, p_pearson = stats.pearsonr(pair_df['score1'], pair_df['score2'])\n",
    "        r_spearman, p_spearman = stats.spearmanr(pair_df['score1'], pair_df['score2'])\n",
    "        mad = np.mean(np.abs(pair_df['score1'] - pair_df['score2']))\n",
    "        m1, m2 = pair_df['score1'].mean(), pair_df['score2'].mean()\n",
    "        j1_short = j1.split('/')[-1] if '/' in j1 else j1\n",
    "        j2_short = j2.split('/')[-1] if '/' in j2 else j2\n",
    "        print(f'\\n  {j1_short} vs {j2_short}  (N={n})')\n",
    "        print(f'    Pearson r  = {r_pearson:.3f}  (p={p_pearson:.4f})')\n",
    "        print(f'    Spearman ρ = {r_spearman:.3f}  (p={p_spearman:.4f})')\n",
    "        print(f'    Mean Abs Diff = {mad:.1f} pts')\n",
    "        print(f'    Mean scores: {m1:.1f} vs {m2:.1f}')\n",
    "\n",
    "    # Mean score by judge\n",
    "    print('\\nMean scores by judge:')\n",
    "    for judge in sorted(df_judged['judge_model'].unique()):\n",
    "        m = df_judged[df_judged['judge_model'] == judge]['overall_score'].mean()\n",
    "        print(f'  {judge.split(\"/\")[-1]:30s}  {m:.1f}')\n",
    "else:\n",
    "    print('No paired judgments found. This requires rejudging runs with multiple judge models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of inter-judge agreement (if pairs exist)\n",
    "if len(df_pairs) > 0:\n",
    "    pair_keys = df_pairs.groupby(['judge1', 'judge2']).ngroups\n",
    "    fig, axes = plt.subplots(1, min(pair_keys, 3), figsize=(5 * min(pair_keys, 3), 5))\n",
    "    if pair_keys == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (pair_key, pair_df) in zip(axes, df_pairs.groupby(['judge1', 'judge2'])):\n",
    "        j1, j2 = pair_key\n",
    "        ax.scatter(pair_df['score1'], pair_df['score2'], alpha=0.5)\n",
    "        ax.plot([0, 100], [0, 100], 'k--', alpha=0.3)\n",
    "        ax.set_xlabel(j1.split('/')[-1])\n",
    "        ax.set_ylabel(j2.split('/')[-1])\n",
    "        r, _ = stats.pearsonr(pair_df['score1'], pair_df['score2'])\n",
    "        ax.set_title(f'r = {r:.3f}, N = {len(pair_df)}')\n",
    "        ax.set_xlim(0, 105)\n",
    "        ax.set_ylim(0, 105)\n",
    "\n",
    "    plt.suptitle('Table 3: Inter-Judge Reliability', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('(No scatter plot: no paired data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Table 4 — Recognition Validation (N=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4: Recognition Validation — 3-way comparison\n",
    "print('Table 4: Base vs Enhanced vs Recognition (N=36)')\n",
    "print('=' * 70)\n",
    "\n",
    "df_val = get_run('recognition_validation')\n",
    "print(f'Run N = {len(df_val)}')\n",
    "print(f'Profiles: {df_val[\"profile_name\"].unique().tolist()}')\n",
    "\n",
    "# Derive prompt_type\n",
    "df_val['pt'] = df_val['prompt_type']\n",
    "\n",
    "for pt in ['base', 'enhanced', 'recognition']:\n",
    "    subset = df_val[df_val['pt'] == pt]['overall_score']\n",
    "    print(f'  {pt:12s}  N={len(subset):3d}  Mean={subset.mean():.1f}  SD={subset.std():.1f}')\n",
    "\n",
    "# One-way ANOVA: F(2, 33)\n",
    "groups = [df_val[df_val['pt'] == pt]['overall_score'].values for pt in ['base', 'enhanced', 'recognition']]\n",
    "f_stat, p_val = stats.f_oneway(*groups)\n",
    "print(f'\\nOne-way ANOVA: F(2, {len(df_val) - 3}) = {f_stat:.2f}, p = {p_val:.4f}')\n",
    "\n",
    "# Effect decomposition\n",
    "base_mean = groups[0].mean()\n",
    "enhanced_mean = groups[1].mean()\n",
    "recog_mean = groups[2].mean()\n",
    "\n",
    "total_effect = recog_mean - base_mean\n",
    "engineering_effect = enhanced_mean - base_mean\n",
    "unique_effect = recog_mean - enhanced_mean\n",
    "\n",
    "print(f'\\nEffect Decomposition:')\n",
    "print(f'  Total recognition effect:        +{total_effect:.1f} pts')\n",
    "print(f'  Prompt engineering (enh vs base): +{engineering_effect:.1f} pts ({engineering_effect/total_effect*100:.0f}%)')\n",
    "print(f'  Recognition unique (rec vs enh):  +{unique_effect:.1f} pts ({unique_effect/total_effect*100:.0f}%)')\n",
    "print(f'\\nPaper reports: +20.1 total, +11.4 engineering, +8.7 unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for Table 4\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "order = ['base', 'enhanced', 'recognition']\n",
    "sns.boxplot(data=df_val, x='pt', y='overall_score', order=order, ax=ax, palette='Set2')\n",
    "sns.stripplot(data=df_val, x='pt', y='overall_score', order=order, ax=ax,\n",
    "              color='black', alpha=0.4, size=4)\n",
    "ax.set_xlabel('Prompt Type')\n",
    "ax.set_ylabel('Overall Score (0-100)')\n",
    "ax.set_title(f'Table 4: Recognition Validation (N={len(df_val)}, F={f_stat:.2f}, p={p_val:.4f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Table 5 — Full Factorial ANOVA (N=342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 5: Full 2x2x2 Factorial\n",
    "print('Table 5: Full Factorial ANOVA')\n",
    "print('=' * 70)\n",
    "\n",
    "df_fact = get_run('full_factorial')\n",
    "print(f'Run N = {len(df_fact)}')\n",
    "\n",
    "# 8-cell means table\n",
    "print('\\n8-Cell Means:')\n",
    "cell_stats = df_fact.groupby('profile_name')['overall_score'].agg(['count', 'mean', 'std']).round(1)\n",
    "cell_stats = cell_stats.sort_index()\n",
    "print(cell_stats.to_string())\n",
    "\n",
    "# Derive A/B/C\n",
    "df_fact['A'] = df_fact['factor_A_recognition'].astype(int)\n",
    "df_fact['B'] = df_fact['factor_B_multi'].astype(int)\n",
    "df_fact['C'] = df_fact['factor_C_learner'].astype(int)\n",
    "\n",
    "# Main effects with 95% CIs\n",
    "print('\\nMain Effects:')\n",
    "for factor, label in [('A', 'Recognition'), ('B', 'Multi-agent'), ('C', 'Learner ego-superego')]:\n",
    "    g0 = df_fact[df_fact[factor] == 0]['overall_score']\n",
    "    g1 = df_fact[df_fact[factor] == 1]['overall_score']\n",
    "    effect = g1.mean() - g0.mean()\n",
    "    # Bootstrap CI via formula: SE of difference\n",
    "    se = np.sqrt(g0.var() / len(g0) + g1.var() / len(g1))\n",
    "    ci_lo = effect - 1.96 * se\n",
    "    ci_hi = effect + 1.96 * se\n",
    "    print(f'  {label:30s}  +{effect:.1f} pts  95% CI [{ci_lo:.1f}, {ci_hi:.1f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA: Type II with statsmodels\n",
    "model = ols('overall_score ~ C(A) * C(B) * C(C)', data=df_fact).fit()\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "\n",
    "# Add eta-squared\n",
    "ss_total = anova_table['sum_sq'].sum()\n",
    "anova_table['eta_sq'] = anova_table['sum_sq'] / ss_total\n",
    "\n",
    "print('\\nType II ANOVA:')\n",
    "print(anova_table[['sum_sq', 'df', 'F', 'PR(>F)', 'eta_sq']].round(4).to_string())\n",
    "\n",
    "# Extract key values for concordance\n",
    "f_A = anova_table.loc['C(A)', 'F']\n",
    "eta_A = anova_table.loc['C(A)', 'eta_sq']\n",
    "print(f'\\nKey: F(A) = {f_A:.2f}, η²(A) = {eta_A:.3f}')\n",
    "print(f'Paper reports: F = 43.27, η² = .109')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Recognition x Multi-agent\n",
    "pivot = df_fact.groupby(['factor_A_recognition', 'factor_B_multi'])['overall_score'].mean().unstack()\n",
    "pivot.index = ['Base', 'Recognition']\n",
    "pivot.columns = ['Single', 'Multi']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax, vmin=70, vmax=90)\n",
    "ax.set_title(f'Table 5: Factorial Mean Scores (N={len(df_fact)})')\n",
    "ax.set_ylabel('Factor A: Recognition')\n",
    "ax.set_xlabel('Factor B: Architecture')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Table 6 — A×B Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 6: A x B Interaction\n",
    "print('Table 6: A×B Interaction')\n",
    "print('=' * 70)\n",
    "\n",
    "# ── Nemotron (N=17) ──────────────────────────────────────────────────────\n",
    "df_ab_nem = get_run('ab_nemotron')\n",
    "print(f'\\nNemotron A×B run: N={len(df_ab_nem)}')\n",
    "print(f'Profiles: {df_ab_nem[\"profile_name\"].unique().tolist()}')\n",
    "\n",
    "for pt in df_ab_nem['prompt_type'].unique():\n",
    "    for multi in [False, True]:\n",
    "        subset = df_ab_nem[(df_ab_nem['prompt_type'] == pt) & (df_ab_nem['factor_B_multi'] == multi)]\n",
    "        arch = 'multi' if multi else 'single'\n",
    "        if len(subset) > 0:\n",
    "            print(f'  {pt:12s} {arch:6s}  N={len(subset):3d}  Mean={subset[\"overall_score\"].mean():.1f}')\n",
    "\n",
    "# ── Kimi replication (N=60) ──────────────────────────────────────────────\n",
    "df_ab_kimi = get_run('ab_kimi')\n",
    "print(f'\\nKimi A×B replication: N={len(df_ab_kimi)}')\n",
    "print(f'Profiles: {df_ab_kimi[\"profile_name\"].unique().tolist()}')\n",
    "\n",
    "for pt in sorted(df_ab_kimi['prompt_type'].unique()):\n",
    "    for multi in [False, True]:\n",
    "        subset = df_ab_kimi[(df_ab_kimi['prompt_type'] == pt) & (df_ab_kimi['factor_B_multi'] == multi)]\n",
    "        arch = 'multi' if multi else 'single'\n",
    "        if len(subset) > 0:\n",
    "            print(f'  {pt:12s} {arch:6s}  N={len(subset):3d}  Mean={subset[\"overall_score\"].mean():.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart for A x B interaction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, (key, title) in zip(axes, [('ab_nemotron', 'Nemotron (N=17)'), ('ab_kimi', 'Kimi Replication (N=60)')]):\n",
    "    df_run = get_run(key)\n",
    "    df_run['Architecture'] = df_run['factor_B_multi'].map({True: 'Multi', False: 'Single'})\n",
    "    df_run['Prompt'] = df_run['prompt_type'].str.capitalize()\n",
    "    sns.barplot(data=df_run, x='Prompt', y='overall_score', hue='Architecture', ax=ax, palette='Set1')\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Overall Score')\n",
    "    ax.set_ylim(50, 100)\n",
    "\n",
    "plt.suptitle('Table 6: A×B Interaction', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tables 7–8 — Domain Generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables 7-8: Domain Generalizability\n",
    "print('Tables 7-8: Domain Generalizability')\n",
    "print('=' * 70)\n",
    "\n",
    "# ── Nemotron elementary (N=47) ───────────────────────────────────────────\n",
    "df_dom_nem = get_run('domain_nemotron')\n",
    "print(f'\\nNemotron Elementary: N={len(df_dom_nem)}')\n",
    "\n",
    "for factor, label in [('factor_A_recognition', 'A: Recognition'), ('factor_B_multi', 'B: Multi-agent'), ('factor_C_learner', 'C: Learner')]:\n",
    "    g0 = df_dom_nem[df_dom_nem[factor] == False]['overall_score']\n",
    "    g1 = df_dom_nem[df_dom_nem[factor] == True]['overall_score']\n",
    "    if len(g0) > 0 and len(g1) > 0:\n",
    "        effect = g1.mean() - g0.mean()\n",
    "        print(f'  {label:30s}  +{effect:.1f} pts (N: {len(g0)}+{len(g1)})')\n",
    "\n",
    "print(f'  Overall mean: {df_dom_nem[\"overall_score\"].mean():.1f}')\n",
    "\n",
    "# ── Kimi elementary replication (N=60) ───────────────────────────────────\n",
    "df_dom_kimi = get_run('domain_kimi')\n",
    "print(f'\\nKimi Elementary Replication: N={len(df_dom_kimi)}')\n",
    "\n",
    "base_scores = df_dom_kimi[df_dom_kimi['factor_A_recognition'] == False]['overall_score']\n",
    "recog_scores = df_dom_kimi[df_dom_kimi['factor_A_recognition'] == True]['overall_score']\n",
    "\n",
    "print(f'  Base mean:        {base_scores.mean():.1f} (N={len(base_scores)})')\n",
    "print(f'  Recognition mean: {recog_scores.mean():.1f} (N={len(recog_scores)})')\n",
    "print(f'  Delta:            +{recog_scores.mean() - base_scores.mean():.1f}')\n",
    "\n",
    "d = cohens_d(recog_scores.values, base_scores.values)\n",
    "print(f'  Cohen\\'s d:        {d:.2f}')\n",
    "print(f'  Paper reports: d ≈ 0.61')\n",
    "\n",
    "# Per-scenario breakdown\n",
    "print(f'\\n  Per-scenario effects (Kimi elementary):')\n",
    "for scenario in sorted(df_dom_kimi['scenario_id'].unique()):\n",
    "    sc_base = df_dom_kimi[(df_dom_kimi['scenario_id'] == scenario) & (df_dom_kimi['factor_A_recognition'] == False)]['overall_score']\n",
    "    sc_recog = df_dom_kimi[(df_dom_kimi['scenario_id'] == scenario) & (df_dom_kimi['factor_A_recognition'] == True)]['overall_score']\n",
    "    if len(sc_base) > 0 and len(sc_recog) > 0:\n",
    "        delta = sc_recog.mean() - sc_base.mean()\n",
    "        print(f'    {scenario:40s}  +{delta:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart: Nemotron elementary vs Philosophy (from factorial)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Elementary\n",
    "for ax, (df_run, title) in zip(axes, [\n",
    "    (df_dom_kimi, 'Elementary (Kimi, N=60)'),\n",
    "    (df_fact, 'Philosophy (Kimi Factorial, N=342)')\n",
    "]):\n",
    "    df_plot = df_run.copy()\n",
    "    df_plot['Condition'] = df_plot['factor_A_recognition'].map({True: 'Recognition', False: 'Base'})\n",
    "    sns.barplot(data=df_plot, x='Condition', y='overall_score', ax=ax, palette='Set2',\n",
    "                order=['Base', 'Recognition'])\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Overall Score')\n",
    "    ax.set_ylim(40, 100)\n",
    "\n",
    "plt.suptitle('Tables 7-8: Domain Generalizability', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Table 9 — Superego Rejection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 9: Superego Rejection Patterns\n",
    "print('Table 9: Superego Rejection Patterns')\n",
    "print('=' * 70)\n",
    "\n",
    "log_dir = Path(LOGS_DIR)\n",
    "log_files = sorted(log_dir.glob('*.json'))\n",
    "print(f'Total dialogue log files: {len(log_files)}')\n",
    "\n",
    "# Extract all superego rejections\n",
    "all_rejections = []\n",
    "files_with_rejections = 0\n",
    "total_superego_entries = 0\n",
    "\n",
    "for log_file in log_files:\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            data = json.load(f)\n",
    "        trace = data.get('dialogueTrace', [])\n",
    "        file_has_rejection = False\n",
    "        for entry in trace:\n",
    "            if entry.get('agent') == 'superego':\n",
    "                total_superego_entries += 1\n",
    "                if entry.get('approved') == False:\n",
    "                    verdict = entry.get('verdict', {}) or {}\n",
    "                    feedback = verdict.get('feedback', '')\n",
    "                    all_rejections.append({\n",
    "                        'file': log_file.name,\n",
    "                        'round': entry.get('round'),\n",
    "                        'feedback': feedback,\n",
    "                        'profile': data.get('profileName', ''),\n",
    "                    })\n",
    "                    file_has_rejection = True\n",
    "        if file_has_rejection:\n",
    "            files_with_rejections += 1\n",
    "    except (json.JSONDecodeError, IOError):\n",
    "        continue\n",
    "\n",
    "print(f'Total superego entries: {total_superego_entries}')\n",
    "print(f'Total rejections: {len(all_rejections)}')\n",
    "print(f'Files with at least one rejection: {files_with_rejections}/{len(log_files)}')\n",
    "\n",
    "# Classify feedback text by pattern\n",
    "REJECTION_CATEGORIES = {\n",
    "    'Engagement': re.compile(r'engage|engag|interact|respond.*learner|learner.*position|acknowledge', re.I),\n",
    "    'Specificity': re.compile(r'specific|vague|generic|concrete|lecture.?\\d|reference', re.I),\n",
    "    'Struggle': re.compile(r'struggl|frustr|stuck|difficult|confus|overwhelm|overload', re.I),\n",
    "    'Memory': re.compile(r'previous|history|past|returning|remember|earlier|last time', re.I),\n",
    "    'Level-matching': re.compile(r'level|advanced|beginner|appropriate|scaffold|zone|ZPD|mismatch', re.I),\n",
    "}\n",
    "\n",
    "category_counts = Counter()\n",
    "for rej in all_rejections:\n",
    "    fb = rej['feedback']\n",
    "    for cat, pattern in REJECTION_CATEGORIES.items():\n",
    "        if pattern.search(fb):\n",
    "            category_counts[cat] += 1\n",
    "\n",
    "print(f'\\nRejection Pattern Frequency:')\n",
    "total_rej = len(all_rejections)\n",
    "for cat in ['Engagement', 'Specificity', 'Struggle', 'Memory', 'Level-matching']:\n",
    "    count = category_counts[cat]\n",
    "    pct = count / total_rej * 100 if total_rej > 0 else 0\n",
    "    print(f'  {cat:20s}  {count:4d}  ({pct:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Table 10 — Dimension Effect Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 10: Per-dimension Cohen's d (factorial run, base vs recognition)\n",
    "print('Table 10: Dimension-Level Effect Sizes')\n",
    "print('=' * 70)\n",
    "\n",
    "STANDARD_DIMS = ['score_relevance', 'score_specificity', 'score_pedagogical',\n",
    "                 'score_personalization', 'score_actionability', 'score_tone']\n",
    "DIM_LABELS = ['Relevance', 'Specificity', 'Pedagogical', 'Personalization', 'Actionability', 'Tone']\n",
    "\n",
    "base_mask = df_fact['factor_A_recognition'] == False\n",
    "recog_mask = df_fact['factor_A_recognition'] == True\n",
    "\n",
    "dim_effects = []\n",
    "for dim, label in zip(STANDARD_DIMS, DIM_LABELS):\n",
    "    base_vals = df_fact.loc[base_mask, dim].dropna()\n",
    "    recog_vals = df_fact.loc[recog_mask, dim].dropna()\n",
    "    d = cohens_d(recog_vals.values, base_vals.values)\n",
    "    dim_effects.append({\n",
    "        'Dimension': label,\n",
    "        'Base Mean': base_vals.mean(),\n",
    "        'Recognition Mean': recog_vals.mean(),\n",
    "        'Cohen\\'s d': d,\n",
    "    })\n",
    "    print(f'  {label:20s}  Base={base_vals.mean():.2f}  Recog={recog_vals.mean():.2f}  d={d:.2f}')\n",
    "\n",
    "df_dim_effects = pd.DataFrame(dim_effects).sort_values(\"Cohen's d\", ascending=False)\n",
    "print(f'\\nPaper reports: Personalization d=1.82, Pedagogical d=1.39, Relevance d=1.11, Tone d=1.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot for dimension effect sizes\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df_plot = df_dim_effects.sort_values(\"Cohen's d\")\n",
    "colors = ['#e74c3c' if d > 0.8 else '#f39c12' if d > 0.5 else '#3498db' for d in df_plot[\"Cohen's d\"]]\n",
    "ax.barh(df_plot['Dimension'], df_plot[\"Cohen's d\"], color=colors)\n",
    "ax.axvline(x=0.8, color='red', linestyle='--', alpha=0.5, label='Large (d=0.8)')\n",
    "ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium (d=0.5)')\n",
    "ax.axvline(x=0.2, color='blue', linestyle='--', alpha=0.5, label='Small (d=0.2)')\n",
    "ax.set_xlabel(\"Cohen's d\")\n",
    "ax.set_title('Table 10: Dimension Effect Sizes (Recognition vs Base)')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Table 11 — Standard Dimensions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 11: Re-weighted scores — standard-only vs recognition-only\n",
    "print('Table 11: Standard Dimensions Only Analysis')\n",
    "print('=' * 70)\n",
    "\n",
    "# Raw weights from rubric YAML\n",
    "WEIGHTS = {\n",
    "    'score_relevance': 0.15,\n",
    "    'score_specificity': 0.15,\n",
    "    'score_pedagogical': 0.15,\n",
    "    'score_personalization': 0.10,\n",
    "    'score_actionability': 0.10,\n",
    "    'score_tone': 0.10,\n",
    "    'score_mutual_recognition': 0.083,\n",
    "    'score_dialectical_responsiveness': 0.083,\n",
    "    'score_transformative_potential': 0.083,\n",
    "    'score_memory_integration': 0.05,\n",
    "    'score_tutor_adaptation': 0.05,\n",
    "    'score_learner_growth': 0.05,\n",
    "}\n",
    "\n",
    "STANDARD_WEIGHT_KEYS = ['score_relevance', 'score_specificity', 'score_pedagogical',\n",
    "                         'score_personalization', 'score_actionability', 'score_tone']\n",
    "RECOG_WEIGHT_KEYS = ['score_mutual_recognition', 'score_dialectical_responsiveness',\n",
    "                      'score_transformative_potential', 'score_memory_integration']\n",
    "\n",
    "def compute_weighted_score(row, weight_keys):\n",
    "    \"\"\"Compute weighted score using only specified dimensions, re-normalized.\"\"\"\n",
    "    total_weight = 0\n",
    "    weighted_sum = 0\n",
    "    for key in weight_keys:\n",
    "        val = row.get(key)\n",
    "        if pd.notna(val):\n",
    "            w = WEIGHTS[key]\n",
    "            weighted_sum += val * w\n",
    "            total_weight += w\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return (weighted_sum / total_weight) * 20  # scale to 0-100\n",
    "\n",
    "df_fact['standard_only_score'] = df_fact.apply(lambda r: compute_weighted_score(r, STANDARD_WEIGHT_KEYS), axis=1)\n",
    "df_fact['recognition_only_score'] = df_fact.apply(lambda r: compute_weighted_score(r, RECOG_WEIGHT_KEYS), axis=1)\n",
    "\n",
    "for condition, label in [(False, 'Base (cells 1-4)'), (True, 'Recognition (cells 5-8)')]:\n",
    "    mask = df_fact['factor_A_recognition'] == condition\n",
    "    n = mask.sum()\n",
    "    overall = df_fact.loc[mask, 'overall_score'].mean()\n",
    "    std_only = df_fact.loc[mask, 'standard_only_score'].mean()\n",
    "    rec_only = df_fact.loc[mask, 'recognition_only_score'].mean()\n",
    "    print(f'  {label:25s}  N={n:3d}  Overall={overall:.1f}  Standard-only={std_only:.1f}  Recog-only={rec_only:.1f}')\n",
    "\n",
    "# Differences\n",
    "base_std = df_fact.loc[~df_fact['factor_A_recognition'], 'standard_only_score'].mean()\n",
    "recog_std = df_fact.loc[df_fact['factor_A_recognition'], 'standard_only_score'].mean()\n",
    "print(f'\\n  Standard-only difference: +{recog_std - base_std:.1f}')\n",
    "print(f'  Paper reports: +6.1 pts advantage persists on standard-only dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Table 12 — Multi-Turn Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 12: Multi-turn scenario results\n",
    "print('Table 12: Multi-Turn Scenarios')\n",
    "print('=' * 70)\n",
    "\n",
    "MULTI_TURN_SCENARIOS = [\n",
    "    'misconception_correction_flow',\n",
    "    'mood_frustration_to_breakthrough',\n",
    "    'mutual_transformation_journey',\n",
    "]\n",
    "\n",
    "# Use ALL data across runs (not run-specific per plan)\n",
    "df_scored = df_all[df_all['overall_score'].notna()].copy()\n",
    "\n",
    "mt_results = []\n",
    "for scenario in MULTI_TURN_SCENARIOS:\n",
    "    sc_data = df_scored[df_scored['scenario_id'] == scenario].copy()\n",
    "    sc_data['is_recog'] = sc_data['profile_name'].str.contains('recog', case=False, na=False)\n",
    "\n",
    "    base = sc_data[~sc_data['is_recog']]['overall_score']\n",
    "    recog = sc_data[sc_data['is_recog']]['overall_score']\n",
    "    avg_rounds = sc_data['dialogue_rounds'].mean() if 'dialogue_rounds' in sc_data.columns else np.nan\n",
    "\n",
    "    if len(base) > 1 and len(recog) > 1:\n",
    "        d = cohens_d(recog.values, base.values)\n",
    "        delta = recog.mean() - base.mean()\n",
    "        mt_results.append({\n",
    "            'Scenario': scenario,\n",
    "            'N': len(sc_data),\n",
    "            'Avg Rounds': avg_rounds,\n",
    "            'Base': base.mean(),\n",
    "            'Recognition': recog.mean(),\n",
    "            'Delta': delta,\n",
    "            'Cohen\\'s d': d,\n",
    "        })\n",
    "        print(f'  {scenario:40s}  N={len(sc_data):4d}  Base={base.mean():.1f}  Recog={recog.mean():.1f}  Δ={delta:+.1f}  d={d:.2f}')\n",
    "\n",
    "print(f'\\nPaper reports: d = 0.85 / 0.59 / 0.78')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart with error bars\n",
    "if mt_results:\n",
    "    df_mt = pd.DataFrame(mt_results)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(df_mt))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, df_mt['Base'], width, label='Base', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, df_mt['Recognition'], width, label='Recognition', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlabel('Scenario')\n",
    "    ax.set_ylabel('Overall Score')\n",
    "    ax.set_title('Table 12: Multi-Turn Scenarios (Base vs Recognition)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([s.replace('_', '\\n') for s in df_mt['Scenario']], fontsize=9)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "    # Annotate Cohen's d\n",
    "    for i, row in df_mt.iterrows():\n",
    "        ax.annotate(f'd={row[\"Cohen\\'s d\"]:.2f}', (i, max(row['Base'], row['Recognition']) + 3),\n",
    "                    ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Table 13 — Bilateral Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 13: Bilateral Transformation Metrics\n",
    "print('Table 13: Bilateral Transformation Metrics')\n",
    "print('=' * 70)\n",
    "\n",
    "log_dir = Path(LOGS_DIR)\n",
    "transformation_data = []\n",
    "\n",
    "for log_file in sorted(log_dir.glob('*.json')):\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data.get('isMultiTurn') or not data.get('transformationAnalysis'):\n",
    "            continue\n",
    "\n",
    "        ta = data['transformationAnalysis']\n",
    "        tp = ta.get('turnProgression', {})\n",
    "        dtr = ta.get('dialogueTraceReport', {})\n",
    "        oa = dtr.get('overallAssessment', {})\n",
    "\n",
    "        profile = data.get('profileName', '')\n",
    "        is_recog = 'recog' in str(profile).lower()\n",
    "\n",
    "        transformation_data.append({\n",
    "            'file': log_file.name,\n",
    "            'profile': profile,\n",
    "            'is_recog': is_recog,\n",
    "            'adaptationIndex': tp.get('adaptationIndex'),\n",
    "            'learnerGrowthIndex': tp.get('learnerGrowthIndex'),\n",
    "            'bilateralTransformationIndex': tp.get('bilateralTransformationIndex'),\n",
    "            'transformationQuality': oa.get('transformationQuality'),\n",
    "        })\n",
    "    except (json.JSONDecodeError, IOError):\n",
    "        continue\n",
    "\n",
    "df_trans = pd.DataFrame(transformation_data)\n",
    "print(f'Dialogue files with transformationAnalysis: {len(df_trans)}')\n",
    "\n",
    "if len(df_trans) > 0:\n",
    "    metrics = ['adaptationIndex', 'learnerGrowthIndex', 'bilateralTransformationIndex', 'transformationQuality']\n",
    "    metric_labels = ['Tutor Adaptation Index', 'Learner Growth Index', 'Bilateral Transformation Index', 'Transformation Quality']\n",
    "\n",
    "    for is_recog, label in [(False, 'Base'), (True, 'Recognition')]:\n",
    "        subset = df_trans[df_trans['is_recog'] == is_recog]\n",
    "        print(f'\\n  {label} (N={len(subset)}):')\n",
    "        for metric, mlabel in zip(metrics, metric_labels):\n",
    "            vals = subset[metric].dropna()\n",
    "            if len(vals) > 0:\n",
    "                print(f'    {mlabel:40s}  Mean={vals.mean():.3f}  SD={vals.std():.3f}')\n",
    "\n",
    "    # Deltas\n",
    "    print(f'\\n  Deltas (Recognition - Base):')\n",
    "    for metric, mlabel in zip(metrics, metric_labels):\n",
    "        base_m = df_trans.loc[~df_trans['is_recog'], metric].dropna().mean()\n",
    "        recog_m = df_trans.loc[df_trans['is_recog'], metric].dropna().mean()\n",
    "        if pd.notna(base_m) and pd.notna(recog_m):\n",
    "            print(f'    {mlabel:40s}  Δ = {recog_m - base_m:+.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Tables 14–15 — Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables 14-15: Lexical Analysis\n",
    "print('Tables 14-15: Lexical Analysis')\n",
    "print('=' * 70)\n",
    "\n",
    "# Stopwords matching scripts/qualitative-analysis.js\n",
    "STOPWORDS = set([\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an',\n",
    "    'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been',\n",
    "    'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does',\n",
    "    \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for',\n",
    "    'from', 'further', 'get', 'got', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her',\n",
    "    'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into',\n",
    "    'is', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'let', \"let's\",\n",
    "    'like', 'make', 'me', 'might', 'more', 'most', \"mustn't\", 'my',\n",
    "    'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "    'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "    'really', 'right', 'same', \"shan't\", 'she', \"she'd\", \"she'll\",\n",
    "    \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'take', 'than',\n",
    "    'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "    \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 'us', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'well', 'were', \"weren't\", 'what', \"what's\", 'when',\n",
    "    \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\",\n",
    "    'whom', 'why', \"why's\", 'will', 'with', \"won't\", 'would', \"wouldn't\",\n",
    "    'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'also', 'been', 'being', 'come', 'even',\n",
    "    'first', 'going', 'good', 'know', 'look', 'much', 'need', 'new', 'now',\n",
    "    'one', 'people', 'really', 'see', 'think', 'thing', 'time', 'two',\n",
    "    'use', 'want', 'way', 'work', 'would', 'year', 'back', 'long', 'say',\n",
    "    'still', 'tell', 'try', 'give', 'go', 'help', 'keep', 'many',\n",
    "    'may', 'put', 'seem', 'show', 'start', 'turn', 'big', 'end', 'set',\n",
    "    'll', 've', 're', 's', 't', 'd', 'don', 'isn', 'doesn', 'didn',\n",
    "    'won', 'can', 'couldn', 'shouldn', 'wasn', 'weren', 'hasn', 'haven',\n",
    "    'hadn', 'aren', 'mustn', 'shan', 'ain',\n",
    "])\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize matching JS implementation.\"\"\"\n",
    "    text = re.sub(r\"[^a-z'\\s-]\", ' ', text.lower())\n",
    "    return [w for w in text.split() if len(w) > 1]\n",
    "\n",
    "def tokenize_filtered(text):\n",
    "    return [w for w in tokenize(text) if w not in STOPWORDS]\n",
    "\n",
    "def count_sentences(text):\n",
    "    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    return max(len(sentences), 1)\n",
    "\n",
    "def extract_messages(suggestions_json):\n",
    "    \"\"\"Extract message text from suggestions JSON.\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(suggestions_json)\n",
    "        if not isinstance(parsed, list):\n",
    "            return []\n",
    "        return [s.get('message', '') for s in parsed if s.get('message')]\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# Build corpora: cells 1-4 (base) vs cells 5-8 (recognition)\n",
    "BASE_CELLS = ['cell_1_base_single_unified', 'cell_2_base_single_psycho',\n",
    "              'cell_3_base_multi_unified', 'cell_4_base_multi_psycho']\n",
    "RECOG_CELLS = ['cell_5_recog_single_unified', 'cell_6_recog_single_psycho',\n",
    "               'cell_7_recog_multi_unified', 'cell_8_recog_multi_psycho']\n",
    "\n",
    "df_corpus = df_all[df_all['suggestions'].notna() & df_all['profile_name'].isin(BASE_CELLS + RECOG_CELLS)].copy()\n",
    "\n",
    "base_messages = []\n",
    "recog_messages = []\n",
    "\n",
    "for _, row in df_corpus.iterrows():\n",
    "    msgs = extract_messages(row['suggestions'])\n",
    "    if row['profile_name'] in BASE_CELLS:\n",
    "        base_messages.extend(msgs)\n",
    "    else:\n",
    "        recog_messages.extend(msgs)\n",
    "\n",
    "print(f'Base messages: {len(base_messages)}')\n",
    "print(f'Recognition messages: {len(recog_messages)}')\n",
    "\n",
    "base_text = ' '.join(base_messages)\n",
    "recog_text = ' '.join(recog_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 14: Lexical diversity metrics\n",
    "print('Table 14: Lexical Diversity Metrics')\n",
    "print('=' * 70)\n",
    "\n",
    "def compute_lexical_metrics(text, label):\n",
    "    all_tokens = tokenize(text)\n",
    "    types = set(all_tokens)\n",
    "    sentences = count_sentences(text)\n",
    "    ttr = len(types) / len(all_tokens) if all_tokens else 0\n",
    "    mean_word_len = np.mean([len(w) for w in all_tokens]) if all_tokens else 0\n",
    "    mean_sent_len = len(all_tokens) / sentences if sentences else 0\n",
    "    return {\n",
    "        'label': label,\n",
    "        'tokens': len(all_tokens),\n",
    "        'vocabulary': len(types),\n",
    "        'ttr': ttr,\n",
    "        'mean_word_length': mean_word_len,\n",
    "        'mean_sentence_length': mean_sent_len,\n",
    "    }\n",
    "\n",
    "lex_base = compute_lexical_metrics(base_text, 'Base (message)')\n",
    "lex_recog = compute_lexical_metrics(recog_text, 'Recognition (message)')\n",
    "\n",
    "df_lex = pd.DataFrame([lex_base, lex_recog]).set_index('label')\n",
    "print(df_lex.round(4).to_string())\n",
    "\n",
    "print(f'\\nPaper reports: TTR 0.039/0.044, Vocab 2319/3689')\n",
    "print(f'Computed:      TTR {lex_base[\"ttr\"]:.3f}/{lex_recog[\"ttr\"]:.3f}, Vocab {lex_base[\"vocabulary\"]}/{lex_recog[\"vocabulary\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 15: Differential word frequency\n",
    "print('Table 15: Differential Word Frequency')\n",
    "print('=' * 70)\n",
    "\n",
    "base_tokens_filtered = tokenize_filtered(base_text)\n",
    "recog_tokens_filtered = tokenize_filtered(recog_text)\n",
    "\n",
    "base_freq = Counter(base_tokens_filtered)\n",
    "recog_freq = Counter(recog_tokens_filtered)\n",
    "\n",
    "base_total = len(base_tokens_filtered)\n",
    "recog_total = len(recog_tokens_filtered)\n",
    "\n",
    "all_words = set(base_freq.keys()) | set(recog_freq.keys())\n",
    "\n",
    "differential = []\n",
    "for word in all_words:\n",
    "    bc = base_freq.get(word, 0)\n",
    "    rc = recog_freq.get(word, 0)\n",
    "    if bc + rc < 10:\n",
    "        continue\n",
    "    base_rate = bc / base_total if base_total else 0\n",
    "    recog_rate = rc / recog_total if recog_total else 0\n",
    "    if base_rate > 0 and recog_rate > 0:\n",
    "        ratio = recog_rate / base_rate\n",
    "    elif recog_rate > 0:\n",
    "        ratio = float('inf')\n",
    "    else:\n",
    "        ratio = 0\n",
    "    differential.append({\n",
    "        'word': word, 'base_count': bc, 'recog_count': rc,\n",
    "        'base_rate': base_rate, 'recog_rate': recog_rate, 'ratio': ratio\n",
    "    })\n",
    "\n",
    "df_diff = pd.DataFrame(differential)\n",
    "\n",
    "# Recognition-skewed (top 15)\n",
    "recog_skewed = df_diff[(df_diff['ratio'] != float('inf')) & (df_diff['ratio'] > 1) & (df_diff['recog_count'] >= 10)]\n",
    "recog_skewed = recog_skewed.sort_values('ratio', ascending=False).head(15)\n",
    "\n",
    "print('\\nRecognition-Skewed Words (top 15):')\n",
    "for _, row in recog_skewed.iterrows():\n",
    "    print(f'  {row[\"word\"]:20s}  base={row[\"base_count\"]:4d}  recog={row[\"recog_count\"]:4d}  ratio={row[\"ratio\"]:.1f}x')\n",
    "\n",
    "# Base-skewed (top 15)\n",
    "base_skewed = df_diff[(df_diff['ratio'] > 0) & (df_diff['ratio'] < 1) & (df_diff['base_count'] >= 10)]\n",
    "base_skewed = base_skewed.sort_values('ratio').head(15)\n",
    "\n",
    "print('\\nBase-Skewed Words (top 15):')\n",
    "for _, row in base_skewed.iterrows():\n",
    "    print(f'  {row[\"word\"]:20s}  base={row[\"base_count\"]:4d}  recog={row[\"recog_count\"]:4d}  ratio={row[\"ratio\"]:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Table 16 — Thematic Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 16: Thematic Coding with Chi-Square\n",
    "print('Table 16: Thematic Code Frequency by Condition')\n",
    "print('=' * 70)\n",
    "\n",
    "# 6 regex categories matching scripts/qualitative-analysis.js exactly\n",
    "THEMATIC_CATEGORIES = {\n",
    "    'engagement': {\n",
    "        'label': 'Engagement markers',\n",
    "        'patterns': [\n",
    "            r'your insight', r'building on your', r'your question', r'your point',\n",
    "            r'your observation', r'your analysis', r'your argument', r'your critique',\n",
    "            r\"you've (raised|identified|highlighted|noticed|pointed out)\",\n",
    "            r\"you're (asking|raising|pushing|exploring|getting at)\",\n",
    "        ],\n",
    "    },\n",
    "    'transformation': {\n",
    "        'label': 'Transformation language',\n",
    "        'patterns': [\n",
    "            r'reconsidering', r'that changes (how I|my)', r\"I hadn't (thought|considered)\",\n",
    "            r'revising (my|the)', r'let me (revise|adjust|rethink)',\n",
    "            r\"you've (helped|pushed|made) me\",\n",
    "            r'your .{1,20} (complicates|enriches|changes)',\n",
    "            r'shifts? (my|the|our) (understanding|framing|approach)',\n",
    "        ],\n",
    "    },\n",
    "    'struggle_honoring': {\n",
    "        'label': 'Struggle-honoring',\n",
    "        'patterns': [\n",
    "            r'wrestling with', r'productive confusion', r'working through',\n",
    "            r'grappling with', r'sitting with (the|this)',\n",
    "            r'tension (between|here|you)', r'difficulty (is|here)',\n",
    "            r'struggle (with|is|here)', r'not (easy|simple|straightforward)',\n",
    "        ],\n",
    "    },\n",
    "    'learner_as_subject': {\n",
    "        'label': 'Learner-as-subject framing',\n",
    "        'patterns': [\n",
    "            r'your interpretation', r'your analysis', r'your understanding',\n",
    "            r\"you're grappling with\", r'your perspective', r'your framework',\n",
    "            r'your reading', r\"what you're (doing|building|developing|constructing)\",\n",
    "            r'your (intellectual|philosophical|analytical)',\n",
    "        ],\n",
    "    },\n",
    "    'directive': {\n",
    "        'label': 'Directive framing',\n",
    "        'patterns': [\n",
    "            r'you should', r'you need to', r'you must',\n",
    "            r'the correct (answer|approach|way)', r'the answer is',\n",
    "            r'let me explain', r\"here's what\",\n",
    "            r'make sure (to|you)', r'first,? you',\n",
    "        ],\n",
    "    },\n",
    "    'generic': {\n",
    "        'label': 'Generic/placeholder',\n",
    "        'patterns': [\n",
    "            r'foundational', r'key concepts', r'learning objectives',\n",
    "            r'knowledge base', r'solid foundation', r'core concepts',\n",
    "            r'build (a|your) (solid|strong)',\n",
    "            r'comprehensive (understanding|overview|review)',\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "def count_matches(text, patterns):\n",
    "    \"\"\"Count total regex matches in text.\"\"\"\n",
    "    total = 0\n",
    "    for pat in patterns:\n",
    "        total += len(re.findall(pat, text, re.IGNORECASE))\n",
    "    return total\n",
    "\n",
    "def count_response_presence(messages, patterns):\n",
    "    \"\"\"Count how many messages contain at least one match.\"\"\"\n",
    "    present = 0\n",
    "    for msg in messages:\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, msg, re.IGNORECASE):\n",
    "                present += 1\n",
    "                break\n",
    "    return present\n",
    "\n",
    "base_word_count = len(tokenize(base_text))\n",
    "recog_word_count = len(tokenize(recog_text))\n",
    "\n",
    "thematic_results = []\n",
    "\n",
    "for cat_key, config in THEMATIC_CATEGORIES.items():\n",
    "    patterns = config['patterns']\n",
    "\n",
    "    base_raw = count_matches(base_text, patterns)\n",
    "    recog_raw = count_matches(recog_text, patterns)\n",
    "\n",
    "    base_per1000 = (base_raw / base_word_count) * 1000 if base_word_count else 0\n",
    "    recog_per1000 = (recog_raw / recog_word_count) * 1000 if recog_word_count else 0\n",
    "    ratio = recog_per1000 / base_per1000 if base_per1000 > 0 else (float('inf') if recog_per1000 > 0 else 1.0)\n",
    "\n",
    "    # Chi-square: response-level presence/absence with Yates correction\n",
    "    base_present = count_response_presence(base_messages, patterns)\n",
    "    base_absent = len(base_messages) - base_present\n",
    "    recog_present = count_response_presence(recog_messages, patterns)\n",
    "    recog_absent = len(recog_messages) - recog_present\n",
    "\n",
    "    # 2x2 contingency table\n",
    "    observed = np.array([[base_present, base_absent], [recog_present, recog_absent]])\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(observed, correction=True)\n",
    "\n",
    "    sig = '*' if p_val < 0.05 else ''\n",
    "\n",
    "    thematic_results.append({\n",
    "        'Category': config['label'],\n",
    "        'Base (per 1000)': base_per1000,\n",
    "        'Recog (per 1000)': recog_per1000,\n",
    "        'Ratio': ratio,\n",
    "        'chi2': chi2,\n",
    "        'p': p_val,\n",
    "        'Sig': sig,\n",
    "    })\n",
    "\n",
    "    print(f'  {config[\"label\"]:30s}  Base={base_per1000:.1f}  Recog={recog_per1000:.1f}  '\n",
    "          f'Ratio={ratio:.2f}x  χ²={chi2:.2f}  p={p_val:.4f} {sig}')\n",
    "\n",
    "print(f'\\nPaper reports: χ² = 141.90 (struggle), 69.85 (engagement), 93.15 (generic)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart for thematic coding\n",
    "if thematic_results:\n",
    "    df_theme = pd.DataFrame(thematic_results)\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    x = np.arange(len(df_theme))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, df_theme['Base (per 1000)'], width, label='Base', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, df_theme['Recog (per 1000)'], width, label='Recognition', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlabel('Thematic Category')\n",
    "    ax.set_ylabel('Occurrences per 1000 words')\n",
    "    ax.set_title('Table 16: Thematic Coding by Condition')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([r['Category'] for r in thematic_results], rotation=30, ha='right', fontsize=9)\n",
    "    ax.legend()\n",
    "\n",
    "    # Annotate significance\n",
    "    for i, row in df_theme.iterrows():\n",
    "        if row['Sig'] == '*':\n",
    "            y_max = max(row['Base (per 1000)'], row['Recog (per 1000)'])\n",
    "            ax.annotate('*', (i, y_max + 0.3), ha='center', fontsize=16, fontweight='bold', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Table 17 — Cost-Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 17: Cost-Benefit by Domain and Architecture\n",
    "print('Table 17: Cost-Benefit Analysis')\n",
    "print('=' * 70)\n",
    "\n",
    "# Philosophy: factorial cells 1,3 (base single/multi) and 5,7 (recog single/multi)\n",
    "phil_cells_single = ['cell_1_base_single_unified', 'cell_5_recog_single_unified']\n",
    "phil_cells_multi = ['cell_3_base_multi_unified', 'cell_7_recog_multi_unified']\n",
    "\n",
    "phil_single = df_fact[df_fact['profile_name'].isin(phil_cells_single)]\n",
    "phil_multi = df_fact[df_fact['profile_name'].isin(phil_cells_multi)]\n",
    "\n",
    "# Elementary: domain nemotron run\n",
    "df_elem = get_run('domain_nemotron')\n",
    "elem_single = df_elem[df_elem['factor_B_multi'] == False]\n",
    "elem_multi = df_elem[df_elem['factor_B_multi'] == True]\n",
    "\n",
    "cost_data = []\n",
    "for domain, single_df, multi_df in [\n",
    "    ('Philosophy', phil_single, phil_multi),\n",
    "    ('Elementary', elem_single, elem_multi),\n",
    "]:\n",
    "    for arch, arch_df in [('Single-agent', single_df), ('Multi-agent', multi_df)]:\n",
    "        avg_score = arch_df['overall_score'].mean()\n",
    "        latency = arch_df['latency_ms'].mean() / 1000 if 'latency_ms' in arch_df.columns and arch_df['latency_ms'].notna().any() else np.nan\n",
    "        cost_data.append({\n",
    "            'Domain': domain,\n",
    "            'Architecture': arch,\n",
    "            'Avg Score': avg_score,\n",
    "            'Latency (s)': latency,\n",
    "            'N': len(arch_df),\n",
    "        })\n",
    "\n",
    "df_cost = pd.DataFrame(cost_data)\n",
    "print(df_cost.round(1).to_string(index=False))\n",
    "\n",
    "# Compute deltas\n",
    "for domain in ['Philosophy', 'Elementary']:\n",
    "    single = df_cost[(df_cost['Domain'] == domain) & (df_cost['Architecture'] == 'Single-agent')].iloc[0]\n",
    "    multi = df_cost[(df_cost['Domain'] == domain) & (df_cost['Architecture'] == 'Multi-agent')].iloc[0]\n",
    "    score_delta = multi['Avg Score'] - single['Avg Score']\n",
    "    latency_mult = multi['Latency (s)'] / single['Latency (s)'] if single['Latency (s)'] > 0 else np.nan\n",
    "    print(f'\\n  {domain}: Score Δ = {score_delta:+.1f}, Latency = {latency_mult:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Transcript Excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript excerpts: 3 high-contrast pairs\n",
    "print('Transcript Excerpts: High-Contrast Pairs')\n",
    "print('=' * 70)\n",
    "\n",
    "PAIR_SCENARIOS = ['struggling_learner', 'recognition_seeking_learner', 'adversarial_tester']\n",
    "\n",
    "df_with_msgs = df_all[\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['suggestions'].notna() &\n",
    "    (df_all['overall_score'] > 0)\n",
    "].copy()\n",
    "\n",
    "for scenario in PAIR_SCENARIOS:\n",
    "    sc = df_with_msgs[df_with_msgs['scenario_id'] == scenario]\n",
    "\n",
    "    # Best recognition\n",
    "    recog_sc = sc[sc['profile_name'].isin(RECOG_CELLS)]\n",
    "    base_sc = sc[sc['profile_name'].isin(BASE_CELLS)]\n",
    "\n",
    "    if len(recog_sc) == 0 or len(base_sc) == 0:\n",
    "        continue\n",
    "\n",
    "    best_recog = recog_sc.loc[recog_sc['overall_score'].idxmax()]\n",
    "    worst_base = base_sc.loc[base_sc['overall_score'].idxmin()]\n",
    "\n",
    "    print(f'\\n--- {scenario} ---')\n",
    "    print(f'  Recognition (id={best_recog[\"id\"]}, {best_recog[\"profile_name\"]}, score={best_recog[\"overall_score\"]:.1f}):')\n",
    "\n",
    "    try:\n",
    "        recog_msgs = json.loads(best_recog['suggestions'])\n",
    "        for s in recog_msgs[:2]:\n",
    "            msg = s.get('message', '')[:200]\n",
    "            print(f'    \"{msg}...\"' if len(s.get('message', '')) > 200 else f'    \"{msg}\"')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print('    (could not parse)')\n",
    "\n",
    "    print(f'  Base (id={worst_base[\"id\"]}, {worst_base[\"profile_name\"]}, score={worst_base[\"overall_score\"]:.1f}):')\n",
    "    try:\n",
    "        base_msgs = json.loads(worst_base['suggestions'])\n",
    "        for s in base_msgs[:2]:\n",
    "            msg = s.get('message', '')[:200]\n",
    "            print(f'    \"{msg}...\"' if len(s.get('message', '')) > 200 else f'    \"{msg}\"')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print('    (could not parse)')\n",
    "\n",
    "    print(f'  Score gap: {best_recog[\"overall_score\"] - worst_base[\"overall_score\"]:.1f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Concordance Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance check: compare computed vs paper-reported values\n",
    "print('CONCORDANCE CHECK')\n",
    "print('=' * 70)\n",
    "print(f'{\"Statistic\":45s} {\"Paper\":>12s} {\"Computed\":>12s} {\"Match\":>8s}')\n",
    "print('-' * 80)\n",
    "\n",
    "def check(label, paper_val, computed_val, tolerance=0.5):\n",
    "    \"\"\"Check if computed value matches paper within tolerance.\"\"\"\n",
    "    if pd.isna(computed_val):\n",
    "        match = '?'\n",
    "    elif abs(computed_val - paper_val) <= tolerance:\n",
    "        match = 'YES'\n",
    "    else:\n",
    "        match = f'NO ({computed_val - paper_val:+.2f})'\n",
    "    print(f'{label:45s} {paper_val:12.2f} {computed_val:12.2f} {match:>8s}')\n",
    "\n",
    "# Sample sizes\n",
    "print('\\n-- Sample Sizes --')\n",
    "total_scored = sum(get_run(k).shape[0] for k in RUN_IDS)\n",
    "check('Total scored (6 primary runs)', 562, total_scored, 5)\n",
    "\n",
    "for key, expected in [('recognition_validation', 36), ('full_factorial', 342), ('ab_nemotron', 17),\n",
    "                       ('ab_kimi', 60), ('domain_nemotron', 47), ('domain_kimi', 60)]:\n",
    "    check(f'  {key} N', expected, get_run(key).shape[0], 1)\n",
    "\n",
    "# Table 4 key values\n",
    "print('\\n-- Table 4: Recognition Validation --')\n",
    "df_v = get_run('recognition_validation')\n",
    "base_m = df_v[df_v['prompt_type'] == 'base']['overall_score'].mean()\n",
    "enh_m = df_v[df_v['prompt_type'] == 'enhanced']['overall_score'].mean()\n",
    "rec_m = df_v[df_v['prompt_type'] == 'recognition']['overall_score'].mean()\n",
    "check('Recognition unique effect (+8.7)', 8.7, rec_m - enh_m, 1.0)\n",
    "check('Total effect (+20.1)', 20.1, rec_m - base_m, 1.0)\n",
    "check('Engineering effect (+11.4)', 11.4, enh_m - base_m, 1.0)\n",
    "\n",
    "# Table 5 key values\n",
    "print('\\n-- Table 5: Factorial ANOVA --')\n",
    "check('F(A: Recognition) = 43.27', 43.27, f_A, 2.0)\n",
    "check('eta-sq(A) = .109', 0.109, eta_A, 0.01)\n",
    "\n",
    "# Multi-turn Cohen's d\n",
    "print('\\n-- Table 12: Multi-Turn Cohen\\'s d --')\n",
    "for scenario, expected_d in [('misconception_correction_flow', 0.85),\n",
    "                               ('mood_frustration_to_breakthrough', 0.59),\n",
    "                               ('mutual_transformation_journey', 0.78)]:\n",
    "    sc_data = df_scored[df_scored['scenario_id'] == scenario].copy()\n",
    "    sc_data['is_recog'] = sc_data['profile_name'].str.contains('recog', case=False, na=False)\n",
    "    base_s = sc_data[~sc_data['is_recog']]['overall_score']\n",
    "    recog_s = sc_data[sc_data['is_recog']]['overall_score']\n",
    "    if len(base_s) > 1 and len(recog_s) > 1:\n",
    "        d_val = cohens_d(recog_s.values, base_s.values)\n",
    "        check(f'd({scenario[:25]})', expected_d, d_val, 0.15)\n",
    "\n",
    "# Thematic chi-squares\n",
    "print('\\n-- Table 16: Thematic Chi-Square --')\n",
    "chi_lookup = {r['Category']: r for r in thematic_results}\n",
    "for label, expected in [('Struggle-honoring', 141.90), ('Engagement markers', 69.85), ('Generic/placeholder', 93.15)]:\n",
    "    if label in chi_lookup:\n",
    "        check(f'chi2({label})', expected, chi_lookup[label]['chi2'], 10.0)\n",
    "\n",
    "# Lexical metrics\n",
    "print('\\n-- Table 14: Lexical Metrics --')\n",
    "check('Base TTR', 0.039, lex_base['ttr'], 0.005)\n",
    "check('Recog TTR', 0.044, lex_recog['ttr'], 0.005)\n",
    "check('Base vocab', 2319, lex_base['vocabulary'], 50)\n",
    "check('Recog vocab', 3689, lex_recog['vocabulary'], 50)\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('Concordance check complete.')\n",
    "print('Values marked YES match within tolerance. Values marked NO show the discrepancy.')\n",
    "print('Small discrepancies may arise from floating-point differences, rounding,\\n'\n",
    "      'or slight DB content changes between paper finalization and current state.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "conn.close()\n",
    "print('Database connection closed. Notebook complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}