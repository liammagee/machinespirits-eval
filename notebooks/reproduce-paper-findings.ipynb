{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility Notebook: Recognition Theory in AI Tutoring (v2.3.10)\n\nThis notebook independently reproduces key statistical findings from the paper,\nusing only the raw SQLite database and dialogue log files.\n\n**Paper version**: v2.3.10 (February 2026)\n\n**Data sources:**\n- `../data/evaluations.db` \u2014 7,172+ scored evaluation rows across 117 runs\n- `../logs/tutor-dialogues/*.json` \u2014 dialogue log files\n\n**Scope**: 31 key evaluations, N=3,292 primary scored responses\n\n**Organization:**\n- **Phase 1** (\u00a7\u00a71\u201317): Core factorial, recognition validation, domain generalizability, lexical/thematic analysis\n- **Phase 2** (\u00a7\u00a718\u201328): Memory isolation, active control, multi-model probe, mechanism architecture, cross-judge replication, blinded assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "source": "# \u2500\u2500 Data availability check \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# The evaluation database and dialogue logs are distributed separately\n# as a GitHub Release artifact (~19 MB compressed).\n\nimport os, sys\nfrom pathlib import Path\n\nDB_PATH = Path('../data/evaluations.db')\nLOGS_DIR = Path('../logs/tutor-dialogues')\n\nmissing = []\nif not DB_PATH.exists():\n    missing.append(f'  - Database: {DB_PATH}')\nif not LOGS_DIR.exists() or not any(LOGS_DIR.glob('*.json')):\n    missing.append(f'  - Dialogue logs: {LOGS_DIR}/')\n\nif missing:\n    print('DATA NOT FOUND \u2014 this notebook requires the evaluation dataset.\\n')\n    print('Missing:')\n    print('\\n'.join(missing))\n    print('\\nTo obtain the data:\\n')\n    print('  1. Download machinespirits-eval-data-v0.2.0.tar.gz from:')\n    print('     https://github.com/liammagee/machinespirits-eval/releases/tag/v0.2.0\\n')\n    print('  2. Extract from the repository root:')\n    print('     tar xzf machinespirits-eval-data-v0.2.0.tar.gz\\n')\n    print('This will populate data/evaluations.db and logs/tutor-dialogues/.')\n    print('Then re-run this cell and continue.')\n    # Uncomment the next line to halt execution if data is missing:\n    # sys.exit(1)\nelse:\n    print(f'Database found: {DB_PATH} ({DB_PATH.stat().st_size / 1e6:.1f} MB)')\n    n_logs = len(list(LOGS_DIR.glob('*.json')))\n    print(f'Dialogue logs found: {n_logs} files in {LOGS_DIR}/')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid', font_scale=1.1)\n",
    "%matplotlib inline\n",
    "\n",
    "print('All imports successful.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Database connection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "DB_PATH = '../data/evaluations.db'\n",
    "LOGS_DIR = '../logs/tutor-dialogues'\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "conn.row_factory = sqlite3.Row\n",
    "\n",
    "df_all = pd.read_sql_query(\"\"\"\n",
    "    SELECT *\n",
    "    FROM evaluation_results\n",
    "    WHERE success = 1\n",
    "\"\"\", conn)\n",
    "\n",
    "print(f'Total rows loaded: {len(df_all)}')\n",
    "print(f'Rows with overall_score: {df_all[\"overall_score\"].notna().sum()}')\n",
    "print(f'Distinct run_ids: {df_all[\"run_id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Run ID dictionary \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# All 31 key evaluations from the paper (Table 2)\nRUN_IDS = {\n    # Phase 1: Core experiments\n    'recognition_validation':     'eval-2026-02-03-86b159cd',\n    'full_factorial':             'eval-2026-02-03-f5d4dd93',\n    'factorial_cells_6_8':        'eval-2026-02-06-a933d745',\n    'ab_kimi':                    'eval-2026-02-05-10b344fb',\n    'domain_kimi':                'eval-2026-02-05-e87f452d',\n    'memory_isolation_1':         'eval-2026-02-06-81f2d5a1',\n    'memory_isolation_2':         'eval-2026-02-06-ac9ea8f5',\n    'active_control':             'eval-2026-02-06-a9ae06ee',\n    'bilateral':                  'eval-2026-02-07-b6d75e87',\n    'hardwired_rules':            'eval-2026-02-08-65a6718f',\n    'impasse':                    'eval-2026-02-08-f896275d',\n    # Multi-model probes\n    'probe_nemotron':             'eval-2026-02-07-722087ac',\n    'probe_deepseek':             'eval-2026-02-07-70ef73a3',\n    'probe_glm':                  'eval-2026-02-07-6b3e6565',\n    'probe_haiku':                'eval-2026-02-07-6ead24c7',\n    # Dynamic rewrite\n    'rewrite_1':                  'eval-2026-02-05-daf60f79',\n    'rewrite_2':                  'eval-2026-02-05-49bb2017',\n    'rewrite_3':                  'eval-2026-02-05-12aebedb',\n    # Phase 2: Dialectical modulation\n    'modulation_standard_1':      'eval-2026-02-11-35c53e99',\n    'modulation_standard_2':      'eval-2026-02-11-5f6d51f5',\n    'modulation_multiturn':       'eval-2026-02-11-a54235ea',\n    'self_reflect':               'eval-2026-02-13-8d40e086',\n    # Phase 2: Mechanism architecture\n    'mechanism_scripted_haiku':   'eval-2026-02-14-e0e3a622',\n    'mechanism_scripted_nemotron':'eval-2026-02-14-49b33fdd',\n    'dynamic_learner':            'eval-2026-02-14-6c033830',\n    'mechanism_headtohead':       'eval-2026-02-14-a2b2717c',\n    'dynamic_base_mechanisms':    'eval-2026-02-15-664073ab',\n    'self_reflect_nemotron':      'eval-2026-02-14-559d854b',\n    'cognitive_prosthesis':       'eval-2026-02-14-50487df7',\n    # Phase 2: Prompt elaboration baseline\n    'naive_baseline_haiku':       'eval-2026-02-17-deee5fd6',\n    'naive_baseline_kimi':        'eval-2026-02-17-27d7b4e3',\n}\n\n# Expected N per run from the paper (Table 2)\nEXPECTED_N = {\n    'recognition_validation': 36,\n    'full_factorial': 262,\n    'factorial_cells_6_8': 88,\n    'ab_kimi': 60,\n    'domain_kimi': 60,\n    'memory_isolation_1': 60,\n    'memory_isolation_2': 60,\n    'active_control': 118,\n    'bilateral': 118,\n    'hardwired_rules': 72,\n    'impasse': 24,\n    'probe_nemotron': 119,\n    'probe_deepseek': 120,\n    'probe_glm': 117,\n    'probe_haiku': 120,\n    'rewrite_1': 27,\n    'rewrite_2': 27,\n    'rewrite_3': 29,\n    'modulation_standard_1': 54,\n    'modulation_standard_2': 30,\n    'modulation_multiturn': 90,\n    'self_reflect': 90,\n    'mechanism_scripted_haiku': 360,\n    'mechanism_scripted_nemotron': 360,\n    'dynamic_learner': 120,\n    'mechanism_headtohead': 120,\n    'dynamic_base_mechanisms': 60,\n    'self_reflect_nemotron': 167,\n    'cognitive_prosthesis': 60,\n    'naive_baseline_haiku': 72,\n    'naive_baseline_kimi': 72,\n}\n\ndef get_run(key, judge_filter='claude-opus'):\n    \"\"\"Get scored rows for a named run, filtering by judge.\n    \n    Args:\n        key: Run key from RUN_IDS dict\n        judge_filter: Filter to rows matching this judge_model pattern.\n                      Default 'claude-opus' = matches both claude-opus-4.5 and claude-opus-4.6.\n                      Pass None to get ALL judges (needed for cross-judge analysis).\n    \"\"\"\n    run_id = RUN_IDS[key]\n    mask = (df_all['run_id'] == run_id) & df_all['overall_score'].notna()\n    if judge_filter:\n        mask &= df_all['judge_model'].str.contains(judge_filter, case=False, na=False)\n    return df_all[mask].copy()\n\n# Verify run sizes\nprint(f'{\"Run\":<35s} {\"N\":>5s} {\"Expected\":>8s} {\"Status\":>10s}')\nprint('-' * 65)\nfor key in sorted(RUN_IDS.keys()):\n    run_id = RUN_IDS[key]\n    opus_mask = df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n    n = len(df_all[(df_all['run_id'] == run_id) & df_all['overall_score'].notna() & opus_mask])\n    expected = EXPECTED_N.get(key, '?')\n    status = 'OK' if n == expected else f'~{n}'\n    print(f'  {key:<33s} {n:5d} {str(expected):>8s} {status:>10s}')\n\nopus_mask = df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\ntotal_n = sum(len(df_all[(df_all['run_id'] == RUN_IDS[k]) & df_all['overall_score'].notna() & opus_mask]) \n              for k in RUN_IDS)\nprint(f'\\nTotal across 31 key runs: N={total_n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Helper functions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "\n",
    "def cohens_d(group1, group2):\n",
    "    \"\"\"Compute Cohen's d (pooled SD).\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    if pooled_std == 0:\n",
    "        return 0.0\n",
    "    return (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "\n",
    "def ci_95(data):\n",
    "    \"\"\"95% confidence interval for the mean.\"\"\"\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    se = stats.sem(data)\n",
    "    h = se * stats.t.ppf(0.975, n - 1)\n",
    "    return m - h, m + h\n",
    "\n",
    "def parse_scores_json(row):\n",
    "    \"\"\"Parse scores_with_reasoning JSON into individual recognition dimension scores.\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(row)\n",
    "        result = {}\n",
    "        for dim in ['mutual_recognition', 'dialectical_responsiveness',\n",
    "                    'transformative_potential', 'memory_integration',\n",
    "                    'tutor_adaptation', 'learner_growth']:\n",
    "            if dim in parsed and 'score' in parsed[dim]:\n",
    "                result[dim] = parsed[dim]['score']\n",
    "            else:\n",
    "                result[dim] = np.nan\n",
    "        return result\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return {dim: np.nan for dim in ['mutual_recognition', 'dialectical_responsiveness',\n",
    "                                         'transformative_potential', 'memory_integration',\n",
    "                                         'tutor_adaptation', 'learner_growth']}\n",
    "\n",
    "# Parse recognition dimension scores from JSON\n",
    "recog_dims = df_all['scores_with_reasoning'].apply(parse_scores_json).apply(pd.Series)\n",
    "for col in recog_dims.columns:\n",
    "    df_all[f'score_{col}'] = recog_dims[col]\n",
    "\n",
    "print('Recognition dimension columns added:')\n",
    "print([c for c in df_all.columns if 'mutual' in c or 'dialectical' in c or 'transformative' in c or 'memory_int' in c or 'tutor_adapt' in c or 'learner_growth' in c])\n",
    "\n",
    "# \u2500\u2500 Derive factors from profile_name \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# CRITICAL: factor columns are only 48% populated; always derive from profile_name\n",
    "\n",
    "def derive_factors(profile):\n",
    "    \"\"\"Derive experimental factors from profile_name.\"\"\"\n",
    "    if pd.isna(profile):\n",
    "        return pd.Series({'factor_A_recognition': None, 'factor_B_multi': None,\n",
    "                          'factor_C_learner': None, 'prompt_type': None})\n",
    "    p = str(profile).lower()\n",
    "    # Factor A: Recognition\n",
    "    if 'recog' in p:\n",
    "        recognition = True\n",
    "        prompt_type = 'recognition'\n",
    "    elif 'enhanced' in p or 'cell_9' in p or 'cell_10' in p or 'cell_11' in p or 'cell_12' in p:\n",
    "        recognition = False\n",
    "        prompt_type = 'enhanced'\n",
    "    elif 'placebo' in p:\n",
    "        recognition = False\n",
    "        prompt_type = 'placebo'\n",
    "    else:\n",
    "        recognition = False\n",
    "        prompt_type = 'base'\n",
    "    # Factor B: Multi-agent tutor\n",
    "    multi = '_multi_' in p\n",
    "    # Factor C: Learner architecture\n",
    "    psycho = 'psycho' in p\n",
    "    return pd.Series({\n",
    "        'factor_A_recognition': recognition,\n",
    "        'factor_B_multi': multi,\n",
    "        'factor_C_learner': psycho,\n",
    "        'prompt_type': prompt_type,\n",
    "    })\n",
    "\n",
    "factors = df_all['profile_name'].apply(derive_factors)\n",
    "for col in factors.columns:\n",
    "    df_all[col] = factors[col]\n",
    "\n",
    "print(f'\\nFactor derivation complete. Sample:')\n",
    "print(df_all[['profile_name', 'factor_A_recognition', 'factor_B_multi', 'factor_C_learner', 'prompt_type']].drop_duplicates().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Table 1 \u2014 Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 1: Verify model configurations per run\n",
    "print('Table 1: Model Configuration by Run')\n",
    "print('=' * 70)\n",
    "\n",
    "for key, run_id in RUN_IDS.items():\n",
    "    df_run = get_run(key)\n",
    "    models = df_run[['model', 'judge_model']].drop_duplicates()\n",
    "    print(f'\\n{key} ({run_id}):')\n",
    "    print(f'  Tutor model(s):  {df_run[\"model\"].unique().tolist()}')\n",
    "    print(f'  Judge model(s):  {df_run[\"judge_model\"].unique().tolist()}')\n",
    "    if 'ego_model' in df_run.columns:\n",
    "        ego = df_run['ego_model'].dropna().unique()\n",
    "        sup = df_run['superego_model'].dropna().unique()\n",
    "        if len(ego) > 0:\n",
    "            print(f'  Ego model(s):    {ego.tolist()}')\n",
    "            print(f'  Superego model(s): {sup.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Table 2 \u2014 Sample Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 2: Full sample summary across all 31 key evaluations\n# Filter to primary judge (Opus) to match paper N-counts\nprint('Table 2: Evaluation Sample Summary (31 Key Evaluations, Opus-judged)')\nprint('=' * 90)\n\nopus_mask = df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n\nsummary_rows = []\nfor key in RUN_IDS:\n    run_id = RUN_IDS[key]\n    scored = len(df_all[(df_all['run_id'] == run_id) & df_all['overall_score'].notna() & opus_mask])\n    expected = EXPECTED_N.get(key, '?')\n    summary_rows.append({\n        'Evaluation': key,\n        'Run ID': run_id[-8:],  # abbreviated\n        'Scored': scored,\n        'Expected': expected,\n        'Match': 'OK' if scored == expected else f'~{scored}',\n    })\n\ndf_summary = pd.DataFrame(summary_rows)\nprint(df_summary.to_string(index=False))\n\ntotal_scored = df_summary['Scored'].sum()\ntotal_expected = sum(v for v in EXPECTED_N.values())\nprint(f'\\nTotal scored:   {total_scored}')\nprint(f'Total expected: {total_expected}')\nprint(f'Paper reports:  N=3,292 primary scored across 31 key evaluations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table 3 \u2014 Inter-Judge Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 3: Inter-judge reliability\n",
    "# Match responses by MD5 hash of suggestions content\n",
    "print('Table 3: Inter-Judge Reliability')\n",
    "print('=' * 70)\n",
    "\n",
    "df_judged = df_all[df_all['judge_model'].notna() & df_all['overall_score'].notna() & df_all['suggestions'].notna()].copy()\n",
    "df_judged['content_hash'] = df_judged['suggestions'].apply(\n",
    "    lambda s: hashlib.md5(s.encode()).hexdigest() if isinstance(s, str) else None\n",
    ")\n",
    "\n",
    "# Group by content hash to find same-response multi-judge pairs\n",
    "hash_groups = df_judged.groupby('content_hash')\n",
    "paired_data = []\n",
    "\n",
    "for content_hash, group in hash_groups:\n",
    "    judges = group['judge_model'].unique()\n",
    "    if len(judges) < 2:\n",
    "        continue\n",
    "    judge_list = sorted(judges)\n",
    "    for i in range(len(judge_list)):\n",
    "        for j in range(i + 1, len(judge_list)):\n",
    "            j1_rows = group[group['judge_model'] == judge_list[i]]\n",
    "            j2_rows = group[group['judge_model'] == judge_list[j]]\n",
    "            for _, r1 in j1_rows.iterrows():\n",
    "                for _, r2 in j2_rows.iterrows():\n",
    "                    paired_data.append({\n",
    "                        'judge1': judge_list[i],\n",
    "                        'judge2': judge_list[j],\n",
    "                        'score1': r1['overall_score'],\n",
    "                        'score2': r2['overall_score'],\n",
    "                        'content_hash': content_hash,\n",
    "                    })\n",
    "\n",
    "df_pairs = pd.DataFrame(paired_data)\n",
    "print(f'Total paired judgments: {len(df_pairs)}')\n",
    "\n",
    "if len(df_pairs) > 0:\n",
    "    # Per-pair correlation\n",
    "    for pair_key, pair_df in df_pairs.groupby(['judge1', 'judge2']):\n",
    "        j1, j2 = pair_key\n",
    "        n = len(pair_df)\n",
    "        r_pearson, p_pearson = stats.pearsonr(pair_df['score1'], pair_df['score2'])\n",
    "        r_spearman, p_spearman = stats.spearmanr(pair_df['score1'], pair_df['score2'])\n",
    "        mad = np.mean(np.abs(pair_df['score1'] - pair_df['score2']))\n",
    "        m1, m2 = pair_df['score1'].mean(), pair_df['score2'].mean()\n",
    "        j1_short = j1.split('/')[-1] if '/' in j1 else j1\n",
    "        j2_short = j2.split('/')[-1] if '/' in j2 else j2\n",
    "        print(f'\\n  {j1_short} vs {j2_short}  (N={n})')\n",
    "        print(f'    Pearson r  = {r_pearson:.3f}  (p={p_pearson:.4f})')\n",
    "        print(f'    Spearman \u03c1 = {r_spearman:.3f}  (p={p_spearman:.4f})')\n",
    "        print(f'    Mean Abs Diff = {mad:.1f} pts')\n",
    "        print(f'    Mean scores: {m1:.1f} vs {m2:.1f}')\n",
    "\n",
    "    # Mean score by judge\n",
    "    print('\\nMean scores by judge:')\n",
    "    for judge in sorted(df_judged['judge_model'].unique()):\n",
    "        m = df_judged[df_judged['judge_model'] == judge]['overall_score'].mean()\n",
    "        print(f'  {judge.split(\"/\")[-1]:30s}  {m:.1f}')\n",
    "else:\n",
    "    print('No paired judgments found. This requires rejudging runs with multiple judge models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of inter-judge agreement (if pairs exist)\n",
    "if len(df_pairs) > 0:\n",
    "    pair_keys = df_pairs.groupby(['judge1', 'judge2']).ngroups\n",
    "    fig, axes = plt.subplots(1, min(pair_keys, 3), figsize=(5 * min(pair_keys, 3), 5))\n",
    "    if pair_keys == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, (pair_key, pair_df) in zip(axes, df_pairs.groupby(['judge1', 'judge2'])):\n",
    "        j1, j2 = pair_key\n",
    "        ax.scatter(pair_df['score1'], pair_df['score2'], alpha=0.5)\n",
    "        ax.plot([0, 100], [0, 100], 'k--', alpha=0.3)\n",
    "        ax.set_xlabel(j1.split('/')[-1])\n",
    "        ax.set_ylabel(j2.split('/')[-1])\n",
    "        r, _ = stats.pearsonr(pair_df['score1'], pair_df['score2'])\n",
    "        ax.set_title(f'r = {r:.3f}, N = {len(pair_df)}')\n",
    "        ax.set_xlim(0, 105)\n",
    "        ax.set_ylim(0, 105)\n",
    "\n",
    "    plt.suptitle('Table 3: Inter-Judge Reliability', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('(No scatter plot: no paired data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Table 4 \u2014 Recognition Validation (N=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 4: Recognition Validation \u2014 3-way comparison\n",
    "print('Table 4: Base vs Enhanced vs Recognition (N=36)')\n",
    "print('=' * 70)\n",
    "\n",
    "df_val = get_run('recognition_validation')\n",
    "print(f'Run N = {len(df_val)}')\n",
    "print(f'Profiles: {df_val[\"profile_name\"].unique().tolist()}')\n",
    "\n",
    "# Derive prompt_type\n",
    "df_val['pt'] = df_val['prompt_type']\n",
    "\n",
    "for pt in ['base', 'enhanced', 'recognition']:\n",
    "    subset = df_val[df_val['pt'] == pt]['overall_score']\n",
    "    print(f'  {pt:12s}  N={len(subset):3d}  Mean={subset.mean():.1f}  SD={subset.std():.1f}')\n",
    "\n",
    "# One-way ANOVA: F(2, 33)\n",
    "groups = [df_val[df_val['pt'] == pt]['overall_score'].values for pt in ['base', 'enhanced', 'recognition']]\n",
    "f_stat, p_val = stats.f_oneway(*groups)\n",
    "print(f'\\nOne-way ANOVA: F(2, {len(df_val) - 3}) = {f_stat:.2f}, p = {p_val:.4f}')\n",
    "\n",
    "# Effect decomposition\n",
    "base_mean = groups[0].mean()\n",
    "enhanced_mean = groups[1].mean()\n",
    "recog_mean = groups[2].mean()\n",
    "\n",
    "total_effect = recog_mean - base_mean\n",
    "engineering_effect = enhanced_mean - base_mean\n",
    "unique_effect = recog_mean - enhanced_mean\n",
    "\n",
    "print(f'\\nEffect Decomposition:')\n",
    "print(f'  Total recognition effect:        +{total_effect:.1f} pts')\n",
    "print(f'  Prompt engineering (enh vs base): +{engineering_effect:.1f} pts ({engineering_effect/total_effect*100:.0f}%)')\n",
    "print(f'  Recognition unique (rec vs enh):  +{unique_effect:.1f} pts ({unique_effect/total_effect*100:.0f}%)')\n",
    "print(f'\\nPaper reports: +20.1 total, +11.4 engineering, +8.7 unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot for Table 4\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "order = ['base', 'enhanced', 'recognition']\n",
    "sns.boxplot(data=df_val, x='pt', y='overall_score', order=order, ax=ax, palette='Set2')\n",
    "sns.stripplot(data=df_val, x='pt', y='overall_score', order=order, ax=ax,\n",
    "              color='black', alpha=0.4, size=4)\n",
    "ax.set_xlabel('Prompt Type')\n",
    "ax.set_ylabel('Overall Score (0-100)')\n",
    "ax.set_title(f'Table 4: Recognition Validation (N={len(df_val)}, F={f_stat:.2f}, p={p_val:.4f})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Table 5 \u2014 Full Factorial ANOVA (N=342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 5: Full 2x2x2 Factorial (N=350, combining f5d4dd93 + a933d745)\n",
    "print('Table 5: Full Factorial ANOVA (N=350)')\n",
    "print('=' * 70)\n",
    "\n",
    "# Combine the two factorial runs (cells 1-5,7 from f5d4dd93 + cells 6,8 from a933d745)\n",
    "factorial_runs = ['eval-2026-02-03-f5d4dd93', 'eval-2026-02-06-a933d745']\n",
    "df_fact = df_all[\n",
    "    df_all['run_id'].isin(factorial_runs) & \n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "].copy()\n",
    "print(f'Run N = {len(df_fact)} (paper reports N=350)')\n",
    "\n",
    "# 8-cell means table\n",
    "print('\\n8-Cell Means:')\n",
    "cell_stats = df_fact.groupby('profile_name')['overall_score'].agg(['count', 'mean', 'std']).round(1)\n",
    "cell_stats = cell_stats.sort_index()\n",
    "print(cell_stats.to_string())\n",
    "print(f'\\nPaper reports: 1=77.6, 2=80.0, 3=76.6, 4=81.5, 5=92.8, 6=83.9, 7=92.3, 8=87.3')\n",
    "\n",
    "# Derive A/B/C\n",
    "df_fact['A'] = df_fact['factor_A_recognition'].astype(int)\n",
    "df_fact['B'] = df_fact['factor_B_multi'].astype(int)\n",
    "df_fact['fC'] = df_fact['factor_C_learner'].astype(int)\n",
    "\n",
    "# Main effects with 95% CIs\n",
    "print('\\nMain Effects:')\n",
    "for factor, label in [('A', 'Recognition'), ('B', 'Multi-agent'), ('fC', 'Learner ego-superego')]:\n",
    "    g0 = df_fact[df_fact[factor] == 0]['overall_score']\n",
    "    g1 = df_fact[df_fact[factor] == 1]['overall_score']\n",
    "    effect = g1.mean() - g0.mean()\n",
    "    se = np.sqrt(g0.var() / len(g0) + g1.var() / len(g1))\n",
    "    ci_lo = effect - 1.96 * se\n",
    "    ci_hi = effect + 1.96 * se\n",
    "    d = cohens_d(g1.values, g0.values)\n",
    "    print(f'  {label:30s}  {effect:+.1f} pts  d={d:.2f}  95% CI [{ci_lo:.1f}, {ci_hi:.1f}]')\n",
    "\n",
    "print(f'\\nPaper reports: Recognition +10.2 (d=0.80), Architecture +0.9, Learner -1.7')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA: Type II with statsmodels\n",
    "model = ols('overall_score ~ C(A) * C(B) * C(fC)', data=df_fact).fit()\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "\n",
    "# Add eta-squared\n",
    "ss_total = anova_table['sum_sq'].sum()\n",
    "anova_table['eta_sq'] = anova_table['sum_sq'] / ss_total\n",
    "\n",
    "print('\\nType II ANOVA:')\n",
    "print(anova_table[['sum_sq', 'df', 'F', 'PR(>F)', 'eta_sq']].round(4).to_string())\n",
    "\n",
    "# Extract key values for concordance\n",
    "f_A = anova_table.loc['C(A)', 'F']\n",
    "eta_A = anova_table.loc['C(A)', 'eta_sq']\n",
    "print(f'\\nKey: F(A) = {f_A:.2f}, \u03b7\u00b2(A) = {eta_A:.3f}')\n",
    "print(f'Paper reports: F = 43.27, \u03b7\u00b2 = .109')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Recognition x Multi-agent\n",
    "pivot = df_fact.groupby(['factor_A_recognition', 'factor_B_multi'])['overall_score'].mean().unstack()\n",
    "pivot.index = ['Base', 'Recognition']\n",
    "pivot.columns = ['Single', 'Multi']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "sns.heatmap(pivot, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax, vmin=70, vmax=90)\n",
    "ax.set_title(f'Table 5: Factorial Mean Scores (N={len(df_fact)})')\n",
    "ax.set_ylabel('Factor A: Recognition')\n",
    "ax.set_xlabel('Factor B: Architecture')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Table 6 \u2014 A\u00d7B Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Table 6: A x B Interaction\nprint('Table 6: A\u00d7B Interaction')\nprint('=' * 70)\n\n# \u2500\u2500 Kimi replication (N=60) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf_ab_kimi = get_run('ab_kimi')\nprint(f'\\nKimi A\u00d7B replication: N={len(df_ab_kimi)}')\nprint(f'Profiles: {df_ab_kimi[\"profile_name\"].unique().tolist()}')\n\nfor pt in sorted(df_ab_kimi['prompt_type'].unique()):\n    for multi in [False, True]:\n        subset = df_ab_kimi[(df_ab_kimi['prompt_type'] == pt) & (df_ab_kimi['factor_B_multi'] == multi)]\n        arch = 'multi' if multi else 'single'\n        if len(subset) > 0:\n            print(f'  {pt:12s} {arch:6s}  N={len(subset):3d}  Mean={subset[\"overall_score\"].mean():.1f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Bar chart for A x B interaction\nfig, ax = plt.subplots(figsize=(8, 5))\n\ndf_run = get_run('ab_kimi')\ndf_run['Architecture'] = df_run['factor_B_multi'].map({True: 'Multi', False: 'Single'})\ndf_run['Prompt'] = df_run['prompt_type'].str.capitalize()\nsns.barplot(data=df_run, x='Prompt', y='overall_score', hue='Architecture', ax=ax, palette='Set1')\nax.set_title('A\u00d7B Interaction: Kimi Replication (N=60)')\nax.set_ylabel('Overall Score')\nax.set_ylim(50, 100)\n\nplt.suptitle('Table 6: A\u00d7B Interaction', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tables 7\u20138 \u2014 Domain Generalizability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Tables 7-8: Domain Generalizability\nprint('Tables 7-8: Domain Generalizability')\nprint('=' * 70)\n\n# \u2500\u2500 Kimi elementary (N=60) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\ndf_dom_kimi = get_run('domain_kimi')\nprint(f'\\nKimi Elementary: N={len(df_dom_kimi)}')\n\nfor factor, label in [('factor_A_recognition', 'A: Recognition'), ('factor_B_multi', 'B: Multi-agent')]:\n    g0 = df_dom_kimi[df_dom_kimi[factor] == False]['overall_score']\n    g1 = df_dom_kimi[df_dom_kimi[factor] == True]['overall_score']\n    if len(g0) > 0 and len(g1) > 0:\n        effect = g1.mean() - g0.mean()\n        print(f'  {label:30s}  +{effect:.1f} pts (N: {len(g0)}+{len(g1)})')\n\nprint(f'  Overall mean: {df_dom_kimi[\"overall_score\"].mean():.1f}')\n\nbase_scores = df_dom_kimi[df_dom_kimi['factor_A_recognition'] == False]['overall_score']\nrecog_scores = df_dom_kimi[df_dom_kimi['factor_A_recognition'] == True]['overall_score']\n\nprint(f'\\n  Base mean:        {base_scores.mean():.1f} (N={len(base_scores)})')\nprint(f'  Recognition mean: {recog_scores.mean():.1f} (N={len(recog_scores)})')\nprint(f'  Delta:            +{recog_scores.mean() - base_scores.mean():.1f}')\n\nd = cohens_d(recog_scores.values, base_scores.values)\nprint(f'  Cohen\\'s d:        {d:.2f}')\nprint(f'  Paper reports: d \u2248 0.61')\n\n# Per-scenario breakdown\nprint(f'\\n  Per-scenario effects (Kimi elementary):')\nfor scenario in sorted(df_dom_kimi['scenario_id'].unique()):\n    sc_base = df_dom_kimi[(df_dom_kimi['scenario_id'] == scenario) & (df_dom_kimi['factor_A_recognition'] == False)]['overall_score']\n    sc_recog = df_dom_kimi[(df_dom_kimi['scenario_id'] == scenario) & (df_dom_kimi['factor_A_recognition'] == True)]['overall_score']\n    if len(sc_base) > 0 and len(sc_recog) > 0:\n        delta = sc_recog.mean() - sc_base.mean()\n        print(f'    {scenario:40s}  +{delta:.1f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Grouped bar chart: Elementary vs Philosophy\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Elementary (Kimi)\nfor ax, (df_run, title) in zip(axes, [\n    (get_run('domain_kimi'), 'Elementary (Kimi, N=60)'),\n    (df_fact, 'Philosophy (Kimi Factorial, N=342)')\n]):\n    df_plot = df_run.copy()\n    df_plot['Condition'] = df_plot['factor_A_recognition'].map({True: 'Recognition', False: 'Base'})\n    sns.barplot(data=df_plot, x='Condition', y='overall_score', ax=ax, palette='Set2',\n                order=['Base', 'Recognition'])\n    ax.set_title(title)\n    ax.set_ylabel('Overall Score')\n    ax.set_ylim(40, 100)\n\nplt.suptitle('Tables 7-8: Domain Generalizability', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Table 9 \u2014 Superego Rejection Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 9: Superego Rejection Patterns\n",
    "print('Table 9: Superego Rejection Patterns')\n",
    "print('=' * 70)\n",
    "\n",
    "log_dir = Path(LOGS_DIR)\n",
    "log_files = sorted(log_dir.glob('*.json'))\n",
    "print(f'Total dialogue log files: {len(log_files)}')\n",
    "\n",
    "# Extract all superego rejections\n",
    "all_rejections = []\n",
    "files_with_rejections = 0\n",
    "total_superego_entries = 0\n",
    "\n",
    "for log_file in log_files:\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            data = json.load(f)\n",
    "        trace = data.get('dialogueTrace', [])\n",
    "        file_has_rejection = False\n",
    "        for entry in trace:\n",
    "            if entry.get('agent') == 'superego':\n",
    "                total_superego_entries += 1\n",
    "                if entry.get('approved') == False:\n",
    "                    verdict = entry.get('verdict', {}) or {}\n",
    "                    feedback = verdict.get('feedback', '')\n",
    "                    all_rejections.append({\n",
    "                        'file': log_file.name,\n",
    "                        'round': entry.get('round'),\n",
    "                        'feedback': feedback,\n",
    "                        'profile': data.get('profileName', ''),\n",
    "                    })\n",
    "                    file_has_rejection = True\n",
    "        if file_has_rejection:\n",
    "            files_with_rejections += 1\n",
    "    except (json.JSONDecodeError, IOError):\n",
    "        continue\n",
    "\n",
    "print(f'Total superego entries: {total_superego_entries}')\n",
    "print(f'Total rejections: {len(all_rejections)}')\n",
    "print(f'Files with at least one rejection: {files_with_rejections}/{len(log_files)}')\n",
    "\n",
    "# Classify feedback text by pattern\n",
    "REJECTION_CATEGORIES = {\n",
    "    'Engagement': re.compile(r'engage|engag|interact|respond.*learner|learner.*position|acknowledge', re.I),\n",
    "    'Specificity': re.compile(r'specific|vague|generic|concrete|lecture.?\\d|reference', re.I),\n",
    "    'Struggle': re.compile(r'struggl|frustr|stuck|difficult|confus|overwhelm|overload', re.I),\n",
    "    'Memory': re.compile(r'previous|history|past|returning|remember|earlier|last time', re.I),\n",
    "    'Level-matching': re.compile(r'level|advanced|beginner|appropriate|scaffold|zone|ZPD|mismatch', re.I),\n",
    "}\n",
    "\n",
    "category_counts = Counter()\n",
    "for rej in all_rejections:\n",
    "    fb = rej['feedback']\n",
    "    for cat, pattern in REJECTION_CATEGORIES.items():\n",
    "        if pattern.search(fb):\n",
    "            category_counts[cat] += 1\n",
    "\n",
    "print(f'\\nRejection Pattern Frequency:')\n",
    "total_rej = len(all_rejections)\n",
    "for cat in ['Engagement', 'Specificity', 'Struggle', 'Memory', 'Level-matching']:\n",
    "    count = category_counts[cat]\n",
    "    pct = count / total_rej * 100 if total_rej > 0 else 0\n",
    "    print(f'  {cat:20s}  {count:4d}  ({pct:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Table 10 \u2014 Dimension Effect Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 10: Per-dimension Cohen's d (factorial run, base vs recognition)\n",
    "print('Table 10: Dimension-Level Effect Sizes')\n",
    "print('=' * 70)\n",
    "\n",
    "STANDARD_DIMS = ['score_relevance', 'score_specificity', 'score_pedagogical',\n",
    "                 'score_personalization', 'score_actionability', 'score_tone']\n",
    "DIM_LABELS = ['Relevance', 'Specificity', 'Pedagogical', 'Personalization', 'Actionability', 'Tone']\n",
    "\n",
    "base_mask = df_fact['factor_A_recognition'] == False\n",
    "recog_mask = df_fact['factor_A_recognition'] == True\n",
    "\n",
    "dim_effects = []\n",
    "for dim, label in zip(STANDARD_DIMS, DIM_LABELS):\n",
    "    base_vals = df_fact.loc[base_mask, dim].dropna()\n",
    "    recog_vals = df_fact.loc[recog_mask, dim].dropna()\n",
    "    d = cohens_d(recog_vals.values, base_vals.values)\n",
    "    dim_effects.append({\n",
    "        'Dimension': label,\n",
    "        'Base Mean': base_vals.mean(),\n",
    "        'Recognition Mean': recog_vals.mean(),\n",
    "        'Cohen\\'s d': d,\n",
    "    })\n",
    "    print(f'  {label:20s}  Base={base_vals.mean():.2f}  Recog={recog_vals.mean():.2f}  d={d:.2f}')\n",
    "\n",
    "df_dim_effects = pd.DataFrame(dim_effects).sort_values(\"Cohen's d\", ascending=False)\n",
    "print(f'\\nPaper reports: Personalization d=1.82, Pedagogical d=1.39, Relevance d=1.11, Tone d=1.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forest plot for dimension effect sizes\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "df_plot = df_dim_effects.sort_values(\"Cohen's d\")\n",
    "colors = ['#e74c3c' if d > 0.8 else '#f39c12' if d > 0.5 else '#3498db' for d in df_plot[\"Cohen's d\"]]\n",
    "ax.barh(df_plot['Dimension'], df_plot[\"Cohen's d\"], color=colors)\n",
    "ax.axvline(x=0.8, color='red', linestyle='--', alpha=0.5, label='Large (d=0.8)')\n",
    "ax.axvline(x=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium (d=0.5)')\n",
    "ax.axvline(x=0.2, color='blue', linestyle='--', alpha=0.5, label='Small (d=0.2)')\n",
    "ax.set_xlabel(\"Cohen's d\")\n",
    "ax.set_title('Table 10: Dimension Effect Sizes (Recognition vs Base)')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Table 11 \u2014 Standard Dimensions Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 11: Re-weighted scores \u2014 standard-only vs recognition-only\n",
    "print('Table 11: Standard Dimensions Only Analysis')\n",
    "print('=' * 70)\n",
    "\n",
    "# Raw weights from rubric YAML\n",
    "WEIGHTS = {\n",
    "    'score_relevance': 0.15,\n",
    "    'score_specificity': 0.15,\n",
    "    'score_pedagogical': 0.15,\n",
    "    'score_personalization': 0.10,\n",
    "    'score_actionability': 0.10,\n",
    "    'score_tone': 0.10,\n",
    "    'score_mutual_recognition': 0.083,\n",
    "    'score_dialectical_responsiveness': 0.083,\n",
    "    'score_transformative_potential': 0.083,\n",
    "    'score_memory_integration': 0.05,\n",
    "    'score_tutor_adaptation': 0.05,\n",
    "    'score_learner_growth': 0.05,\n",
    "}\n",
    "\n",
    "STANDARD_WEIGHT_KEYS = ['score_relevance', 'score_specificity', 'score_pedagogical',\n",
    "                         'score_personalization', 'score_actionability', 'score_tone']\n",
    "RECOG_WEIGHT_KEYS = ['score_mutual_recognition', 'score_dialectical_responsiveness',\n",
    "                      'score_transformative_potential', 'score_memory_integration']\n",
    "\n",
    "def compute_weighted_score(row, weight_keys):\n",
    "    \"\"\"Compute weighted score using only specified dimensions, re-normalized.\"\"\"\n",
    "    total_weight = 0\n",
    "    weighted_sum = 0\n",
    "    for key in weight_keys:\n",
    "        val = row.get(key)\n",
    "        if pd.notna(val):\n",
    "            w = WEIGHTS[key]\n",
    "            weighted_sum += val * w\n",
    "            total_weight += w\n",
    "    if total_weight == 0:\n",
    "        return np.nan\n",
    "    return (weighted_sum / total_weight) * 20  # scale to 0-100\n",
    "\n",
    "df_fact['standard_only_score'] = df_fact.apply(lambda r: compute_weighted_score(r, STANDARD_WEIGHT_KEYS), axis=1)\n",
    "df_fact['recognition_only_score'] = df_fact.apply(lambda r: compute_weighted_score(r, RECOG_WEIGHT_KEYS), axis=1)\n",
    "\n",
    "for condition, label in [(False, 'Base (cells 1-4)'), (True, 'Recognition (cells 5-8)')]:\n",
    "    mask = df_fact['factor_A_recognition'] == condition\n",
    "    n = mask.sum()\n",
    "    overall = df_fact.loc[mask, 'overall_score'].mean()\n",
    "    std_only = df_fact.loc[mask, 'standard_only_score'].mean()\n",
    "    rec_only = df_fact.loc[mask, 'recognition_only_score'].mean()\n",
    "    print(f'  {label:25s}  N={n:3d}  Overall={overall:.1f}  Standard-only={std_only:.1f}  Recog-only={rec_only:.1f}')\n",
    "\n",
    "# Differences\n",
    "base_std = df_fact.loc[~df_fact['factor_A_recognition'], 'standard_only_score'].mean()\n",
    "recog_std = df_fact.loc[df_fact['factor_A_recognition'], 'standard_only_score'].mean()\n",
    "print(f'\\n  Standard-only difference: +{recog_std - base_std:.1f}')\n",
    "print(f'  Paper reports: +6.1 pts advantage persists on standard-only dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Table 12 \u2014 Multi-Turn Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 12: Multi-turn scenario results\n",
    "print('Table 12: Multi-Turn Scenarios')\n",
    "print('=' * 70)\n",
    "\n",
    "MULTI_TURN_SCENARIOS = [\n",
    "    'misconception_correction_flow',\n",
    "    'mood_frustration_to_breakthrough',\n",
    "    'mutual_transformation_journey',\n",
    "]\n",
    "\n",
    "# Use ALL data across runs (not run-specific per plan)\n",
    "df_scored = df_all[df_all['overall_score'].notna() & df_all['judge_model'].str.contains('claude-opus', case=False, na=False)].copy()\n",
    "\n",
    "mt_results = []\n",
    "for scenario in MULTI_TURN_SCENARIOS:\n",
    "    sc_data = df_scored[df_scored['scenario_id'] == scenario].copy()\n",
    "    sc_data['is_recog'] = sc_data['profile_name'].str.contains('recog', case=False, na=False)\n",
    "\n",
    "    base = sc_data[~sc_data['is_recog']]['overall_score']\n",
    "    recog = sc_data[sc_data['is_recog']]['overall_score']\n",
    "    avg_rounds = sc_data['dialogue_rounds'].mean() if 'dialogue_rounds' in sc_data.columns else np.nan\n",
    "\n",
    "    if len(base) > 1 and len(recog) > 1:\n",
    "        d = cohens_d(recog.values, base.values)\n",
    "        delta = recog.mean() - base.mean()\n",
    "        mt_results.append({\n",
    "            'Scenario': scenario,\n",
    "            'N': len(sc_data),\n",
    "            'Avg Rounds': avg_rounds,\n",
    "            'Base': base.mean(),\n",
    "            'Recognition': recog.mean(),\n",
    "            'Delta': delta,\n",
    "            'Cohen\\'s d': d,\n",
    "        })\n",
    "        print(f'  {scenario:40s}  N={len(sc_data):4d}  Base={base.mean():.1f}  Recog={recog.mean():.1f}  \u0394={delta:+.1f}  d={d:.2f}')\n",
    "\n",
    "print(f'\\nPaper reports: d = 0.85 / 0.59 / 0.78')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart with error bars\n",
    "if mt_results:\n",
    "    df_mt = pd.DataFrame(mt_results)\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(df_mt))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, df_mt['Base'], width, label='Base', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, df_mt['Recognition'], width, label='Recognition', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlabel('Scenario')\n",
    "    ax.set_ylabel('Overall Score')\n",
    "    ax.set_title('Table 12: Multi-Turn Scenarios (Base vs Recognition)')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([s.replace('_', '\\n') for s in df_mt['Scenario']], fontsize=9)\n",
    "    ax.legend()\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "    # Annotate Cohen's d\n",
    "    for i, row in df_mt.iterrows():\n",
    "        ax.annotate(f'd={row[\"Cohen\\'s d\"]:.2f}', (i, max(row['Base'], row['Recognition']) + 3),\n",
    "                    ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Table 13 \u2014 Bilateral Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 13: Bilateral Transformation Metrics\n",
    "print('Table 13: Bilateral Transformation Metrics')\n",
    "print('=' * 70)\n",
    "\n",
    "log_dir = Path(LOGS_DIR)\n",
    "transformation_data = []\n",
    "\n",
    "for log_file in sorted(log_dir.glob('*.json')):\n",
    "    try:\n",
    "        with open(log_file) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if not data.get('isMultiTurn') or not data.get('transformationAnalysis'):\n",
    "            continue\n",
    "\n",
    "        ta = data['transformationAnalysis']\n",
    "        tp = ta.get('turnProgression', {})\n",
    "        dtr = ta.get('dialogueTraceReport', {})\n",
    "        oa = dtr.get('overallAssessment', {})\n",
    "\n",
    "        profile = data.get('profileName', '')\n",
    "        is_recog = 'recog' in str(profile).lower()\n",
    "\n",
    "        transformation_data.append({\n",
    "            'file': log_file.name,\n",
    "            'profile': profile,\n",
    "            'is_recog': is_recog,\n",
    "            'adaptationIndex': tp.get('adaptationIndex'),\n",
    "            'learnerGrowthIndex': tp.get('learnerGrowthIndex'),\n",
    "            'bilateralTransformationIndex': tp.get('bilateralTransformationIndex'),\n",
    "            'transformationQuality': oa.get('transformationQuality'),\n",
    "        })\n",
    "    except (json.JSONDecodeError, IOError):\n",
    "        continue\n",
    "\n",
    "df_trans = pd.DataFrame(transformation_data)\n",
    "print(f'Dialogue files with transformationAnalysis: {len(df_trans)}')\n",
    "\n",
    "if len(df_trans) > 0:\n",
    "    metrics = ['adaptationIndex', 'learnerGrowthIndex', 'bilateralTransformationIndex', 'transformationQuality']\n",
    "    metric_labels = ['Tutor Adaptation Index', 'Learner Growth Index', 'Bilateral Transformation Index', 'Transformation Quality']\n",
    "\n",
    "    for is_recog, label in [(False, 'Base'), (True, 'Recognition')]:\n",
    "        subset = df_trans[df_trans['is_recog'] == is_recog]\n",
    "        print(f'\\n  {label} (N={len(subset)}):')\n",
    "        for metric, mlabel in zip(metrics, metric_labels):\n",
    "            vals = subset[metric].dropna()\n",
    "            if len(vals) > 0:\n",
    "                print(f'    {mlabel:40s}  Mean={vals.mean():.3f}  SD={vals.std():.3f}')\n",
    "\n",
    "    # Deltas\n",
    "    print(f'\\n  Deltas (Recognition - Base):')\n",
    "    for metric, mlabel in zip(metrics, metric_labels):\n",
    "        base_m = df_trans.loc[~df_trans['is_recog'], metric].dropna().mean()\n",
    "        recog_m = df_trans.loc[df_trans['is_recog'], metric].dropna().mean()\n",
    "        if pd.notna(base_m) and pd.notna(recog_m):\n",
    "            print(f'    {mlabel:40s}  \u0394 = {recog_m - base_m:+.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Tables 14\u201315 \u2014 Lexical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables 14-15: Lexical Analysis\n",
    "print('Tables 14-15: Lexical Analysis')\n",
    "print('=' * 70)\n",
    "\n",
    "# Stopwords matching scripts/qualitative-analysis.js\n",
    "STOPWORDS = set([\n",
    "    'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an',\n",
    "    'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been',\n",
    "    'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can',\n",
    "    \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does',\n",
    "    \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for',\n",
    "    'from', 'further', 'get', 'got', 'had', \"hadn't\", 'has', \"hasn't\",\n",
    "    'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her',\n",
    "    'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n",
    "    \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into',\n",
    "    'is', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'let', \"let's\",\n",
    "    'like', 'make', 'me', 'might', 'more', 'most', \"mustn't\", 'my',\n",
    "    'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or',\n",
    "    'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own',\n",
    "    'really', 'right', 'same', \"shan't\", 'she', \"she'd\", \"she'll\",\n",
    "    \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'take', 'than',\n",
    "    'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then',\n",
    "    'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n",
    "    \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until',\n",
    "    'up', 'us', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\",\n",
    "    \"we've\", 'well', 'were', \"weren't\", 'what', \"what's\", 'when',\n",
    "    \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\",\n",
    "    'whom', 'why', \"why's\", 'will', 'with', \"won't\", 'would', \"wouldn't\",\n",
    "    'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours',\n",
    "    'yourself', 'yourselves', 'also', 'been', 'being', 'come', 'even',\n",
    "    'first', 'going', 'good', 'know', 'look', 'much', 'need', 'new', 'now',\n",
    "    'one', 'people', 'really', 'see', 'think', 'thing', 'time', 'two',\n",
    "    'use', 'want', 'way', 'work', 'would', 'year', 'back', 'long', 'say',\n",
    "    'still', 'tell', 'try', 'give', 'go', 'help', 'keep', 'many',\n",
    "    'may', 'put', 'seem', 'show', 'start', 'turn', 'big', 'end', 'set',\n",
    "    'll', 've', 're', 's', 't', 'd', 'don', 'isn', 'doesn', 'didn',\n",
    "    'won', 'can', 'couldn', 'shouldn', 'wasn', 'weren', 'hasn', 'haven',\n",
    "    'hadn', 'aren', 'mustn', 'shan', 'ain',\n",
    "])\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Tokenize matching JS implementation.\"\"\"\n",
    "    text = re.sub(r\"[^a-z'\\s-]\", ' ', text.lower())\n",
    "    return [w for w in text.split() if len(w) > 1]\n",
    "\n",
    "def tokenize_filtered(text):\n",
    "    return [w for w in tokenize(text) if w not in STOPWORDS]\n",
    "\n",
    "def count_sentences(text):\n",
    "    sentences = [s for s in re.split(r'[.!?]+', text) if s.strip()]\n",
    "    return max(len(sentences), 1)\n",
    "\n",
    "def extract_messages(suggestions_json):\n",
    "    \"\"\"Extract message text from suggestions JSON.\"\"\"\n",
    "    try:\n",
    "        parsed = json.loads(suggestions_json)\n",
    "        if not isinstance(parsed, list):\n",
    "            return []\n",
    "        return [s.get('message', '') for s in parsed if s.get('message')]\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []\n",
    "\n",
    "# Build corpora: cells 1-4 (base) vs cells 5-8 (recognition)\n",
    "BASE_CELLS = ['cell_1_base_single_unified', 'cell_2_base_single_psycho',\n",
    "              'cell_3_base_multi_unified', 'cell_4_base_multi_psycho']\n",
    "RECOG_CELLS = ['cell_5_recog_single_unified', 'cell_6_recog_single_psycho',\n",
    "               'cell_7_recog_multi_unified', 'cell_8_recog_multi_psycho']\n",
    "\n",
    "df_corpus = df_all[df_all['suggestions'].notna() & df_all['judge_model'].str.contains('claude-opus', case=False, na=False) & df_all['profile_name'].isin(BASE_CELLS + RECOG_CELLS)].copy()\n",
    "\n",
    "base_messages = []\n",
    "recog_messages = []\n",
    "\n",
    "for _, row in df_corpus.iterrows():\n",
    "    msgs = extract_messages(row['suggestions'])\n",
    "    if row['profile_name'] in BASE_CELLS:\n",
    "        base_messages.extend(msgs)\n",
    "    else:\n",
    "        recog_messages.extend(msgs)\n",
    "\n",
    "print(f'Base messages: {len(base_messages)}')\n",
    "print(f'Recognition messages: {len(recog_messages)}')\n",
    "\n",
    "base_text = ' '.join(base_messages)\n",
    "recog_text = ' '.join(recog_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 14: Lexical diversity metrics\n",
    "print('Table 14: Lexical Diversity Metrics')\n",
    "print('=' * 70)\n",
    "\n",
    "def compute_lexical_metrics(text, label):\n",
    "    all_tokens = tokenize(text)\n",
    "    types = set(all_tokens)\n",
    "    sentences = count_sentences(text)\n",
    "    ttr = len(types) / len(all_tokens) if all_tokens else 0\n",
    "    mean_word_len = np.mean([len(w) for w in all_tokens]) if all_tokens else 0\n",
    "    mean_sent_len = len(all_tokens) / sentences if sentences else 0\n",
    "    return {\n",
    "        'label': label,\n",
    "        'tokens': len(all_tokens),\n",
    "        'vocabulary': len(types),\n",
    "        'ttr': ttr,\n",
    "        'mean_word_length': mean_word_len,\n",
    "        'mean_sentence_length': mean_sent_len,\n",
    "    }\n",
    "\n",
    "lex_base = compute_lexical_metrics(base_text, 'Base (message)')\n",
    "lex_recog = compute_lexical_metrics(recog_text, 'Recognition (message)')\n",
    "\n",
    "df_lex = pd.DataFrame([lex_base, lex_recog]).set_index('label')\n",
    "print(df_lex.round(4).to_string())\n",
    "\n",
    "print(f'\\nPaper reports: TTR 0.039/0.044, Vocab 2319/3689')\n",
    "print(f'Computed:      TTR {lex_base[\"ttr\"]:.3f}/{lex_recog[\"ttr\"]:.3f}, Vocab {lex_base[\"vocabulary\"]}/{lex_recog[\"vocabulary\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 15: Differential word frequency\n",
    "print('Table 15: Differential Word Frequency')\n",
    "print('=' * 70)\n",
    "\n",
    "base_tokens_filtered = tokenize_filtered(base_text)\n",
    "recog_tokens_filtered = tokenize_filtered(recog_text)\n",
    "\n",
    "base_freq = Counter(base_tokens_filtered)\n",
    "recog_freq = Counter(recog_tokens_filtered)\n",
    "\n",
    "base_total = len(base_tokens_filtered)\n",
    "recog_total = len(recog_tokens_filtered)\n",
    "\n",
    "all_words = set(base_freq.keys()) | set(recog_freq.keys())\n",
    "\n",
    "differential = []\n",
    "for word in all_words:\n",
    "    bc = base_freq.get(word, 0)\n",
    "    rc = recog_freq.get(word, 0)\n",
    "    if bc + rc < 10:\n",
    "        continue\n",
    "    base_rate = bc / base_total if base_total else 0\n",
    "    recog_rate = rc / recog_total if recog_total else 0\n",
    "    if base_rate > 0 and recog_rate > 0:\n",
    "        ratio = recog_rate / base_rate\n",
    "    elif recog_rate > 0:\n",
    "        ratio = float('inf')\n",
    "    else:\n",
    "        ratio = 0\n",
    "    differential.append({\n",
    "        'word': word, 'base_count': bc, 'recog_count': rc,\n",
    "        'base_rate': base_rate, 'recog_rate': recog_rate, 'ratio': ratio\n",
    "    })\n",
    "\n",
    "df_diff = pd.DataFrame(differential)\n",
    "\n",
    "# Recognition-skewed (top 15)\n",
    "recog_skewed = df_diff[(df_diff['ratio'] != float('inf')) & (df_diff['ratio'] > 1) & (df_diff['recog_count'] >= 10)]\n",
    "recog_skewed = recog_skewed.sort_values('ratio', ascending=False).head(15)\n",
    "\n",
    "print('\\nRecognition-Skewed Words (top 15):')\n",
    "for _, row in recog_skewed.iterrows():\n",
    "    print(f'  {row[\"word\"]:20s}  base={row[\"base_count\"]:4d}  recog={row[\"recog_count\"]:4d}  ratio={row[\"ratio\"]:.1f}x')\n",
    "\n",
    "# Base-skewed (top 15)\n",
    "base_skewed = df_diff[(df_diff['ratio'] > 0) & (df_diff['ratio'] < 1) & (df_diff['base_count'] >= 10)]\n",
    "base_skewed = base_skewed.sort_values('ratio').head(15)\n",
    "\n",
    "print('\\nBase-Skewed Words (top 15):')\n",
    "for _, row in base_skewed.iterrows():\n",
    "    print(f'  {row[\"word\"]:20s}  base={row[\"base_count\"]:4d}  recog={row[\"recog_count\"]:4d}  ratio={row[\"ratio\"]:.2f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Table 16 \u2014 Thematic Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table 16: Thematic Coding with Chi-Square\n",
    "print('Table 16: Thematic Code Frequency by Condition')\n",
    "print('=' * 70)\n",
    "\n",
    "# 6 regex categories matching scripts/qualitative-analysis.js exactly\n",
    "THEMATIC_CATEGORIES = {\n",
    "    'engagement': {\n",
    "        'label': 'Engagement markers',\n",
    "        'patterns': [\n",
    "            r'your insight', r'building on your', r'your question', r'your point',\n",
    "            r'your observation', r'your analysis', r'your argument', r'your critique',\n",
    "            r\"you've (raised|identified|highlighted|noticed|pointed out)\",\n",
    "            r\"you're (asking|raising|pushing|exploring|getting at)\",\n",
    "        ],\n",
    "    },\n",
    "    'transformation': {\n",
    "        'label': 'Transformation language',\n",
    "        'patterns': [\n",
    "            r'reconsidering', r'that changes (how I|my)', r\"I hadn't (thought|considered)\",\n",
    "            r'revising (my|the)', r'let me (revise|adjust|rethink)',\n",
    "            r\"you've (helped|pushed|made) me\",\n",
    "            r'your .{1,20} (complicates|enriches|changes)',\n",
    "            r'shifts? (my|the|our) (understanding|framing|approach)',\n",
    "        ],\n",
    "    },\n",
    "    'struggle_honoring': {\n",
    "        'label': 'Struggle-honoring',\n",
    "        'patterns': [\n",
    "            r'wrestling with', r'productive confusion', r'working through',\n",
    "            r'grappling with', r'sitting with (the|this)',\n",
    "            r'tension (between|here|you)', r'difficulty (is|here)',\n",
    "            r'struggle (with|is|here)', r'not (easy|simple|straightforward)',\n",
    "        ],\n",
    "    },\n",
    "    'learner_as_subject': {\n",
    "        'label': 'Learner-as-subject framing',\n",
    "        'patterns': [\n",
    "            r'your interpretation', r'your analysis', r'your understanding',\n",
    "            r\"you're grappling with\", r'your perspective', r'your framework',\n",
    "            r'your reading', r\"what you're (doing|building|developing|constructing)\",\n",
    "            r'your (intellectual|philosophical|analytical)',\n",
    "        ],\n",
    "    },\n",
    "    'directive': {\n",
    "        'label': 'Directive framing',\n",
    "        'patterns': [\n",
    "            r'you should', r'you need to', r'you must',\n",
    "            r'the correct (answer|approach|way)', r'the answer is',\n",
    "            r'let me explain', r\"here's what\",\n",
    "            r'make sure (to|you)', r'first,? you',\n",
    "        ],\n",
    "    },\n",
    "    'generic': {\n",
    "        'label': 'Generic/placeholder',\n",
    "        'patterns': [\n",
    "            r'foundational', r'key concepts', r'learning objectives',\n",
    "            r'knowledge base', r'solid foundation', r'core concepts',\n",
    "            r'build (a|your) (solid|strong)',\n",
    "            r'comprehensive (understanding|overview|review)',\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "def count_matches(text, patterns):\n",
    "    \"\"\"Count total regex matches in text.\"\"\"\n",
    "    total = 0\n",
    "    for pat in patterns:\n",
    "        total += len(re.findall(pat, text, re.IGNORECASE))\n",
    "    return total\n",
    "\n",
    "def count_response_presence(messages, patterns):\n",
    "    \"\"\"Count how many messages contain at least one match.\"\"\"\n",
    "    present = 0\n",
    "    for msg in messages:\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, msg, re.IGNORECASE):\n",
    "                present += 1\n",
    "                break\n",
    "    return present\n",
    "\n",
    "base_word_count = len(tokenize(base_text))\n",
    "recog_word_count = len(tokenize(recog_text))\n",
    "\n",
    "thematic_results = []\n",
    "\n",
    "for cat_key, config in THEMATIC_CATEGORIES.items():\n",
    "    patterns = config['patterns']\n",
    "\n",
    "    base_raw = count_matches(base_text, patterns)\n",
    "    recog_raw = count_matches(recog_text, patterns)\n",
    "\n",
    "    base_per1000 = (base_raw / base_word_count) * 1000 if base_word_count else 0\n",
    "    recog_per1000 = (recog_raw / recog_word_count) * 1000 if recog_word_count else 0\n",
    "    ratio = recog_per1000 / base_per1000 if base_per1000 > 0 else (float('inf') if recog_per1000 > 0 else 1.0)\n",
    "\n",
    "    # Chi-square: response-level presence/absence with Yates correction\n",
    "    base_present = count_response_presence(base_messages, patterns)\n",
    "    base_absent = len(base_messages) - base_present\n",
    "    recog_present = count_response_presence(recog_messages, patterns)\n",
    "    recog_absent = len(recog_messages) - recog_present\n",
    "\n",
    "    # 2x2 contingency table\n",
    "    observed = np.array([[base_present, base_absent], [recog_present, recog_absent]])\n",
    "    chi2, p_val, dof, expected = stats.chi2_contingency(observed, correction=True)\n",
    "\n",
    "    sig = '*' if p_val < 0.05 else ''\n",
    "\n",
    "    thematic_results.append({\n",
    "        'Category': config['label'],\n",
    "        'Base (per 1000)': base_per1000,\n",
    "        'Recog (per 1000)': recog_per1000,\n",
    "        'Ratio': ratio,\n",
    "        'chi2': chi2,\n",
    "        'p': p_val,\n",
    "        'Sig': sig,\n",
    "    })\n",
    "\n",
    "    print(f'  {config[\"label\"]:30s}  Base={base_per1000:.1f}  Recog={recog_per1000:.1f}  '\n",
    "          f'Ratio={ratio:.2f}x  \u03c7\u00b2={chi2:.2f}  p={p_val:.4f} {sig}')\n",
    "\n",
    "print(f'\\nPaper reports: \u03c7\u00b2 = 141.90 (struggle), 69.85 (engagement), 93.15 (generic)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart for thematic coding\n",
    "if thematic_results:\n",
    "    df_theme = pd.DataFrame(thematic_results)\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    x = np.arange(len(df_theme))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = ax.bar(x - width/2, df_theme['Base (per 1000)'], width, label='Base', color='#3498db')\n",
    "    bars2 = ax.bar(x + width/2, df_theme['Recog (per 1000)'], width, label='Recognition', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlabel('Thematic Category')\n",
    "    ax.set_ylabel('Occurrences per 1000 words')\n",
    "    ax.set_title('Table 16: Thematic Coding by Condition')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([r['Category'] for r in thematic_results], rotation=30, ha='right', fontsize=9)\n",
    "    ax.legend()\n",
    "\n",
    "    # Annotate significance\n",
    "    for i, row in df_theme.iterrows():\n",
    "        if row['Sig'] == '*':\n",
    "            y_max = max(row['Base (per 1000)'], row['Recog (per 1000)'])\n",
    "            ax.annotate('*', (i, y_max + 0.3), ha='center', fontsize=16, fontweight='bold', color='red')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Table 17 \u2014 Cost-Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Table 17: Cost-Benefit by Domain and Architecture\nprint('Table 17: Cost-Benefit Analysis')\nprint('=' * 70)\n\n# Philosophy: factorial cells 1,3 (base single/multi) and 5,7 (recog single/multi)\nphil_cells_single = ['cell_1_base_single_unified', 'cell_5_recog_single_unified']\nphil_cells_multi = ['cell_3_base_multi_unified', 'cell_7_recog_multi_unified']\n\nphil_single = df_fact[df_fact['profile_name'].isin(phil_cells_single)]\nphil_multi = df_fact[df_fact['profile_name'].isin(phil_cells_multi)]\n\n# Elementary: domain Kimi run\ndf_elem = get_run('domain_kimi')\nelem_single = df_elem[df_elem['factor_B_multi'] == False]\nelem_multi = df_elem[df_elem['factor_B_multi'] == True]\n\ncost_data = []\nfor domain, single_df, multi_df in [\n    ('Philosophy', phil_single, phil_multi),\n    ('Elementary', elem_single, elem_multi),\n]:\n    for arch, arch_df in [('Single-agent', single_df), ('Multi-agent', multi_df)]:\n        avg_score = arch_df['overall_score'].mean()\n        latency = arch_df['latency_ms'].mean() / 1000 if 'latency_ms' in arch_df.columns and arch_df['latency_ms'].notna().any() else np.nan\n        cost_data.append({\n            'Domain': domain,\n            'Architecture': arch,\n            'Avg Score': avg_score,\n            'Latency (s)': latency,\n            'N': len(arch_df),\n        })\n\ndf_cost = pd.DataFrame(cost_data)\nprint(df_cost.round(1).to_string(index=False))\n\n# Compute deltas\nfor domain in ['Philosophy', 'Elementary']:\n    single = df_cost[(df_cost['Domain'] == domain) & (df_cost['Architecture'] == 'Single-agent')].iloc[0]\n    multi = df_cost[(df_cost['Domain'] == domain) & (df_cost['Architecture'] == 'Multi-agent')].iloc[0]\n    score_delta = multi['Avg Score'] - single['Avg Score']\n    latency_mult = multi['Latency (s)'] / single['Latency (s)'] if single['Latency (s)'] > 0 else np.nan\n    print(f'\\n  {domain}: Score \u0394 = {score_delta:+.1f}, Latency = {latency_mult:.1f}x')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Transcript Excerpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript excerpts: 3 high-contrast pairs\n",
    "print('Transcript Excerpts: High-Contrast Pairs')\n",
    "print('=' * 70)\n",
    "\n",
    "PAIR_SCENARIOS = ['struggling_learner', 'recognition_seeking_learner', 'adversarial_tester']\n",
    "\n",
    "df_with_msgs = df_all[\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['suggestions'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False) &\n",
    "    (df_all['overall_score'] > 0)\n",
    "].copy()\n",
    "\n",
    "for scenario in PAIR_SCENARIOS:\n",
    "    sc = df_with_msgs[df_with_msgs['scenario_id'] == scenario]\n",
    "\n",
    "    # Best recognition\n",
    "    recog_sc = sc[sc['profile_name'].isin(RECOG_CELLS)]\n",
    "    base_sc = sc[sc['profile_name'].isin(BASE_CELLS)]\n",
    "\n",
    "    if len(recog_sc) == 0 or len(base_sc) == 0:\n",
    "        continue\n",
    "\n",
    "    best_recog = recog_sc.loc[recog_sc['overall_score'].idxmax()]\n",
    "    worst_base = base_sc.loc[base_sc['overall_score'].idxmin()]\n",
    "\n",
    "    print(f'\\n--- {scenario} ---')\n",
    "    print(f'  Recognition (id={best_recog[\"id\"]}, {best_recog[\"profile_name\"]}, score={best_recog[\"overall_score\"]:.1f}):')\n",
    "\n",
    "    try:\n",
    "        recog_msgs = json.loads(best_recog['suggestions'])\n",
    "        for s in recog_msgs[:2]:\n",
    "            msg = s.get('message', '')[:200]\n",
    "            print(f'    \"{msg}...\"' if len(s.get('message', '')) > 200 else f'    \"{msg}\"')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print('    (could not parse)')\n",
    "\n",
    "    print(f'  Base (id={worst_base[\"id\"]}, {worst_base[\"profile_name\"]}, score={worst_base[\"overall_score\"]:.1f}):')\n",
    "    try:\n",
    "        base_msgs = json.loads(worst_base['suggestions'])\n",
    "        for s in base_msgs[:2]:\n",
    "            msg = s.get('message', '')[:200]\n",
    "            print(f'    \"{msg}...\"' if len(s.get('message', '')) > 200 else f'    \"{msg}\"')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        print('    (could not parse)')\n",
    "\n",
    "    print(f'  Score gap: {best_recog[\"overall_score\"] - worst_base[\"overall_score\"]:.1f} points')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n\n# Phase 2: Extended Analyses (Paper \u00a76.2\u20136.19)\n\nThe following sections reproduce findings added in paper versions v2.0\u2013v2.3.10."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 18. Table 4b \u2014 Memory Isolation 2\u00d72 (\u00a76.2)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Table 4b: Memory Isolation 2\u00d72 (N=120, two runs pooled)\n",
    "# Opus-judged only \u2014 CRITICAL: must filter by judge to avoid multi-judge pooling artifact\n",
    "print('Table 4b: Memory Isolation 2\u00d72 (\u00a76.2)')\n",
    "print('=' * 70)\n",
    "\n",
    "mem_runs = ['eval-2026-02-06-81f2d5a1', 'eval-2026-02-06-ac9ea8f5']\n",
    "df_mem = df_all[\n",
    "    df_all['run_id'].isin(mem_runs) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "# Derive 2\u00d72 factors from cell identity:\n",
    "#   cell_1  = Base (no recog, standard)       \u2192 recog=0, mem=0 (baseline)\n",
    "#   cell_19 = Memory only (no recog, +memory) \u2192 recog=0, mem=1\n",
    "#   cell_20 = Recog only (recog, no memory)   \u2192 recog=1, mem=0\n",
    "#   cell_5  = Both (recog + memory)            \u2192 recog=1, mem=1\n",
    "df_mem['has_recognition'] = df_mem['profile_name'].str.contains('cell_5|cell_20', regex=True)\n",
    "df_mem['has_memory'] = df_mem['profile_name'].str.contains('cell_5|cell_19', regex=True)\n",
    "\n",
    "# 2\u00d72 cell means\n",
    "print('\\n2\u00d72 Cell Means:')\n",
    "for recog in [False, True]:\n",
    "    for mem in [False, True]:\n",
    "        mask = (df_mem['has_recognition'] == recog) & (df_mem['has_memory'] == mem)\n",
    "        subset = df_mem[mask]['overall_score']\n",
    "        r_label = 'Recog' if recog else 'Base'\n",
    "        m_label = '+Mem' if mem else 'NoMem'\n",
    "        print(f'  {r_label:5s} {m_label:5s}  N={len(subset):3d}  Mean={subset.mean():.1f}  SD={subset.std():.1f}')\n",
    "\n",
    "# Main effects\n",
    "recog_scores = df_mem[df_mem['has_recognition']]['overall_score']\n",
    "base_scores = df_mem[~df_mem['has_recognition']]['overall_score']\n",
    "mem_scores = df_mem[df_mem['has_memory']]['overall_score']\n",
    "nomem_scores = df_mem[~df_mem['has_memory']]['overall_score']\n",
    "\n",
    "d_recog = cohens_d(recog_scores.values, base_scores.values)\n",
    "d_mem = cohens_d(mem_scores.values, nomem_scores.values)\n",
    "\n",
    "t_recog, p_recog = stats.ttest_ind(recog_scores, base_scores)\n",
    "t_mem, p_mem = stats.ttest_ind(mem_scores, nomem_scores)\n",
    "\n",
    "print(f'\\nRecognition main effect: d={d_recog:.2f}, t={t_recog:.2f}, p={p_recog:.4f}')\n",
    "print(f'Memory main effect:     d={d_mem:.2f}, t={t_mem:.2f}, p={p_mem:.4f}')\n",
    "\n",
    "# Interaction\n",
    "both = df_mem[df_mem['has_recognition'] & df_mem['has_memory']]['overall_score'].mean()\n",
    "recog_only = df_mem[df_mem['has_recognition'] & ~df_mem['has_memory']]['overall_score'].mean()\n",
    "mem_only = df_mem[~df_mem['has_recognition'] & df_mem['has_memory']]['overall_score'].mean()\n",
    "neither = df_mem[~df_mem['has_recognition'] & ~df_mem['has_memory']]['overall_score'].mean()\n",
    "interaction = (both - recog_only) - (mem_only - neither)\n",
    "print(f'Interaction: {interaction:+.1f}')\n",
    "\n",
    "print(f'\\nPaper reports: Recognition d=1.71, Memory d=0.46, Interaction -4.2')\n",
    "print(f'Total N = {len(df_mem)} (paper reports N=120)')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 19. Active Control Comparison (\u00a76.2)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Active Control (\u00a76.2): Nemotron ego, Opus judge\n# Model confound: factorial used kimi, active control used nemotron\n# Paper uses same-model (nemotron) comparison\nprint('Active Control Comparison (\u00a76.2)')\nprint('=' * 70)\n\ndf_ac = get_run('active_control')\nprint(f'Active control run N = {len(df_ac)}')\n\n# Active control cell means\nfor profile in sorted(df_ac['profile_name'].unique()):\n    subset = df_ac[df_ac['profile_name'] == profile]['overall_score']\n    print(f'  {profile:45s}  N={len(subset):3d}  Mean={subset.mean():.1f}')\n\n# Same-model comparison: need nemotron base/recog from other runs\n# Use the nemotron probe run (722087ac) for same-model base/recog\ndf_nem_probe = get_run('probe_nemotron')\nnem_base = df_nem_probe[~df_nem_probe['profile_name'].str.contains('recog', case=False)]['overall_score']\nnem_recog = df_nem_probe[df_nem_probe['profile_name'].str.contains('recog', case=False)]['overall_score']\n\n# Active control: cells 15-18 are placebo\nac_scores = df_ac[df_ac['profile_name'].str.contains('placebo|cell_1[5-8]', case=False)]['overall_score']\nif len(ac_scores) == 0:\n    ac_scores = df_ac['overall_score']  # all rows are active control\n\nprint(f'\\nSame-model (Nemotron) comparison:')\nprint(f'  Base (probe):           N={len(nem_base):3d}  Mean={nem_base.mean():.1f}')\nprint(f'  Active control:         N={len(ac_scores):3d}  Mean={ac_scores.mean():.1f}  (+{ac_scores.mean() - nem_base.mean():.1f} vs base)')\nprint(f'  Recognition (probe):    N={len(nem_recog):3d}  Mean={nem_recog.mean():.1f}  (+{nem_recog.mean() - nem_base.mean():.1f} vs base)')\nprint(f'\\nPaper reports: base ~58, active control 66.5 (+~9), recognition ~73 (+~15)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 20. Table 7b \u2014 Multi-Model A\u00d7B Probe (\u00a76.4)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Table 7b: Multi-Model A\u00d7B Probe (\u00a76.4)\n# 5 ego models \u00d7 {base, recog} \u00d7 {single, multi} \u2014 single-agent learner only\nprint('Table 7b: Multi-Model A\u00d7B Probe (\u00a76.4)')\nprint('=' * 70)\n\nprobe_config = {\n    'Kimi K2.5':   {'run': 'full_factorial', 'cells': ['cell_1_base_single_unified', 'cell_3_base_multi_unified',\n                                                         'cell_5_recog_single_unified', 'cell_7_recog_multi_unified']},\n    'Nemotron':    {'run': 'probe_nemotron'},\n    'DeepSeek':    {'run': 'probe_deepseek'},\n    'GLM-4.7':     {'run': 'probe_glm'},\n    'Haiku':       {'run': 'probe_haiku'},\n}\n\nprobe_results = []\nfor model_name, config in probe_config.items():\n    if 'cells' in config:\n        # Kimi: use specific single-learner cells from factorial\n        df_probe = get_run(config['run'])\n        df_probe = df_probe[df_probe['profile_name'].isin(config['cells'])]\n    else:\n        df_probe = get_run(config['run'])\n    \n    is_recog = df_probe['profile_name'].str.contains('recog', case=False)\n    is_multi = df_probe['profile_name'].str.contains('multi', case=False)\n    \n    base_single = df_probe[~is_recog & ~is_multi]['overall_score']\n    base_multi = df_probe[~is_recog & is_multi]['overall_score']\n    recog_single = df_probe[is_recog & ~is_multi]['overall_score']\n    recog_multi = df_probe[is_recog & is_multi]['overall_score']\n    \n    recog_effect = df_probe[is_recog]['overall_score'].mean() - df_probe[~is_recog]['overall_score'].mean()\n    arch_effect = df_probe[is_multi]['overall_score'].mean() - df_probe[~is_multi]['overall_score'].mean()\n    \n    # A\u00d7B interaction: (recog_multi - recog_single) - (base_multi - base_single)\n    if len(base_single) > 0 and len(base_multi) > 0 and len(recog_single) > 0 and len(recog_multi) > 0:\n        interaction = (recog_multi.mean() - recog_single.mean()) - (base_multi.mean() - base_single.mean())\n    else:\n        interaction = np.nan\n    \n    n = len(df_probe)\n    probe_results.append({\n        'Model': model_name, 'N': n,\n        'Recog Effect': recog_effect, 'Arch Effect': arch_effect,\n        'A\u00d7B Interaction': interaction,\n    })\n    \n    print(f'  {model_name:12s}  N={n:3d}  Recog={recog_effect:+.1f}  Arch={arch_effect:+.1f}  A\u00d7B={interaction:+.1f}')\n\nprint(f'\\nPaper reports: Recognition range +9.6 to +17.8, A\u00d7B range -5.7 to +0.5')\nprint(f'No meaningful A\u00d7B synergy \u2014 architecture is additive, not synergistic')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 21. Hardwired Rules Ablation (\u00a76.7)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Hardwired Rules Ablation (\u00a76.7, N=72)\n",
    "# Cells 13-14: superego rules embedded directly in ego prompt (no separate superego)\n",
    "# Comparison: hardwired vs normal factorial base (cells 1-2, separate superego)\n",
    "print('Hardwired Rules Ablation (\u00a76.7)')\n",
    "print('=' * 70)\n",
    "\n",
    "df_hw = get_run('hardwired_rules')\n",
    "print(f'Hardwired N = {len(df_hw)}')\n",
    "\n",
    "for profile in sorted(df_hw['profile_name'].unique()):\n",
    "    subset = df_hw[df_hw['profile_name'] == profile]['overall_score']\n",
    "    print(f'  {profile:45s}  N={len(subset):3d}  Mean={subset.mean():.1f}  SD={subset.std():.1f}')\n",
    "\n",
    "hw_mean = df_hw['overall_score'].mean()\n",
    "\n",
    "# Compare to factorial base cells (separate superego, same ego model)\n",
    "factorial_runs = ['eval-2026-02-03-f5d4dd93', 'eval-2026-02-06-a933d745']\n",
    "opus_mask = df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "df_fact_base = df_all[\n",
    "    df_all['run_id'].isin(factorial_runs) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    opus_mask &\n",
    "    df_all['profile_name'].str.contains('base', case=False) &\n",
    "    ~df_all['profile_name'].str.contains('recog', case=False)\n",
    "].copy()\n",
    "fact_base_mean = df_fact_base['overall_score'].mean()\n",
    "\n",
    "print(f'\\nComparison (same ego model, kimi k2.5):')\n",
    "print(f'  Hardwired (rules in ego prompt):  M={hw_mean:.1f} (N={len(df_hw)})')\n",
    "print(f'  Factorial base (separate superego): M={fact_base_mean:.1f} (N={len(df_fact_base)})')\n",
    "print(f'  Difference: {hw_mean - fact_base_mean:+.1f} pts')\n",
    "\n",
    "d_hw = cohens_d(df_hw['overall_score'].values, df_fact_base['overall_score'].values)\n",
    "print(f'  d = {d_hw:.2f}')\n",
    "print(f'\\nPaper: hardwired \u2248 factorial base \u2014 embedding rules in ego prompt does not improve over separate superego')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 22. Dialectical Modulation (\u00a76.8)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Dialectical Modulation (\u00a76.8)\n",
    "# Standard (cells 22-27): eval-2026-02-11-35c53e99, 5f6d51f5\n",
    "# Multi-turn (cells 28-33): eval-2026-02-11-a54235ea\n",
    "print('Dialectical Modulation (\u00a76.8)')\n",
    "print('=' * 70)\n",
    "\n",
    "# Standard modulation\n",
    "mod_std_runs = ['eval-2026-02-11-35c53e99', 'eval-2026-02-11-5f6d51f5']\n",
    "df_mod_std = df_all[\n",
    "    df_all['run_id'].isin(mod_std_runs) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "print(f'Standard modulation (cells 22-27): N={len(df_mod_std)}')\n",
    "is_recog = df_mod_std['profile_name'].str.contains('recog', case=False)\n",
    "base_m = df_mod_std[~is_recog]['overall_score'].mean()\n",
    "recog_m = df_mod_std[is_recog]['overall_score'].mean()\n",
    "print(f'  Base={base_m:.1f}, Recog={recog_m:.1f}, \u0394={recog_m - base_m:+.1f}')\n",
    "\n",
    "# By superego disposition\n",
    "for disp in ['suspicious', 'adversary', 'advocate']:\n",
    "    mask = df_mod_std['profile_name'].str.contains(disp, case=False)\n",
    "    sub = df_mod_std[mask]\n",
    "    is_r = sub['profile_name'].str.contains('recog', case=False)\n",
    "    b = sub[~is_r]['overall_score'].mean()\n",
    "    r = sub[is_r]['overall_score'].mean()\n",
    "    print(f'  {disp:12s}  Base={b:.1f}  Recog={r:.1f}  \u0394={r - b:+.1f}')\n",
    "\n",
    "# Multi-turn modulation\n",
    "df_mod_mt = get_run('modulation_multiturn')\n",
    "print(f'\\nMulti-turn modulation (cells 28-33): N={len(df_mod_mt)}')\n",
    "is_recog = df_mod_mt['profile_name'].str.contains('recog', case=False)\n",
    "base_m = df_mod_mt[~is_recog]['overall_score'].mean()\n",
    "recog_m = df_mod_mt[is_recog]['overall_score'].mean()\n",
    "d = cohens_d(df_mod_mt[is_recog]['overall_score'].values, df_mod_mt[~is_recog]['overall_score'].values)\n",
    "print(f'  Base={base_m:.1f}, Recog={recog_m:.1f}, \u0394={recog_m - base_m:+.1f}, d={d:.2f}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 23. Tables 18\u201319 \u2014 Mechanism Architecture with Dynamic Learners (\u00a76.10)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Table 18: Scripted Learner Mechanism Robustness (\u00a76.10)\n# Cells 40-59 with scripted (unified) learner \u2014 mechanisms DON'T differentiate\nprint('Table 18: Scripted Learner \u2014 Mechanism Robustness')\nprint('=' * 70)\n\ndf_scripted = get_run('mechanism_scripted_haiku')\nprint(f'Haiku scripted (cells 40-59): N={len(df_scripted)}')\n\nis_recog = df_scripted['profile_name'].str.contains('recog', case=False)\nbase_m = df_scripted[~is_recog]['overall_score'].mean()\nrecog_m = df_scripted[is_recog]['overall_score'].mean()\nd = cohens_d(df_scripted[is_recog]['overall_score'].values, df_scripted[~is_recog]['overall_score'].values)\nprint(f'  Base={base_m:.1f}, Recog={recog_m:.1f}, \u0394={recog_m - base_m:+.1f}, d={d:.2f}')\n\n# Show mechanism spread under recognition (should cluster within ~2.4 pts)\nrecog_cells = df_scripted[is_recog].groupby('profile_name')['overall_score'].mean().sort_values()\nprint(f'\\n  Recognition cell range: {recog_cells.min():.1f} \u2013 {recog_cells.max():.1f} (spread={recog_cells.max() - recog_cells.min():.1f})')\nprint(f'  Paper reports: all mechanisms cluster within 2.4 pts under scripted learners')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Table 19: Dynamic Learner 2\u00d74 Mechanism Matrix (\u00a76.10)\n",
    "# Cells 60-65, 69-70 with ego_superego (dynamic) learner \u2014 mechanisms DO differentiate\n",
    "print('Table 19: Dynamic Learner \u2014 2\u00d74 Mechanism Matrix')\n",
    "print('=' * 70)\n",
    "\n",
    "# Combine the three dynamic learner runs\n",
    "dynamic_runs = [\n",
    "    'eval-2026-02-14-6c033830',   # cells 60-63\n",
    "    'eval-2026-02-14-a2b2717c',   # cells 61,63,64,65 (head-to-head)\n",
    "    'eval-2026-02-15-664073ab',   # cells 69-70 (base counterparts)\n",
    "]\n",
    "df_dyn = df_all[\n",
    "    df_all['run_id'].isin(dynamic_runs) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "print(f'Dynamic learner runs combined: N={len(df_dyn)}')\n",
    "\n",
    "# Map cells to mechanisms\n",
    "mechanism_map = {\n",
    "    'cell_60': 'Self-reflect',\n",
    "    'cell_61': 'Self-reflect',\n",
    "    'cell_62': 'Profiling',\n",
    "    'cell_63': 'Profiling',\n",
    "    'cell_64': 'Intersubjective',\n",
    "    'cell_65': 'Combined',\n",
    "    'cell_69': 'Intersubjective',\n",
    "    'cell_70': 'Combined',\n",
    "}\n",
    "\n",
    "df_dyn['mechanism'] = df_dyn['profile_name'].apply(\n",
    "    lambda p: next((v for k, v in mechanism_map.items() if k in str(p)), 'Unknown')\n",
    ")\n",
    "df_dyn['is_recog'] = df_dyn['profile_name'].str.contains('recog', case=False)\n",
    "\n",
    "# 2\u00d74 matrix\n",
    "print(f'\\n{\"Mechanism\":<20s} {\"Base N\":>7s} {\"Base\":>7s} {\"Recog N\":>8s} {\"Recog\":>7s} {\"Delta\":>7s}')\n",
    "print('-' * 60)\n",
    "for mech in ['Self-reflect', 'Profiling', 'Intersubjective', 'Combined']:\n",
    "    mech_data = df_dyn[df_dyn['mechanism'] == mech]\n",
    "    base_data = mech_data[~mech_data['is_recog']]['overall_score']\n",
    "    recog_data = mech_data[mech_data['is_recog']]['overall_score']\n",
    "    delta = recog_data.mean() - base_data.mean() if len(base_data) > 0 and len(recog_data) > 0 else np.nan\n",
    "    print(f'  {mech:<18s} {len(base_data):7d} {base_data.mean():7.1f} {len(recog_data):8d} {recog_data.mean():7.1f} {delta:+7.1f}')\n",
    "\n",
    "# Overall recognition effect with dynamic learner\n",
    "base_all = df_dyn[~df_dyn['is_recog']]['overall_score']\n",
    "recog_all = df_dyn[df_dyn['is_recog']]['overall_score']\n",
    "d = cohens_d(recog_all.values, base_all.values)\n",
    "print(f'\\nOverall recognition effect (dynamic learner): +{recog_all.mean() - base_all.mean():.1f} pts, d={d:.2f}')\n",
    "print(f'Paper reports: +14.8 pts (double the +7.6 with scripted), d\u22481.0')\n",
    "print(f'Profiling highest under recog, intersubjective lowest under base')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 24. Cross-Judge Replication (\u00a75.8, \u00a76.19, Table 22)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Cross-Judge Replication (\u00a75.8, \u00a76.19)\n",
    "# Same responses scored by both Claude (Opus) and GPT-5.2\n",
    "print('Cross-Judge Replication (\u00a75.8, \u00a76.19)')\n",
    "print('=' * 70)\n",
    "\n",
    "# Find runs that have both Opus and GPT-5.2 judgments\n",
    "df_scored = df_all[df_all['overall_score'].notna()].copy()\n",
    "df_scored['content_hash'] = df_scored['suggestions'].apply(\n",
    "    lambda s: hashlib.md5(s.encode()).hexdigest() if isinstance(s, str) else None\n",
    ")\n",
    "\n",
    "# Pair same-response judgments\n",
    "claude_rows = df_scored[df_scored['judge_model'].str.contains('claude-opus', case=False, na=False)]\n",
    "gpt_rows = df_scored[df_scored['judge_model'].str.contains('gpt-5.2', case=False, na=False)]\n",
    "\n",
    "paired = claude_rows.merge(gpt_rows, on='content_hash', suffixes=('_claude', '_gpt'))\n",
    "paired = paired[paired['content_hash'].notna()]\n",
    "\n",
    "if len(paired) > 0:\n",
    "    r_pearson, p_pearson = stats.pearsonr(paired['overall_score_claude'], paired['overall_score_gpt'])\n",
    "    r_spearman, p_spearman = stats.spearmanr(paired['overall_score_claude'], paired['overall_score_gpt'])\n",
    "    \n",
    "    print(f'Paired judgments: N={len(paired)}')\n",
    "    print(f'  Claude mean: {paired[\"overall_score_claude\"].mean():.1f}')\n",
    "    print(f'  GPT-5.2 mean: {paired[\"overall_score_gpt\"].mean():.1f}')\n",
    "    print(f'  Pearson r:  {r_pearson:.3f} (p={p_pearson:.4f})')\n",
    "    print(f'  Spearman \u03c1: {r_spearman:.3f} (p={p_spearman:.4f})')\n",
    "    \n",
    "    # Recognition effect under each judge\n",
    "    paired['is_recog'] = paired['profile_name_claude'].str.contains('recog', case=False)\n",
    "    for judge_col, label in [('overall_score_claude', 'Claude'), ('overall_score_gpt', 'GPT-5.2')]:\n",
    "        base_m = paired[~paired['is_recog']][judge_col].mean()\n",
    "        recog_m = paired[paired['is_recog']][judge_col].mean()\n",
    "        d = cohens_d(paired[paired['is_recog']][judge_col].values, \n",
    "                     paired[~paired['is_recog']][judge_col].values)\n",
    "        print(f'  {label:8s} recognition effect: +{recog_m - base_m:.1f} pts, d={d:.2f}')\n",
    "    \n",
    "    print(f'\\nPaper reports: inter-judge r=0.49\u20130.64, GPT finds ~58% of Claude effect magnitudes')\n",
    "else:\n",
    "    print('No paired cross-judge data found. Rejudging required.')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 25. Blinded Assessment (\u00a76.15, Table 21b)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Blinded Assessment (\u00a76.15, Table 21b)\n",
    "# 2\u00d72: blinding (yes/no) \u00d7 model (Opus/Haiku)\n",
    "print('Blinded Assessment \u2014 Table 21b (\u00a76.15)')\n",
    "print('=' * 70)\n",
    "\n",
    "bilateral_run = 'eval-2026-02-07-b6d75e87'\n",
    "df_bilateral = df_all[\n",
    "    (df_all['run_id'] == bilateral_run) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\n",
    "].copy()\n",
    "\n",
    "# Check for qualitative assessment columns\n",
    "has_unblinded = df_bilateral['qualitative_assessment'].notna().sum()\n",
    "has_blinded = df_bilateral['blinded_qualitative_assessment'].notna().sum()\n",
    "\n",
    "print(f'Bilateral run N = {len(df_bilateral)}')\n",
    "print(f'  Unblinded assessments: {has_unblinded}')\n",
    "print(f'  Blinded assessments:   {has_blinded}')\n",
    "\n",
    "if has_blinded > 0 and has_unblinded > 0:\n",
    "    # Parse qualitative tags from assessments\n",
    "    import re\n",
    "    \n",
    "    def extract_tags(assessment_text):\n",
    "        \"\"\"Extract tags from qualitative assessment JSON or text.\"\"\"\n",
    "        if pd.isna(assessment_text):\n",
    "            return []\n",
    "        try:\n",
    "            parsed = json.loads(assessment_text)\n",
    "            if isinstance(parsed, dict):\n",
    "                return parsed.get('tags', [])\n",
    "            return []\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # Try regex extraction\n",
    "            tags = re.findall(r'(?:recognition_moment|scaffolded_reframing|productive_struggle|'\n",
    "                             r'epistemic_humility|genuine_inquiry|pattern_disruption|'\n",
    "                             r'missed_opportunity|surface_engagement|formulaic_response|'\n",
    "                             r'premature_closure|defensive_retreat)', str(assessment_text))\n",
    "            return tags\n",
    "    \n",
    "    df_bilateral['unblinded_tags'] = df_bilateral['qualitative_assessment'].apply(extract_tags)\n",
    "    df_bilateral['blinded_tags'] = df_bilateral['blinded_qualitative_assessment'].apply(extract_tags)\n",
    "    \n",
    "    is_recog = df_bilateral['profile_name'].str.contains('recog', case=False)\n",
    "    \n",
    "    # Count key positive tags\n",
    "    positive_tags = ['recognition_moment', 'scaffolded_reframing', 'productive_struggle']\n",
    "    \n",
    "    for condition, label in [(~is_recog, 'Base'), (is_recog, 'Recognition')]:\n",
    "        subset = df_bilateral[condition]\n",
    "        n = len(subset)\n",
    "        ub_pos = sum(any(t in tags for t in positive_tags) for tags in subset['unblinded_tags'])\n",
    "        bl_pos = sum(any(t in tags for t in positive_tags) for tags in subset['blinded_tags'])\n",
    "        print(f'\\n  {label} (N={n}):')\n",
    "        print(f'    Unblinded positive tags: {ub_pos}/{n} ({100*ub_pos/n:.0f}%)')\n",
    "        print(f'    Blinded positive tags:   {bl_pos}/{n} ({100*bl_pos/n:.0f}%)')\n",
    "    \n",
    "    print(f'\\nPaper conclusion: Opus blinded replication confirms assessments track')\n",
    "    print(f'genuine dialogue properties, not condition labels')\n",
    "else:\n",
    "    print('Blinded/unblinded assessment data not fully available.')\n",
    "    print(f'Unblinded model: {df_bilateral[\"qualitative_model\"].dropna().unique()}')\n",
    "    print(f'Blinded model: {df_bilateral[\"blinded_qualitative_model\"].dropna().unique()}')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 26. Impasse Strategy Coding (\u00a76.14)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Impasse Strategy Coding (\u00a76.14)\n# N=24: 3 scenarios \u00d7 4 cells \u00d7 2 runs\nprint('Impasse Strategy Coding (\u00a76.14)')\nprint('=' * 70)\n\ndf_imp = get_run('impasse')\nprint(f'Impasse run N = {len(df_imp)}')\n\nis_recog = df_imp['profile_name'].str.contains('recog', case=False)\nbase_m = df_imp[~is_recog]['overall_score'].mean()\nrecog_m = df_imp[is_recog]['overall_score'].mean()\n\nprint(f'  Base mean:        {base_m:.1f} (N={len(df_imp[~is_recog])})')\nprint(f'  Recognition mean: {recog_m:.1f} (N={len(df_imp[is_recog])})')\nprint(f'  Delta:            {recog_m - base_m:+.1f}')\n\n# Per-scenario\nfor scenario in sorted(df_imp['scenario_id'].unique()):\n    sc = df_imp[df_imp['scenario_id'] == scenario]\n    sc_base = sc[~sc['profile_name'].str.contains('recog', case=False)]['overall_score']\n    sc_recog = sc[sc['profile_name'].str.contains('recog', case=False)]['overall_score']\n    if len(sc_base) > 0 and len(sc_recog) > 0:\n        print(f'  {scenario:35s}  Base={sc_base.mean():.1f}  Recog={sc_recog.mean():.1f}  \u0394={sc_recog.mean() - sc_base.mean():+.1f}')\n\nprint(f'\\nPaper reports: 12/12 base=withdrawal, 10/12 recog=scaffolded_reframing')\nprint(f'\u03c7\u00b2(3)=24.00, p<.001, V=1.000 (perfect separation)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 27. Opus 4.5\u21924.6 Judge Version Stability Check (\u00a78.1)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Judge Version Stability Check (\u00a78.1)\n",
    "# Opus 4.6 released Feb 5, 2026. DB stores aliases, not pinned versions.\n",
    "# Check whether recognition deltas shifted across the version boundary.\n",
    "print('Judge Version Stability: Opus 4.5\u21924.6 (\u00a78.1)')\n",
    "print('=' * 70)\n",
    "\n",
    "# Key paper runs with kimi ego, cells 1 & 5\n",
    "version_check_runs = [\n",
    "    'eval-2026-02-03-f5d4dd93',  # factorial (pre-Feb5)\n",
    "    'eval-2026-02-03-86b159cd',  # validation (pre-Feb5)\n",
    "    'eval-2026-02-05-10b344fb',  # A\u00d7B replication (post-Feb5)\n",
    "    'eval-2026-02-06-81f2d5a1',  # memory isolation 1 (post-Feb5)\n",
    "    'eval-2026-02-06-ac9ea8f5',  # memory isolation 2 (post-Feb5)\n",
    "]\n",
    "\n",
    "df_vc = df_all[\n",
    "    df_all['run_id'].isin(version_check_runs) &\n",
    "    df_all['overall_score'].notna() &\n",
    "    (df_all['model'] == 'moonshotai/kimi-k2.5') &\n",
    "    df_all['judge_model'].str.contains('claude-opus', case=False, na=False) &\n",
    "    df_all['profile_name'].isin(['cell_1_base_single_unified', 'cell_5_recog_single_unified'])\n",
    "].copy()\n",
    "\n",
    "df_vc['period'] = df_vc['created_at'].apply(\n",
    "    lambda x: 'pre-Feb5 (Opus 4.5?)' if x < '2026-02-05' else 'post-Feb5 (Opus 4.6?)'\n",
    ")\n",
    "df_vc['is_recog'] = df_vc['profile_name'].str.contains('recog', case=False)\n",
    "\n",
    "for period in ['pre-Feb5 (Opus 4.5?)', 'post-Feb5 (Opus 4.6?)']:\n",
    "    p_data = df_vc[df_vc['period'] == period]\n",
    "    base = p_data[~p_data['is_recog']]['overall_score']\n",
    "    recog = p_data[p_data['is_recog']]['overall_score']\n",
    "    delta = recog.mean() - base.mean()\n",
    "    print(f'  {period:25s}  Base={base.mean():.1f} (N={len(base)})  Recog={recog.mean():.1f} (N={len(recog)})  \u0394={delta:+.1f}')\n",
    "\n",
    "print(f'\\nPaper reports: +16.3 pre-release vs +15.6 post-release')\n",
    "print(f'Absolute scores shift ~2 pts; recognition delta stable across version boundary')"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28. Concordance Check\n",
    "\n",
    "Verify computed statistics match paper-reported values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concordance check: compare computed vs paper-reported values (v2.3.10)\n# Uses get_run() which defaults to Opus-only filtering\nprint('CONCORDANCE CHECK (v2.3.10)')\nprint('=' * 80)\nprint(f'{\"Statistic\":50s} {\"Paper\":>10s} {\"Computed\":>10s} {\"Match\":>8s}')\nprint('-' * 85)\n\npasses = 0\nfails = 0\n\ndef check(label, paper_val, computed_val, tolerance=0.5):\n    \"\"\"Check if computed value matches paper within tolerance.\"\"\"\n    global passes, fails\n    if pd.isna(computed_val):\n        match = '?'\n    elif abs(computed_val - paper_val) <= tolerance:\n        match = 'YES'\n        passes += 1\n    else:\n        match = f'NO ({computed_val - paper_val:+.1f})'\n        fails += 1\n    print(f'{label:50s} {paper_val:10.1f} {computed_val:10.1f} {match:>8s}')\n\n# \u2500\u2500 Sample sizes (Opus-judged) \u2500\u2500\nprint('\\n-- Key Run Sample Sizes (Opus-judged) --')\ncheck('Recognition validation N', 36, len(get_run('recognition_validation')), 2)\ncheck('Full factorial N (f5d4dd93)', 262, len(get_run('full_factorial')), 5)\ncheck('Factorial cells 6,8 N (a933d745)', 88, len(get_run('factorial_cells_6_8')), 2)\ncheck('Memory isolation run 1 N', 60, len(get_run('memory_isolation_1')), 2)\ncheck('Memory isolation run 2 N', 60, len(get_run('memory_isolation_2')), 2)\ncheck('Active control N', 118, len(get_run('active_control')), 2)\ncheck('Dynamic learner N (6c033830)', 120, len(get_run('dynamic_learner')), 2)\ncheck('Mechanism head-to-head N', 120, len(get_run('mechanism_headtohead')), 2)\n\n# \u2500\u2500 Table 5: Factorial main effects \u2500\u2500\nprint('\\n-- Table 5: Factorial ANOVA --')\nfactorial_runs = ['eval-2026-02-03-f5d4dd93', 'eval-2026-02-06-a933d745']\nopus_mask = df_all['judge_model'].str.contains('claude-opus', case=False, na=False)\ndf_f = df_all[\n    df_all['run_id'].isin(factorial_runs) & df_all['overall_score'].notna() & opus_mask\n].copy()\nis_recog = df_f['profile_name'].str.contains('recog', case=False)\nbase_scores = df_f[~is_recog]['overall_score']\nrecog_scores = df_f[is_recog]['overall_score']\ndelta = recog_scores.mean() - base_scores.mean()\nd_val = cohens_d(recog_scores.values, base_scores.values)\ncheck('Factorial N (combined)', 350, len(df_f), 5)\ncheck('Recognition delta (+10.2)', 10.2, delta, 1.0)\ncheck('Recognition d (0.80)', 0.80, d_val, 0.15)\n\n# \u2500\u2500 Table 4b: Memory isolation \u2500\u2500\nprint('\\n-- Table 4b: Memory Isolation --')\nmem_runs = ['eval-2026-02-06-81f2d5a1', 'eval-2026-02-06-ac9ea8f5']\ndf_m = df_all[\n    df_all['run_id'].isin(mem_runs) & df_all['overall_score'].notna() & opus_mask\n].copy()\ncheck('Memory isolation N', 120, len(df_m), 5)\nmem_recog = df_m[df_m['profile_name'].str.contains('cell_5|cell_20', regex=True)]['overall_score']\nmem_base = df_m[df_m['profile_name'].str.contains('cell_1|cell_19', regex=True)]['overall_score']\nd_mem_recog = cohens_d(mem_recog.values, mem_base.values)\ncheck('Recognition d (1.71)', 1.71, d_mem_recog, 0.2)\n\n# \u2500\u2500 Multi-model probe \u2500\u2500\nprint('\\n-- Table 7b: Multi-Model Probe --')\nfor key, expected_n in [('probe_nemotron', 119), ('probe_deepseek', 120),\n                          ('probe_glm', 117), ('probe_haiku', 120)]:\n    check(f'{key} N', expected_n, len(get_run(key)), 2)\n\n# \u2500\u2500 Dynamic learner mechanism architecture \u2500\u2500\nprint('\\n-- Table 19: Dynamic Learner --')\ndyn_runs = ['eval-2026-02-14-6c033830', 'eval-2026-02-14-a2b2717c', 'eval-2026-02-15-664073ab']\ndf_dyn = df_all[\n    df_all['run_id'].isin(dyn_runs) & df_all['overall_score'].notna() & opus_mask\n].copy()\ndyn_recog = df_dyn[df_dyn['profile_name'].str.contains('recog', case=False)]['overall_score']\ndyn_base = df_dyn[~df_dyn['profile_name'].str.contains('recog', case=False)]['overall_score']\ndyn_delta = dyn_recog.mean() - dyn_base.mean()\ncheck('Dynamic learner recog delta (+14.5)', 14.5, dyn_delta, 2.0)\n\n# \u2500\u2500 Judge version stability \u2500\u2500\nprint('\\n-- \u00a78.1: Judge Version Stability --')\nkimi_fact = df_all[\n    df_all['run_id'].isin(factorial_runs) & df_all['overall_score'].notna() & opus_mask\n].copy()\nkimi_fact['date'] = pd.to_datetime(kimi_fact['created_at'])\npre = kimi_fact[kimi_fact['date'] < '2026-02-05']\npost = kimi_fact[kimi_fact['date'] >= '2026-02-05']\nis_r_pre = pre['profile_name'].str.contains('recog', case=False)\nis_r_post = post['profile_name'].str.contains('recog', case=False)\nif len(pre[is_r_pre]) > 0 and len(pre[~is_r_pre]) > 0:\n    pre_delta = pre[is_r_pre]['overall_score'].mean() - pre[~is_r_pre]['overall_score'].mean()\n    check('Pre-Feb5 recognition delta (+16.3)', 16.3, pre_delta, 2.0)\nif len(post[is_r_post]) > 0 and len(post[~is_r_post]) > 0:\n    post_delta = post[is_r_post]['overall_score'].mean() - post[~is_r_post]['overall_score'].mean()\n    check('Post-Feb5 recognition delta (+15.6)', 15.6, post_delta, 2.0)\n\n# \u2500\u2500 Summary \u2500\u2500\nprint('\\n' + '=' * 80)\ntotal = passes + fails\nprint(f'Concordance: {passes}/{total} checks passed ({100*passes/total:.0f}%)')\nif fails > 0:\n    print(f'  {fails} discrepancies \u2014 review tolerance or DB changes')\nelse:\n    print('  All checks within tolerance.')\nprint('Note: Small discrepancies arise from rounding, ongoing runs, or DB updates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "conn.close()\n",
    "print('Database connection closed. Notebook complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
